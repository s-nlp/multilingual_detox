{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Translate the English training set into Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = []\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    df = pd.read_csv(f'../data/russian_data/{split}.tsv', sep='\\t')\n",
    "    df['split'] = split\n",
    "    parts.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_ru = pd.concat(parts, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>neutral_comment</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>toxic</th>\n",
       "      <th>confidence_toxic</th>\n",
       "      <th>is_match</th>\n",
       "      <th>confidence_is_match</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6142</td>\n",
       "      <td>–∏,—á—ë,–±–ª—è–¥—å –≥–¥–µ —ç—Ç–æ—Ç —Ö–µ—Ä–æ–π –±—ã–ª –¥–æ —ç—Ç–æ–≥–æ —Å–æ —Å–≤–æ–∏...</td>\n",
       "      <td>–ù—É –∏ –≥–¥–µ —ç—Ç–æ—Ç –≥–µ—Ä–æ–π –±—ã–ª,—Å–æ —Å–≤–æ–∏–º–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç...</td>\n",
       "      <td>0.999007</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9210</td>\n",
       "      <td>–µ–±–∞–Ω—å–∫–æ –∏–∑ –∫–∞–∫–æ–≥–æ –µ–±–æ–Ω–∞—Ä–∏—è —Ç–µ–±—è –≤—ã–ø—É—Å—Ç–∏–ª–∏???üòÇüòÇüòÇ</td>\n",
       "      <td>–û—Ç–∫—É–¥–∞ —Ç—ã –≤–∑—è–ª—Å—è —Ç–∞–∫–æ–π?</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8692</td>\n",
       "      <td>—Ö–µ—Ä–Ω—É –≤—Å—è–∫—É—é –ø–∏—à—É—Ç,–∏–∑-–∑–∞ —ç—Ç–æ–≥–æ –ª–∞–π–∫–∞.–¥–æ–ª–±–æ–µ–±–∏–∑–º.</td>\n",
       "      <td>–ß—É—à—å –≤—Å—è–∫—É—é –ø–∏—à—É—Ç, –∏–∑- –∑–∞ —ç—Ç–æ–≥–æ –ª–∞–π–∫–∞.</td>\n",
       "      <td>0.998891</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10814</td>\n",
       "      <td>–≤ –≥—Å–≤–≥ –∑–∞ —Ç–∞–∫–∏–µ —Å–∞–ø–æ–≥–∏ –ø–∏–∑–¥—é–ª–µ–π –ø–æ–ª—É—á–∏–ª –±—ã –æ—Ç ...</td>\n",
       "      <td>–í –ì–°–í–ì –∑–∞ —Ç–∞–∫–∏–µ —Å–∞–ø–æ–≥–∏ –Ω–∞–∫–∞–∑–∞–ª–∏ –±—ã —Å–æ—Å–ª—É–∂–∏–≤—Ü—ã</td>\n",
       "      <td>0.998883</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7915</td>\n",
       "      <td>–∫–∞–∫–æ–π –¥–µ–±–∏–ª —ç—Ç—É –ø—Ä–∏–¥—É–º—ã–≤–∞–µ—Ç ü§£</td>\n",
       "      <td>–ö—Ç–æ —ç—Ç–æ –ø—Ä–∏–¥—É–º—ã–≤–∞–µ—Ç?</td>\n",
       "      <td>0.921756</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7053</th>\n",
       "      <td>6522</td>\n",
       "      <td>–¥–µ–±–∏–ª,–∑–∞ —Ä–µ—à–æ—Ç–∫—É —Ç–µ–±—è –∏ –ª–∏—à–∏—Ç—å –≤—Å–µ–≥–æ!</td>\n",
       "      <td>–û—Ç–∫—Ä—ã—Ç—å –±—ã –Ω–∞ —Ç–µ–±—è —Å—É–¥–µ–±–Ω–æ–µ –¥–µ–ª–æ</td>\n",
       "      <td>0.995274</td>\n",
       "      <td>ru_test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9777</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7054</th>\n",
       "      <td>17248</td>\n",
       "      <td>–ø–æ–ø—ã—Ç–∫–∞ –≤—ã*–±–Ω—É—Ç—å—Å—è? –Ω–µ –≤—ã –æ–¥–Ω–∏ –Ω–µ –ø—å–µ—Ç–µ. –∂–¥–µ—Ç–µ...</td>\n",
       "      <td>–ü–æ—á–µ–º—É –≤—ã —Ö–≤–∞—Å—Ç–∞–µ—Ç–µ—Å—å? –Ω–µ –≤—ã –æ–¥–Ω–∏ –Ω–µ —É–ø–æ—Ç—Ä–µ–±–ª—è...</td>\n",
       "      <td>0.998892</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7055</th>\n",
       "      <td>11330</td>\n",
       "      <td>–≤—ã –Ω—ã—Ç–∏–∫–∏ –∑–∞–∫–æ–ª–µ–±–∞–ª–∏.–µ–¥—å –≤ —á–µ—Ö–∏—é.—Å–∫–∞–∂–µ—à—å –∑–∞ –¥–æ...</td>\n",
       "      <td>–ï–∑–∂–∞–π—Ç–µ –≤ –ß–µ—Ö–∏—é –∏ –ø–æ—Ç–æ–º –±—É–¥–µ—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å , –ø—Ä–æ ...</td>\n",
       "      <td>0.947868</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9166</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7056</th>\n",
       "      <td>67</td>\n",
       "      <td>–≤ –≥–ª–∞–∑–∞—Ö —á—É–∂–æ–≥–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞ –æ–Ω–∏ –∫–æ–Ω–µ—á–Ω–æ –∫—Ä–∞—Å–∞–≤...</td>\n",
       "      <td>–í –≥–ª–∞–∑–∞—Ö –¥—Ä—É–≥–æ–≥–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞ –æ–Ω–∏ –∫–æ–Ω–µ—á–Ω–æ –∫—Ä–∞—Å–∞...</td>\n",
       "      <td>0.994028</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9341</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9727</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7057</th>\n",
       "      <td>7090</td>\n",
       "      <td>—Å–∞–º —Å—É–∫–∞ —á–º–æ –µ–±–∞–Ω–Ω–æ–µ –ø–æ-—Ä—É—Å–∫–∏ –ø–∏—Å–∞—Ç—å –Ω–∞—É—á–∏—Å—å, ...</td>\n",
       "      <td>–°–∞–º –ø–æ-—Ä—É—Å—Å–∫–∏ –ø–∏—Å–∞—Ç—å –Ω–∞—É—á–∏—Å—å</td>\n",
       "      <td>0.997218</td>\n",
       "      <td>ru_test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9777</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7058 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        idx                                      toxic_comment  \\\n",
       "0      6142  –∏,—á—ë,–±–ª—è–¥—å –≥–¥–µ —ç—Ç–æ—Ç —Ö–µ—Ä–æ–π –±—ã–ª –¥–æ —ç—Ç–æ–≥–æ —Å–æ —Å–≤–æ–∏...   \n",
       "1      9210    –µ–±–∞–Ω—å–∫–æ –∏–∑ –∫–∞–∫–æ–≥–æ –µ–±–æ–Ω–∞—Ä–∏—è —Ç–µ–±—è –≤—ã–ø—É—Å—Ç–∏–ª–∏???üòÇüòÇüòÇ   \n",
       "2      8692   —Ö–µ—Ä–Ω—É –≤—Å—è–∫—É—é –ø–∏—à—É—Ç,–∏–∑-–∑–∞ —ç—Ç–æ–≥–æ –ª–∞–π–∫–∞.–¥–æ–ª–±–æ–µ–±–∏–∑–º.   \n",
       "3     10814  –≤ –≥—Å–≤–≥ –∑–∞ —Ç–∞–∫–∏–µ —Å–∞–ø–æ–≥–∏ –ø–∏–∑–¥—é–ª–µ–π –ø–æ–ª—É—á–∏–ª –±—ã –æ—Ç ...   \n",
       "4      7915                      –∫–∞–∫–æ–π –¥–µ–±–∏–ª —ç—Ç—É –ø—Ä–∏–¥—É–º—ã–≤–∞–µ—Ç ü§£   \n",
       "...     ...                                                ...   \n",
       "7053   6522              –¥–µ–±–∏–ª,–∑–∞ —Ä–µ—à–æ—Ç–∫—É —Ç–µ–±—è –∏ –ª–∏—à–∏—Ç—å –≤—Å–µ–≥–æ!   \n",
       "7054  17248  –ø–æ–ø—ã—Ç–∫–∞ –≤—ã*–±–Ω—É—Ç—å—Å—è? –Ω–µ –≤—ã –æ–¥–Ω–∏ –Ω–µ –ø—å–µ—Ç–µ. –∂–¥–µ—Ç–µ...   \n",
       "7055  11330  –≤—ã –Ω—ã—Ç–∏–∫–∏ –∑–∞–∫–æ–ª–µ–±–∞–ª–∏.–µ–¥—å –≤ —á–µ—Ö–∏—é.—Å–∫–∞–∂–µ—à—å –∑–∞ –¥–æ...   \n",
       "7056     67  –≤ –≥–ª–∞–∑–∞—Ö —á—É–∂–æ–≥–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞ –æ–Ω–∏ –∫–æ–Ω–µ—á–Ω–æ –∫—Ä–∞—Å–∞–≤...   \n",
       "7057   7090  —Å–∞–º —Å—É–∫–∞ —á–º–æ –µ–±–∞–Ω–Ω–æ–µ –ø–æ-—Ä—É—Å–∫–∏ –ø–∏—Å–∞—Ç—å –Ω–∞—É—á–∏—Å—å, ...   \n",
       "\n",
       "                                        neutral_comment  toxicity_score  \\\n",
       "0     –ù—É –∏ –≥–¥–µ —ç—Ç–æ—Ç –≥–µ—Ä–æ–π –±—ã–ª,—Å–æ —Å–≤–æ–∏–º–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç...        0.999007   \n",
       "1                               –û—Ç–∫—É–¥–∞ —Ç—ã –≤–∑—è–ª—Å—è —Ç–∞–∫–æ–π?        0.999326   \n",
       "2                –ß—É—à—å –≤—Å—è–∫—É—é –ø–∏—à—É—Ç, –∏–∑- –∑–∞ —ç—Ç–æ–≥–æ –ª–∞–π–∫–∞.        0.998891   \n",
       "3         –í –ì–°–í–ì –∑–∞ —Ç–∞–∫–∏–µ —Å–∞–ø–æ–≥–∏ –Ω–∞–∫–∞–∑–∞–ª–∏ –±—ã —Å–æ—Å–ª—É–∂–∏–≤—Ü—ã        0.998883   \n",
       "4                                  –ö—Ç–æ —ç—Ç–æ –ø—Ä–∏–¥—É–º—ã–≤–∞–µ—Ç?        0.921756   \n",
       "...                                                 ...             ...   \n",
       "7053                   –û—Ç–∫—Ä—ã—Ç—å –±—ã –Ω–∞ —Ç–µ–±—è —Å—É–¥–µ–±–Ω–æ–µ –¥–µ–ª–æ        0.995274   \n",
       "7054  –ü–æ—á–µ–º—É –≤—ã —Ö–≤–∞—Å—Ç–∞–µ—Ç–µ—Å—å? –Ω–µ –≤—ã –æ–¥–Ω–∏ –Ω–µ —É–ø–æ—Ç—Ä–µ–±–ª—è...        0.998892   \n",
       "7055  –ï–∑–∂–∞–π—Ç–µ –≤ –ß–µ—Ö–∏—é –∏ –ø–æ—Ç–æ–º –±—É–¥–µ—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å , –ø—Ä–æ ...        0.947868   \n",
       "7056  –í –≥–ª–∞–∑–∞—Ö –¥—Ä—É–≥–æ–≥–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞ –æ–Ω–∏ –∫–æ–Ω–µ—á–Ω–æ –∫—Ä–∞—Å–∞...        0.994028   \n",
       "7057                       –°–∞–º –ø–æ-—Ä—É—Å—Å–∫–∏ –ø–∏—Å–∞—Ç—å –Ω–∞—É—á–∏—Å—å        0.997218   \n",
       "\n",
       "      dataset  toxic  confidence_toxic  is_match  confidence_is_match  split  \n",
       "0          ru  False            0.9999      True               0.9985  train  \n",
       "1          ru  False            0.9999      True               0.9972  train  \n",
       "2          ru  False            0.9999      True               0.9964  train  \n",
       "3          ru  False            0.9999      True               0.9950  train  \n",
       "4          ru  False            0.9999      True               0.9950  train  \n",
       "...       ...    ...               ...       ...                  ...    ...  \n",
       "7053  ru_test  False            0.9987      True               0.9777   test  \n",
       "7054       ru  False            0.9997      True               0.9800   test  \n",
       "7055       ru  False            0.9999      True               0.9166   test  \n",
       "7056       ru  False            0.9341      True               0.9727   test  \n",
       "7057  ru_test  False            0.9799      True               0.9777   test  \n",
       "\n",
       "[7058 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    5058\n",
       "dev      1000\n",
       "test     1000\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_ru.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7058, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_ru.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_by_symbol(text, symbol=',', max_len=400):\n",
    "    if len(text) <= max_len:\n",
    "        return [text]\n",
    "    chunks = re.split(symbol, text)\n",
    "    if len(chunks) <= 1:\n",
    "        return [text]\n",
    "    result = [chunks[0]]\n",
    "    for chunk in chunks[1:]:\n",
    "        result.append(symbol)\n",
    "        result.append(chunk)\n",
    "    return result\n",
    "\n",
    "def join_texts(texts, max_len=400):\n",
    "    result = []\n",
    "    prev_text = ''\n",
    "    for text in texts:\n",
    "        if len(text) + len(prev_text) > max_len:\n",
    "            result.append(prev_text)\n",
    "            prev_text = text\n",
    "        else:\n",
    "            prev_text = prev_text + text\n",
    "    result.append(prev_text)\n",
    "    return result\n",
    "\n",
    "def hard_split(text, max_len=300):\n",
    "    parts = list(sent_tokenize(text))\n",
    "    result = []\n",
    "    for part in parts:\n",
    "        chunks = [part]\n",
    "        for symbol in [',', '-', ' ']:\n",
    "            chunks = [c2 for c in chunks for c2 in split_by_symbol(c, symbol, max_len=max_len)]\n",
    "        result.extend(chunks)\n",
    "    result = join_texts(result, max_len=max_len)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "How to obtain a fresh SID:\n",
    "* go to translate.yandex.ru\n",
    "* open the \"network\" panel of the developers console\n",
    "* enter any text in the translation form\n",
    "* find the request to \"https://translate.yandex.net/api/v1/tr.json/translate\" and copy its first parameter (\"id\")\n",
    "'''\n",
    "\n",
    "import requests\n",
    "\n",
    "SID = 'd893eb46.629f444a.4b7f3984.74722d74657874-1-0'\n",
    "\n",
    "def translate_yandex(search_str, direction='en-ru', full_response=False):\n",
    "    try:\n",
    "        url = f'https://translate.yandex.net/api/v1/tr.json/translate?id={SID}&srv=tr-text&lang={direction}&reason=auto&format=text'\n",
    "\n",
    "        post_header = {}\n",
    "        post_header['Accept'] = '*/*'\n",
    "        post_header['Accept-Encoding'] = 'gzip, deflate'\n",
    "        post_header['Accept-Language'] = 'en-US,en;q=0.9'\n",
    "        post_header['Cache-Control'] = 'no-cache'\n",
    "        post_header['Connection'] = 'keep-alive'\n",
    "        post_header['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "        post_header['Host'] = 'translate.yandex.com'\n",
    "        post_header['Referer'] = 'https://translate.yandex.com/'\n",
    "        post_header['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 YaBrowser/21.8.2.383 Yowser/2.5 Safari/537.36'\n",
    "\n",
    "\n",
    "        data_payload = {'text': search_str, 'options': '4'}\n",
    "\n",
    "        resp = requests.post(url, headers=post_header, data=data_payload).json()\n",
    "        if full_response:\n",
    "            return resp\n",
    "\n",
    "        if resp.get('message') == 'The text size exceeds the maximum':\n",
    "            parts = hard_split(search_str)\n",
    "            if len(parts) > 1:\n",
    "                return 200, ' '.join([translate_yandex(part, dir=dir)[1] for part in parts])\n",
    "\n",
    "        return resp['code'], resp['text'][0]\n",
    "    except Exception as e:\n",
    "        print(resp)\n",
    "        return 0, ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, '–∏–¥–∏ –∫ —á–µ—Ä—Ç—É')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_yandex('go to hell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–∏,—á—ë,–±–ª—è–¥—å –≥–¥–µ —ç—Ç–æ—Ç —Ö–µ—Ä–æ–π –±—ã–ª –¥–æ —ç—Ç–æ–≥–æ —Å–æ —Å–≤–æ–∏–º–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏?',\n",
       " '–µ–±–∞–Ω—å–∫–æ –∏–∑ –∫–∞–∫–æ–≥–æ –µ–±–æ–Ω–∞—Ä–∏—è —Ç–µ–±—è –≤—ã–ø—É—Å—Ç–∏–ª–∏???üòÇüòÇüòÇ',\n",
       " '—Ö–µ—Ä–Ω—É –≤—Å—è–∫—É—é –ø–∏—à—É—Ç,–∏–∑-–∑–∞ —ç—Ç–æ–≥–æ –ª–∞–π–∫–∞.–¥–æ–ª–±–æ–µ–±–∏–∑–º.',\n",
       " '–≤ –≥—Å–≤–≥ –∑–∞ —Ç–∞–∫–∏–µ —Å–∞–ø–æ–≥–∏ –ø–∏–∑–¥—é–ª–µ–π –ø–æ–ª—É—á–∏–ª –±—ã –æ—Ç —Å–æ—Å–ª—É–∂–∏–≤—Ü–µ–≤',\n",
       " '–∫–∞–∫–æ–π –¥–µ–±–∏–ª —ç—Ç—É –ø—Ä–∏–¥—É–º—ã–≤–∞–µ—Ç ü§£']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_ru.toxic_comment[:5].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and, what, where the fuck was this dick before with his evidence?',\n",
       " 'what fucking hospital did they let you out of???üòÇüòÇüòÇ',\n",
       " 'they write all sorts of bullshit, because of this like.motherfucking.',\n",
       " 'in gsvg , I would have received a pussy from my colleagues for such boots',\n",
       " 'what kind of moron comes up with this one ü§£']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[translate_yandex(t, 'ru-en')[1] for t in detox_ru.toxic_comment[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Well, where was this hero, with his evidence?',\n",
       " 'Where did you come from?',\n",
       " 'They write all sorts of nonsense, because of this like.',\n",
       " 'In the GSVG, colleagues would be punished for such boots',\n",
       " \"Who's coming up with this?\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[translate_yandex(t, 'ru-en')[1] for t in detox_ru.neutral_comment[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a7073cd20f47aeb118939f54863b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toxic_en = []\n",
    "neutral_en = []\n",
    "for i in trange(0, len(detox_ru)):\n",
    "    toxic_en.append(translate_yandex(detox_ru.toxic_comment[i], 'ru-en')[1])\n",
    "    neutral_en.append(translate_yandex(detox_ru.neutral_comment[i], 'ru-en')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_ru['toxic_en'] = toxic_en\n",
    "detox_ru['neutral_en'] = neutral_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>neutral_comment</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>toxic</th>\n",
       "      <th>confidence_toxic</th>\n",
       "      <th>is_match</th>\n",
       "      <th>confidence_is_match</th>\n",
       "      <th>split</th>\n",
       "      <th>toxic_en</th>\n",
       "      <th>neutral_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>16602</td>\n",
       "      <td>–¥–∏–º–∞ –ø–µ—Ç—Ä–æ–≤—Å–∫–∏–π –ø–∏–¥–æ—Ä —Ç—ã –≥–Ω–æ–π–Ω—ã–π</td>\n",
       "      <td>–¥–∏–º–∞ –ø–µ—Ç—Ä–æ–≤—Å–∫–∏–π –Ω–µ —Ö–æ—Ä–æ—à–∏–π —á–µ–ª–æ–≤–µ–∫</td>\n",
       "      <td>0.999302</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>train</td>\n",
       "      <td>Dima Petrovsky You're a purulent faggot</td>\n",
       "      <td>Dima Petrovsky is not a good person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>7266</td>\n",
       "      <td>—É—Ä–æ–¥—ã....–æ–Ω –æ–¥–∏–Ω –∏–∑ –ª—É—á—à–∏—Ö –ø–æ–ª–∫–æ–≤–æ–¥—Ü–µ–≤ –≤ –∏—Å—Ç–æ—Ä–∏–∏...–∂–∞–ª—å</td>\n",
       "      <td>–ñ–∞–ª–∫–æ –æ–¥–Ω–æ–≥–æ –∏–∑ –ª—É—á—à–∏—Ö –ø–æ–ª–∫–æ–≤–æ–¥—Ü–µ–≤ –≤ –∏—Å—Ç–æ—Ä–∏–∏</td>\n",
       "      <td>0.986603</td>\n",
       "      <td>ru_test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9379</td>\n",
       "      <td>train</td>\n",
       "      <td>freaks....he is one of the best generals in history...sorry</td>\n",
       "      <td>I feel sorry for one of the best generals in history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>8643</td>\n",
       "      <td>–∑–∞—á–µ–º —Å —Ç–∞–∫–∏–º –∂–∏—Ç—å —Ç–≤–∞—Ä—å. —Ä–∞—Å—Å—Ç—Ä–µ–ª—è—Ç—å</td>\n",
       "      <td>–ó–∞—á–µ–º —Å —Ç–∞–∫–∏–º –∂–∏—Ç—å?</td>\n",
       "      <td>0.999340</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9069</td>\n",
       "      <td>train</td>\n",
       "      <td>why live with such a creature. shoot</td>\n",
       "      <td>Why live with this?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6975</th>\n",
       "      <td>16053</td>\n",
       "      <td>–Ω–∏ —Ö—É—è —Å–µ–±–µ —á—É—á–µ–ª–æ, –∞ –≥–¥–µ –≥–ª–∞–≤–Ω—ã–π?????</td>\n",
       "      <td>–ê –≥–¥–µ –≥–ª–∞–≤–Ω—ã–π?</td>\n",
       "      <td>0.948019</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9600</td>\n",
       "      <td>test</td>\n",
       "      <td>not a fucking scarecrow, but where is the main one?????</td>\n",
       "      <td>And where is the main one?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6631</th>\n",
       "      <td>12592</td>\n",
       "      <td>–æ –¥–æ–ª–±–æ—ë–± –ø—Ä–æ—Å–Ω—É–ª—Å—è –∏–¥–∏ –Ω–∞ —É–∫–æ–ª —Ç–µ–±–µ —á–µ–∫–∞—é—Ç—å</td>\n",
       "      <td>–æ –ø—Ä–æ—Å–Ω—É–ª—Å—è –∏–¥–∏ –Ω–∞ —É–∫–æ–ª —Ç–µ–±–µ —á–µ–∫–∞—é—Ç—å</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9911</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>test</td>\n",
       "      <td>oh, the fucker woke up, go for an injection to check you</td>\n",
       "      <td>oh woke up go for a shot you check out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        idx                                            toxic_comment  \\\n",
       "4481  16602                         –¥–∏–º–∞ –ø–µ—Ç—Ä–æ–≤—Å–∫–∏–π –ø–∏–¥–æ—Ä —Ç—ã –≥–Ω–æ–π–Ω—ã–π   \n",
       "1373   7266  —É—Ä–æ–¥—ã....–æ–Ω –æ–¥–∏–Ω –∏–∑ –ª—É—á—à–∏—Ö –ø–æ–ª–∫–æ–≤–æ–¥—Ü–µ–≤ –≤ –∏—Å—Ç–æ—Ä–∏–∏...–∂–∞–ª—å   \n",
       "1239   8643                    –∑–∞—á–µ–º —Å —Ç–∞–∫–∏–º –∂–∏—Ç—å —Ç–≤–∞—Ä—å. —Ä–∞—Å—Å—Ç—Ä–µ–ª—è—Ç—å   \n",
       "6975  16053                   –Ω–∏ —Ö—É—è —Å–µ–±–µ —á—É—á–µ–ª–æ, –∞ –≥–¥–µ –≥–ª–∞–≤–Ω—ã–π?????   \n",
       "6631  12592             –æ –¥–æ–ª–±–æ—ë–± –ø—Ä–æ—Å–Ω—É–ª—Å—è –∏–¥–∏ –Ω–∞ —É–∫–æ–ª —Ç–µ–±–µ —á–µ–∫–∞—é—Ç—å   \n",
       "\n",
       "                                   neutral_comment  toxicity_score  dataset  \\\n",
       "4481            –¥–∏–º–∞ –ø–µ—Ç—Ä–æ–≤—Å–∫–∏–π –Ω–µ —Ö–æ—Ä–æ—à–∏–π —á–µ–ª–æ–≤–µ–∫        0.999302       ru   \n",
       "1373  –ñ–∞–ª–∫–æ –æ–¥–Ω–æ–≥–æ –∏–∑ –ª—É—á—à–∏—Ö –ø–æ–ª–∫–æ–≤–æ–¥—Ü–µ–≤ –≤ –∏—Å—Ç–æ—Ä–∏–∏        0.986603  ru_test   \n",
       "1239                           –ó–∞—á–µ–º —Å —Ç–∞–∫–∏–º –∂–∏—Ç—å?        0.999340       ru   \n",
       "6975                                –ê –≥–¥–µ –≥–ª–∞–≤–Ω—ã–π?        0.948019       ru   \n",
       "6631          –æ –ø—Ä–æ—Å–Ω—É–ª—Å—è –∏–¥–∏ –Ω–∞ —É–∫–æ–ª —Ç–µ–±–µ —á–µ–∫–∞—é—Ç—å        0.999194       ru   \n",
       "\n",
       "      toxic  confidence_toxic  is_match  confidence_is_match  split  \\\n",
       "4481  False            0.9999      True               0.9915  train   \n",
       "1373  False            0.9996      True               0.9379  train   \n",
       "1239  False            0.9955      True               0.9069  train   \n",
       "6975  False            0.9999      True               0.9600   test   \n",
       "6631  False            0.9911      True               0.9794   test   \n",
       "\n",
       "                                                         toxic_en  \\\n",
       "4481                      Dima Petrovsky You're a purulent faggot   \n",
       "1373  freaks....he is one of the best generals in history...sorry   \n",
       "1239                         why live with such a creature. shoot   \n",
       "6975      not a fucking scarecrow, but where is the main one?????   \n",
       "6631     oh, the fucker woke up, go for an injection to check you   \n",
       "\n",
       "                                                neutral_en  \n",
       "4481                   Dima Petrovsky is not a good person  \n",
       "1373  I feel sorry for one of the best generals in history  \n",
       "1239                                   Why live with this?  \n",
       "6975                            And where is the main one?  \n",
       "6631                oh woke up go for a shot you check out  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_ru.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_ru.to_csv('detox_ru2en_yandex.tsv', index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006942476622272598"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(detox_ru.toxic_en == detox_ru.neutral_en).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textdistance import levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_ru['edit_distance_ru'] = [levenshtein.distance(*row) for row in detox_ru[['toxic_comment', 'neutral_comment']].values]\n",
    "detox_ru['edit_distance_en'] = [levenshtein.distance(*row) for row in detox_ru[['toxic_en', 'neutral_en']].values]\n",
    "\n",
    "detox_ru['edit_sim_ru'] = [levenshtein.normalized_similarity(*row) for row in detox_ru[['toxic_comment', 'neutral_comment']].values]\n",
    "detox_ru['edit_sim_en'] = [levenshtein.normalized_similarity(*row) for row in detox_ru[['toxic_en', 'neutral_en']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>confidence_toxic</th>\n",
       "      <th>confidence_is_match</th>\n",
       "      <th>edit_distance_ru</th>\n",
       "      <th>edit_distance_en</th>\n",
       "      <th>edit_sim_ru</th>\n",
       "      <th>edit_sim_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7058.000000</td>\n",
       "      <td>7058.000000</td>\n",
       "      <td>7058.000000</td>\n",
       "      <td>7058.000000</td>\n",
       "      <td>7058.000000</td>\n",
       "      <td>7058.000000</td>\n",
       "      <td>7058.000000</td>\n",
       "      <td>7058.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8509.176821</td>\n",
       "      <td>0.984075</td>\n",
       "      <td>0.984749</td>\n",
       "      <td>0.972116</td>\n",
       "      <td>22.059365</td>\n",
       "      <td>29.011193</td>\n",
       "      <td>0.639245</td>\n",
       "      <td>0.599037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6270.499049</td>\n",
       "      <td>0.030714</td>\n",
       "      <td>0.023933</td>\n",
       "      <td>0.024514</td>\n",
       "      <td>15.346834</td>\n",
       "      <td>18.704621</td>\n",
       "      <td>0.202165</td>\n",
       "      <td>0.202235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800721</td>\n",
       "      <td>0.900300</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.020408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3351.250000</td>\n",
       "      <td>0.986220</td>\n",
       "      <td>0.979900</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.448276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6819.000000</td>\n",
       "      <td>0.996124</td>\n",
       "      <td>0.997200</td>\n",
       "      <td>0.981100</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.620690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13514.000000</td>\n",
       "      <td>0.999306</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.990600</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21360.000000</td>\n",
       "      <td>0.999356</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                idx  toxicity_score  confidence_toxic  confidence_is_match  \\\n",
       "count   7058.000000     7058.000000       7058.000000          7058.000000   \n",
       "mean    8509.176821        0.984075          0.984749             0.972116   \n",
       "std     6270.499049        0.030714          0.023933             0.024514   \n",
       "min        1.000000        0.800721          0.900300             0.900000   \n",
       "25%     3351.250000        0.986220          0.979900             0.960100   \n",
       "50%     6819.000000        0.996124          0.997200             0.981100   \n",
       "75%    13514.000000        0.999306          0.999900             0.990600   \n",
       "max    21360.000000        0.999356          0.999900             0.999900   \n",
       "\n",
       "       edit_distance_ru  edit_distance_en  edit_sim_ru  edit_sim_en  \n",
       "count       7058.000000       7058.000000  7058.000000  7058.000000  \n",
       "mean          22.059365         29.011193     0.639245     0.599037  \n",
       "std           15.346834         18.704621     0.202165     0.202235  \n",
       "min            0.000000          0.000000     0.030303     0.020408  \n",
       "25%           11.000000         15.000000     0.510204     0.448276  \n",
       "50%           18.000000         25.000000     0.677966     0.620690  \n",
       "75%           29.000000         38.000000     0.800000     0.761905  \n",
       "max          132.000000        172.000000     1.000000     1.000000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_ru.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_ru2en = detox_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_ru2en.to_csv('detox_ru2en_yandex.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the Russian model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_ru2en = pd.read_csv('detox_ru2en_yandex.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter text pairs by similarity to escape translation artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7058, 16)\n",
      "(6739, 16)\n"
     ]
    }
   ],
   "source": [
    "detox_ru2en_filtered = detox_ru2en[\n",
    "    ((detox_ru2en.edit_distance_en >= detox_ru2en.edit_distance_ru.quantile(0.01)) \n",
    "    & (detox_ru2en.edit_distance_en <= detox_ru2en.edit_distance_ru.quantile(0.99)) \n",
    "    & (detox_ru2en.edit_sim_en >= detox_ru2en.edit_sim_ru.quantile(0.01)) \n",
    "    & (detox_ru2en.edit_sim_en <= detox_ru2en.edit_sim_ru.quantile(0.99))\n",
    "    )\n",
    "]\n",
    "\n",
    "print(detox_ru2en.shape)\n",
    "print(detox_ru2en_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = detox_ru2en_filtered[detox_ru2en_filtered.split=='train']\n",
    "val = detox_ru2en_filtered[detox_ru2en_filtered.split=='dev']\n",
    "test = detox_ru2en_filtered[detox_ru2en_filtered.split=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'target'],\n",
       "        num_rows: 4825\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['text', 'target'],\n",
       "        num_rows: 961\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = DatasetDict({\n",
    "    'train': Dataset.from_dict({'text': train.toxic_en, 'target': train.neutral_en}),\n",
    "    'dev': Dataset.from_dict({'text': val.toxic_en, 'target': val.neutral_en}),\n",
    "})\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and evaluate baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import CHRF\n",
    "chrfpp = CHRF(word_order=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline chrf++: 60% if not change the texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.26783177670117"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrfpp.corpus_score(val.toxic_en.tolist(), [val.neutral_en.tolist()]).score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A baseline that removes bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "def detokenize(text):\n",
    "    for symbol in \",.?!'\":\n",
    "        text = text.replace(' ' + symbol, symbol)\n",
    "    return text\n",
    "\n",
    "class Remover:\n",
    "    def __init__(self, ratio_threshold=2):\n",
    "        self.ratio_threshold = ratio_threshold\n",
    "    def fit(self, x, y):\n",
    "        self.x_count = Counter(w.lower() for text in x for w in wordpunct_tokenize(text))\n",
    "        self.y_count = Counter(w.lower() for text in y for w in wordpunct_tokenize(text))\n",
    "    def predict(self, x):\n",
    "        results = []\n",
    "        for text in x:\n",
    "            words = []\n",
    "            for w in wordpunct_tokenize(text):\n",
    "                key = w.lower()\n",
    "                if (self.x_count[key] + 1) / (self.y_count[key] + 1) > self.ratio_threshold:\n",
    "                    continue\n",
    "                words.append(w)\n",
    "            results.append(detokenize(' '.join(words)))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = Remover(2.0)\n",
    "remover.fit(train.toxic_en, train.neutral_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.26938732564818"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrfpp.corpus_score(remover.predict(val.toxic_en), [val.neutral_en.tolist()]).score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple word-based translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from itertools import product\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# https://johnlekberg.com/blog/2020-10-25-seq-align.html\n",
    "\n",
    "\n",
    "def needleman_wunsch(x, y, sim=None, verbose=False):\n",
    "    \"\"\"Run the Needleman-Wunsch algorithm on two sequences.\n",
    "\n",
    "    x, y -- sequences.\n",
    "\n",
    "    Code based on pseudocode in Section 3 of:\n",
    "\n",
    "    Naveed, Tahir; Siddiqui, Imitaz Saeed; Ahmed, Shaftab.\n",
    "    \"Parallel Needleman-Wunsch Algorithm for Grid.\" n.d.\n",
    "    https://upload.wikimedia.org/wikipedia/en/c/c4/ParallelNeedlemanAlgorithm.pdf\n",
    "    \"\"\"\n",
    "    N, M = len(x), len(y)\n",
    "    if sim is None:\n",
    "        s = lambda a, b: int(a == b)\n",
    "    else:\n",
    "        s = sim\n",
    "\n",
    "    DIAG = -1, -1\n",
    "    LEFT = -1, 0\n",
    "    UP = 0, -1\n",
    "\n",
    "    # Create tables F and Ptr\n",
    "    F = {}\n",
    "    Ptr = {}\n",
    "\n",
    "    F[-1, -1] = 0\n",
    "    for i in range(N):\n",
    "        F[i, -1] = -i\n",
    "    for j in range(M):\n",
    "        F[-1, j] = -j\n",
    "\n",
    "    option_Ptr = DIAG, LEFT, UP\n",
    "    for i, j in product(range(N), range(M)):\n",
    "        option_F = (\n",
    "            F[i - 1, j - 1] + s(x[i], y[j]),\n",
    "            F[i - 1, j] - 1,\n",
    "            F[i, j - 1] - 1,\n",
    "        )\n",
    "        F[i, j], Ptr[i, j] = max(zip(option_F, option_Ptr))\n",
    "\n",
    "    # Work backwards from (N - 1, M - 1) to (0, 0)\n",
    "    # to find the best alignment.\n",
    "    alignment = deque()\n",
    "    i, j = N - 1, M - 1\n",
    "    if verbose:\n",
    "        tq = tqdm(total=max(N, M))\n",
    "        \n",
    "    while i >= 0 and j >= 0:\n",
    "        direction = Ptr[i, j]\n",
    "        if direction == DIAG:\n",
    "            element = i, j\n",
    "        elif direction == LEFT:\n",
    "            element = i, None\n",
    "        elif direction == UP:\n",
    "            element = None, j\n",
    "        alignment.appendleft(element)\n",
    "        di, dj = direction\n",
    "        i, j = i + di, j + dj\n",
    "    while i >= 0:\n",
    "        alignment.appendleft((i, None))\n",
    "        i -= 1\n",
    "    while j >= 0:\n",
    "        alignment.appendleft((None, j))\n",
    "        j -= 1\n",
    "\n",
    "    return list(alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "from tqdm.auto import tqdm, trange\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextReplacer:\n",
    "    def __init__(self, max_n=3, smooth_n=10, min_n=10, min_p=0.01):\n",
    "        self.max_n = max_n\n",
    "        self.smooth_n = smooth_n\n",
    "        self.min_n = min_n\n",
    "        self.min_p = min_p\n",
    "        \n",
    "        self.replace_proba = {}\n",
    "        self.replaced_tuples = {}\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return nltk.wordpunct_tokenize('_bos_ ' + text + ' _eos_')\n",
    "    \n",
    "    def detokenize(self, text):\n",
    "        text = text.strip()\n",
    "        for symbol in '.,?!':\n",
    "            text = text.replace(' ' + symbol, symbol)\n",
    "        if text.startswith('_bos_'):\n",
    "            text = text[5:]\n",
    "        if text.endswith('_eos_'):\n",
    "            text = text[:-5]\n",
    "        return text.strip()\n",
    "    \n",
    "    def fit(self, x_train, y_train):\n",
    "        raw_counts = Counter()\n",
    "        replace_counts = Counter()\n",
    "        \n",
    "        for i in trange(len(x_train)):\n",
    "            xx, yy = x_train[i], y_train[i]\n",
    "            xx, yy = self.tokenize(xx), self.tokenize(yy)\n",
    "            alignment = needleman_wunsch(xx, yy)\n",
    "            ixx, iyy = list(zip(*alignment))\n",
    "            for gram_size in range(1, self.max_n + 1):\n",
    "                for start in range(len(ixx) - gram_size + 1):\n",
    "                    xgram = tuple([xx[c] for c in ixx[start: start + gram_size] if c is not None])\n",
    "                    ygram = tuple([yy[c] for c in iyy[start: start + gram_size] if c is not None])\n",
    "                    if xgram:\n",
    "                        xg, yg = ' '.join([''] + list(xgram) + ['']), ' '.join([''] + list(ygram) + [''])\n",
    "                        raw_counts[xg] += 1\n",
    "                        if xgram != ygram:\n",
    "                            replace_counts[(xg, yg)] += 1\n",
    "    \n",
    "        self.replace_proba = defaultdict(list)\n",
    "        self.replaced_tuples = dict()\n",
    "\n",
    "        for pair, n_sub in replace_counts.most_common():\n",
    "            if n_sub >= self.min_n:\n",
    "                xx, yy = pair\n",
    "                pr = n_sub / (self.smooth_n + raw_counts[xx])\n",
    "                if pr >= self.min_p:\n",
    "                    self.replace_proba[xx].append([yy, pr])\n",
    "                    self.replaced_tuples[tuple(xx.strip().split())] = raw_counts[xx]\n",
    "\n",
    "        for k, v in self.replace_proba.items():\n",
    "            tot = sum(p for r, p in v)\n",
    "            if tot < 1:\n",
    "                v.append([k, 1 - tot])\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform_one(self, text, n_out=None, temperature=None):\n",
    "        xx = self.tokenize(text)\n",
    "        found_grams = []\n",
    "        for gram_size in range(1, self.max_n + 1):\n",
    "            for start in range(len(xx) - gram_size + 1):\n",
    "                xgram = tuple([c for c in xx[start: start + gram_size] if c is not None])\n",
    "                if xgram and xgram in self.replaced_tuples:\n",
    "                    found_grams.append((xgram, self.replaced_tuples[xgram], len(xgram)))\n",
    "        found_grams = sorted(found_grams, key=lambda x: (x[2], x[1]), reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for i in range(n_out or 1):\n",
    "            untext = ' '.join([''] + xx + [''])\n",
    "            for gram, gn, gl in found_grams:\n",
    "                gram_text = ' '.join([''] + list(gram) + [''])\n",
    "                reps, ww = zip(*self.replace_proba[gram_text])\n",
    "                if not temperature:\n",
    "                    chosen_rep = list(reps)[np.argmax(ww)]\n",
    "                else: # chose randomly\n",
    "                    weights = [w ** (1 / temperature) for w in ww]\n",
    "                    chosen_rep = random.choices(list(reps), weights=weights)[0]\n",
    "                untext = untext.replace(gram_text, chosen_rep)\n",
    "            results.append(self.detokenize(untext))\n",
    "        if not n_out:\n",
    "            return results[0]\n",
    "        return results\n",
    "    \n",
    "    def transform(self, texts, n_out=None, temperature=None):\n",
    "        return [self.transform_one(text, n_out=n_out, temperature=temperature) for text in tqdm(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bb39c5b27144d9ab3b895f0742b1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4825 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<__main__.TextReplacer at 0x7f2acd0e8c10>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer = TextReplacer(min_n=3)\n",
    "replacer.fit(train.toxic_en.tolist(), train.neutral_en.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0aa0984f2414f08936373f78c87d684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/961 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "64.33395440698264"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrfpp.corpus_score(replacer.transform(val.toxic_en), [val.neutral_en.tolist()]).score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(\n",
    "    text, model, tokenizer, n=None, max_length=\"auto\", beams=5,\n",
    "):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    if max_length == \"auto\":\n",
    "        max_length = inputs.shape[1] + 10\n",
    "\n",
    "    result = model.generate(\n",
    "        inputs,\n",
    "        num_return_sequences=n or 1,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        repetition_penalty=10.0,\n",
    "        max_length=max_length,\n",
    "        min_length=int(0.5 * max_length),\n",
    "        num_beams=beams,\n",
    "        #forced_bos_token_id=tokenizer.lang_code_to_id[tokenizer.tgt_lang],\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. or the loud ass one - thousand ton beast roaring towards you howling its horn .',\n",
       " 'mandated  and \" right fucking now \" would be good .',\n",
       " '* neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .',\n",
       " '* well shit , cunt shot himself .',\n",
       " \"&gt i wouldn 't care how ignorant you are in you weren 't pretending to know shit .\"]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/english_data/test_toxic_parallel.txt', 'r') as f:\n",
    "    test_inputs = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "test_inputs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune mBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'facebook/mbart-large-50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_model)# .cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, padding=True)\n",
    "    labels = tokenizer(examples[\"target\"], padding=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b9b7f585a047a1b87e51f13d9e1406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff6de8258ea4b96a0ff427577ea7660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_data = raw_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/home/dale/models/detox-parallel/translate-ru2en-full-mbart\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=1, # 8 is too much \n",
    "    weight_decay=1e-5,\n",
    "    max_steps=10_000,\n",
    "    learning_rate=1e-5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=1,\n",
    "    eval_steps=500, \n",
    "    save_steps=500,\n",
    "    logging_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    # trying to save memory: see https://huggingface.co/docs/transformers/performance\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adafactor\",\n",
    "    gradient_accumulation_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tok_data[\"train\"],\n",
    "    eval_dataset=tok_data[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 4825\n",
      "  Num Epochs = 17\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10000\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:09:24, Epoch 16/17]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.571500</td>\n",
       "      <td>0.384305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.414300</td>\n",
       "      <td>0.348551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.350200</td>\n",
       "      <td>0.350092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.287800</td>\n",
       "      <td>0.378403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.246300</td>\n",
       "      <td>0.376747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.200800</td>\n",
       "      <td>0.374015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.161300</td>\n",
       "      <td>0.405448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.439400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.115300</td>\n",
       "      <td>0.459208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.097200</td>\n",
       "      <td>0.493408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.085400</td>\n",
       "      <td>0.511145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.493166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.516707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.533217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.539263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.042800</td>\n",
       "      <td>0.553437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.039800</td>\n",
       "      <td>0.560322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.557832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.565602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.567903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-1000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-1000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-1500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-1500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-2000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-2000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-2500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-2500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-3000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-3000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-2500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-3500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-3500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-4000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-4000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-3500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-4500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-4500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-4000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-5000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-5000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-4500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-5500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-5500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-6000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-6000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-5500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-6500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-6500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-6000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-7000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-7000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-6500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-7500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-7500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-7000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-8000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-8000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-7500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-8500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-8500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-8000] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-9000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-9000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-8500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-9500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-9500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-9000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-10000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-10000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-9500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/dale/models/detox-parallel/translate-ru2en-full-mbart/checkpoint-1000 (score: 0.34855106472969055).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10000, training_loss=0.2542056949615479, metrics={'train_runtime': 4164.9986, 'train_samples_per_second': 19.208, 'train_steps_per_second': 2.401, 'total_flos': 1.0560774671204352e+16, 'train_loss': 0.2542056949615479, 'epoch': 16.56})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2adaf60b0a0e4cfcb6d1bd6bf6ac98a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/961 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.12601361802126\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "model.eval()\n",
    "for text in tqdm(val.toxic_en):\n",
    "    with torch.inference_mode():\n",
    "        out = tokenizer.decode(\n",
    "            model.generate(**tokenizer(text, return_tensors='pt').to(model.device), num_beams=5, max_length=256)[0], \n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        preds.append(out)\n",
    "        \n",
    "print(chrfpp.corpus_score(preds, [val.neutral_en.tolist()]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5058                                                                                         what kind of scum from the duma offered old people pasta\n",
       "5059                                                                                                               pizdobol you don't touch my mother\n",
       "5060                                                                                                                   lucky idiots also justify them\n",
       "5061                       and this one climbs into deputies, well, if not an alcoholic, then a faggot would be a disgrace to sing his fucking songs.\n",
       "5062    the creatures are not people let her go she will be alive and if you don't need her don't start a dog I love dogs there is a dog in the house\n",
       "Name: toxic_en, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.toxic_en[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What kind of person from the duma offered old people pasta',\n",
       " \"You don't touch my mother\",\n",
       " 'Lucky people also justify them',\n",
       " 'and this one climbs into deputies, well, if not an alcoholic, then it would be a disgrace to sing his songs.',\n",
       " \"People let her go she will be alive and if you don't need her don't start a dog I love dogs there is a dog in the house\"]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cbfed090b04bc2afa94cab6467ba6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs = [paraphrase(text, model, tokenizer) for text in tqdm(test_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/translate-train_yandex-full-mbart/results_en.txt', 'w') as f:\n",
    "    for text in test_outputs:\n",
    "        f.write(text+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune mBART with BOTH English (original) and Russian (translated) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'target'],\n",
       "        num_rows: 9650\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['text', 'target'],\n",
       "        num_rows: 961\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = DatasetDict({\n",
    "    'train': Dataset.from_dict({\n",
    "        'text': train.toxic_en.tolist() + train.toxic_comment.tolist(), \n",
    "        'target': train.neutral_en.tolist() + train.neutral_comment.tolist()}),\n",
    "    'dev': Dataset.from_dict({'text': val.toxic_en, 'target': val.neutral_en}),\n",
    "})\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"don't lie, miserable, 100 thousand came to white (not a fact! yellow house) in Washington!\",\n",
       " 'target': '100,000 people came to the White House in Washington.'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(raw_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'facebook/mbart-large-50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_model)# .cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, padding=True)\n",
    "    labels = tokenizer(examples[\"target\"], padding=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61b5a5cb2e8433d8d2874d23f796403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0136a79dfb4640a6baf74ac3f807573e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_data = raw_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=1, # 8 is too much \n",
    "    weight_decay=1e-5,\n",
    "    max_steps=10_000,\n",
    "    learning_rate=1e-5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=1,\n",
    "    eval_steps=500, \n",
    "    save_steps=500,\n",
    "    logging_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    # trying to save memory: see https://huggingface.co/docs/transformers/performance\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adafactor\",\n",
    "    gradient_accumulation_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tok_data[\"train\"],\n",
    "    eval_dataset=tok_data[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 9650\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10000\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:09:44, Epoch 8/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.740500</td>\n",
       "      <td>0.399959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.460200</td>\n",
       "      <td>0.362033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.377700</td>\n",
       "      <td>0.344698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.340100</td>\n",
       "      <td>0.337635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.340251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.341592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.344094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.228300</td>\n",
       "      <td>0.363490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.215900</td>\n",
       "      <td>0.353970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.379440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.377812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>0.382607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.145100</td>\n",
       "      <td>0.406286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>0.403805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.420204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.118700</td>\n",
       "      <td>0.426775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.120600</td>\n",
       "      <td>0.439401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.441627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.104600</td>\n",
       "      <td>0.442464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.444633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-1000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-1000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-500] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-1500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-1500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-2000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-2000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-2500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-2500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-3000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-3000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-2500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-3500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-3500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-4000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-4000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-3500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-4500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-4500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-4000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-5000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-5000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-4500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-5500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-5500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-6000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-6000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-5500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-6500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-6500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-6000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-7000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-7000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-6500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-7500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-7500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-7000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-8000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-8000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-7500] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-8500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-8500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-8000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-9000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-9000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-8500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-9500\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-9500/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-9000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: target, text. If target, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 961\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-10000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-10000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-9500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/dale/models/detox-parallel/translate-ru2en_yandex-full_bilingual-mbart/checkpoint-2000 (score: 0.3376348912715912).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10000, training_loss=0.33632232246398924, metrics={'train_runtime': 4184.5054, 'train_samples_per_second': 19.118, 'train_steps_per_second': 2.39, 'total_flos': 1.0677778515296256e+16, 'train_loss': 0.33632232246398924, 'epoch': 8.29})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d811acf0a24886bc3651f99300fcfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/961 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "model.eval()\n",
    "for text in tqdm(val.toxic_en):\n",
    "    with torch.inference_mode():\n",
    "        out = tokenizer.decode(\n",
    "            model.generate(**tokenizer(text, return_tensors='pt').to(model.device), num_beams=5, max_length=256)[0], \n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        preds.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.54680919764111\n"
     ]
    }
   ],
   "source": [
    "print(chrfpp.corpus_score(preds, [val.neutral_en.tolist()]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5058                                                                                         what kind of scum from the duma offered old people pasta\n",
       "5059                                                                                                               pizdobol you don't touch my mother\n",
       "5060                                                                                                                   lucky idiots also justify them\n",
       "5061                       and this one climbs into deputies, well, if not an alcoholic, then a faggot would be a disgrace to sing his fucking songs.\n",
       "5062    the creatures are not people let her go she will be alive and if you don't need her don't start a dog I love dogs there is a dog in the house\n",
       "Name: toxic_en, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.toxic_en[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what kind of person from the duma offered old people pasta',\n",
       " \"You don't touch my mother\",\n",
       " 'Lucky people also justify their actions',\n",
       " 'And this one climbs into deputies, well, if not an alcoholic, then it would be a disgrace to sing his songs',\n",
       " \"Let her go she will be alive and if you don't need her don't start a dog I love dogs there is a dog in the house\"]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4aec3a5fe647cabeeab0cc11324616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs = [paraphrase(text, model, tokenizer) for text in tqdm(test_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/translate-train_yandex-full_bilingual-mbart/results_en.txt', 'w') as f:\n",
    "    for text in test_outputs:\n",
    "        f.write(text+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /home/dale/projects/paradetox2/evaluation_detox\n",
    "python metric.py --inputs /home/dale/projects/multilingual_detox/data/english_data/test_toxic_parallel.txt \\\n",
    "    --preds /home/dale/projects/multilingual_detox/results/translate-train_yandex-full-mbart/results_en.txt \\\n",
    "    --cola_classifier_path /home/dale/models/cola_classifier_fairseq \\\n",
    "    --wieting_model_path /home/dale/models/wieting_similarity/sim.pt \\\n",
    "    --wieting_tokenizer_path /home/dale/models/wieting_similarity/sim.sp.30k.model \\\n",
    "    --batch_size 32\n",
    "cat results.md\n",
    "```\n",
    "\n",
    "| Model | ACC | EMB_SIM | SIM | CharPPL | TokenPPL | FL | GM | J | BLEU |\n",
    "| ----- | --- | ------- | --- | ------- | -------- | -- | -- | - | ---- |\n",
    "results_en.txt|0.6528|0.8848|0.8660|6.3930|146.1058|0.8823|0.0000|0.4681|0.7305|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /home/dale/projects/paradetox2/evaluation_detox\n",
    "python metric.py --inputs /home/dale/projects/multilingual_detox/data/english_data/test_toxic_parallel.txt \\\n",
    "    --preds /home/dale/projects/multilingual_detox/results/translate-train_yandex-full_bilingual-mbart/results_en.txt \\\n",
    "    --cola_classifier_path /home/dale/models/cola_classifier_fairseq \\\n",
    "    --wieting_model_path /home/dale/models/wieting_similarity/sim.pt \\\n",
    "    --wieting_tokenizer_path /home/dale/models/wieting_similarity/sim.sp.30k.model \\\n",
    "    --batch_size 32\n",
    "cat results.md\n",
    "```\n",
    "\n",
    "| Model | ACC | EMB_SIM | SIM | CharPPL | TokenPPL | FL | GM | J | BLEU |\n",
    "| ----- | --- | ------- | --- | ------- | -------- | -- | -- | - | ---- |\n",
    "results_en.txt|0.7765|0.8657|0.8229|5.9643|97.8173|0.9031|6.4201|0.5566|0.6886|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3k",
   "language": "python",
   "name": "p3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
