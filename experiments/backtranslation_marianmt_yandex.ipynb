{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer English to Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/home/dale/models/detox-parallel/mbart_5000_EN'\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/mbart-large-50')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(\n",
    "    text, model, tokenizer, n=None, max_length=\"auto\", beams=5,\n",
    "):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    if max_length == \"auto\":\n",
    "        max_length = inputs.shape[1] + 10\n",
    "\n",
    "    result = model.generate(\n",
    "        inputs,\n",
    "        num_return_sequences=n or 1,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        repetition_penalty=10.0,\n",
    "        max_length=max_length,\n",
    "        min_length=int(0.5 * max_length),\n",
    "        num_beams=beams,\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't like this.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase('fuck this', model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MariaNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"facebook/wmt19-en-ru\"\n",
    "model_name1 = 'Helsinki-NLP/opus-mt-en-ru'\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
    "model1 = AutoModelForSeq2SeqLM.from_pretrained(model_name1).to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name2 = 'Helsinki-NLP/opus-mt-ru-en'\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "model2 = AutoModelForSeq2SeqLM.from_pretrained(model_name2).to('cuda:1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def detokenize(text):\n",
    "    for symbol in \",.?'!\":\n",
    "        text = text.replace(' ' + symbol, symbol)\n",
    "    return text\n",
    "\n",
    "\n",
    "def translate(texts, model, tokenizer, num_beams=5, max_length='auto', repetition_penalty=16.0, **kwargs):\n",
    "    sentences = []\n",
    "    sent_sizes = []\n",
    "    for text in texts:\n",
    "        sents = sent_tokenize(detokenize(text))\n",
    "        sentences.extend(sents)\n",
    "        sent_sizes.append(len(sents))\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    if max_length == 'auto': \n",
    "        max_length = int(10 + inputs.input_ids.shape[1] * 1.5)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs.to(model.device), num_beams=num_beams, repetition_penalty=repetition_penalty, **kwargs)\n",
    "    out_sents = [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]\n",
    "    out_texts = []\n",
    "    cursor = 0\n",
    "    for size in sent_sizes:\n",
    "        out_texts.append(' '.join(out_sents[cursor: cursor+size]))\n",
    "        cursor += size\n",
    "    return out_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/russian_data/test.tsv', sep='\\t')\n",
    "test_inputs = test_data[\"toxic_comment\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /home/dale/models/detox-parallel\n",
    "mkdir mbart_5000_EN\n",
    "cd mbart_5000_EN\n",
    "scp dale@nlp2:/home/moskovskiy/workspace/nlp/multilingual_tst/mbarts/mbart_5000_EN/* .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8  # 15 minutes to translate the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['укропидорг лавринов! общайся лучше с ией - так хоть на человека похож!',\n",
       " 'терпеть не могу самок, которые за мужской хер готовы родину продать',\n",
       " 'фсё для таких пидарасоф как ты']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fd89549ffb4f6099c6c14a03db3e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_src_en_marianmt = []\n",
    "texts = test_inputs\n",
    "for i in trange(0, len(texts), batch_size):\n",
    "    test_src_en_marianmt.extend(translate(texts[i:i+batch_size], model2, tokenizer2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The laurin crimp! You'd better talk to the Yi, at least you look like a man!\",\n",
       " \"I hate females who are willing to sell their homeland for a man's dick.\",\n",
       " 'For fuckers like you.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_src_en_marianmt[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d04f06f1964f0fb3750b0304af1922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_trg_en_marianmt = []\n",
    "for text in tqdm(test_src_en_marianmt):\n",
    "    test_trg_en_marianmt.append(paraphrase(text, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"You'd better talk to the Yi, at least you look like a man!\",\n",
       " \"I hate females who are willing to sell their homeland for a man's money.\",\n",
       " 'For unsuitable people like you.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trg_en_marianmt[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7086dc92c2447b81eab5fabd652c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_trg_ru_marianmt = []\n",
    "texts = test_trg_en_marianmt\n",
    "for i in trange(0, len(texts), batch_size):\n",
    "    test_trg_ru_marianmt.extend(translate(texts[i:i+batch_size], model1, tokenizer1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Тебе лучше поговорить с И, по крайней мере ты выглядишь как мужчина!',\n",
       " 'Я ненавижу женщин, которые готовы продать свою родину за мужские деньги.',\n",
       " 'Для неподходящих людей вроде тебя.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trg_ru_marianmt[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/backtranslate-marianmt/'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'translated_ru2en.txt', 'w') as f:\n",
    "    for line in test_src_en_marianmt:\n",
    "        f.write(line + '\\n')\n",
    "        \n",
    "with open(path + 'translated_ru2en_paraphrased.txt', 'w') as f:\n",
    "    for line in test_trg_en_marianmt:\n",
    "        f.write(line + '\\n')\n",
    "        \n",
    "with open(path + 'results_ru.txt', 'w') as f:\n",
    "    for line in test_trg_ru_marianmt:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yandex translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_by_symbol(text, symbol=',', max_len=400):\n",
    "    if len(text) <= max_len:\n",
    "        return [text]\n",
    "    chunks = re.split(symbol, text)\n",
    "    if len(chunks) <= 1:\n",
    "        return [text]\n",
    "    result = [chunks[0]]\n",
    "    for chunk in chunks[1:]:\n",
    "        result.append(symbol)\n",
    "        result.append(chunk)\n",
    "    return result\n",
    "\n",
    "def join_texts(texts, max_len=400):\n",
    "    result = []\n",
    "    prev_text = ''\n",
    "    for text in texts:\n",
    "        if len(text) + len(prev_text) > max_len:\n",
    "            result.append(prev_text)\n",
    "            prev_text = text\n",
    "        else:\n",
    "            prev_text = prev_text + text\n",
    "    result.append(prev_text)\n",
    "    return result\n",
    "\n",
    "def hard_split(text, max_len=300):\n",
    "    parts = list(sent_tokenize(text))\n",
    "    result = []\n",
    "    for part in parts:\n",
    "        chunks = [part]\n",
    "        for symbol in [',', '-', ' ']:\n",
    "            chunks = [c2 for c in chunks for c2 in split_by_symbol(c, symbol, max_len=max_len)]\n",
    "        result.extend(chunks)\n",
    "    result = join_texts(result, max_len=max_len)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to obtain a fresh SID:\n",
    "* go to translate.yandex.ru\n",
    "* open the \"network\" panel of the developers console\n",
    "* enter any text in the translation form\n",
    "* find the request to \"https://translate.yandex.net/api/v1/tr.json/translate\" and copy its first parameter (\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "SID = '7d9e19fd.62960895.db4a787d.74722d74657874-8-0'\n",
    "\n",
    "def translate_yandex(search_str, direction='en-ru', full_response=False):\n",
    "    try:\n",
    "        url = f'https://translate.yandex.net/api/v1/tr.json/translate?id={SID}&srv=tr-text&lang={direction}&reason=auto&format=text'\n",
    "\n",
    "        post_header = {}\n",
    "        post_header['Accept'] = '*/*'\n",
    "        post_header['Accept-Encoding'] = 'gzip, deflate'\n",
    "        post_header['Accept-Language'] = 'en-US,en;q=0.9'\n",
    "        post_header['Cache-Control'] = 'no-cache'\n",
    "        post_header['Connection'] = 'keep-alive'\n",
    "        post_header['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "        post_header['Host'] = 'translate.yandex.com'\n",
    "        post_header['Referer'] = 'https://translate.yandex.com/'\n",
    "        post_header['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 YaBrowser/21.8.2.383 Yowser/2.5 Safari/537.36'\n",
    "\n",
    "\n",
    "        data_payload = {'text': search_str, 'options': '4'}\n",
    "\n",
    "        resp = requests.post(url, headers=post_header, data=data_payload).json()\n",
    "        #print(resp)\n",
    "        if full_response:\n",
    "            return resp\n",
    "\n",
    "        if resp.get('message') == 'The text size exceeds the maximum':\n",
    "            parts = hard_split(search_str)\n",
    "            if len(parts) > 1:\n",
    "                return 200, ' '.join([translate_yandex(part, dir=dir)[1] for part in parts])\n",
    "\n",
    "        return resp['code'], resp['text'][0]\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        return 0, ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['укропидорг лавринов! общайся лучше с ией - так хоть на человека похож!',\n",
       " 'терпеть не могу самок, которые за мужской хер готовы родину продать',\n",
       " 'фсё для таких пидарасоф как ты']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02bbcb3a1954836ac9042ff3b29ef49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_src_en_yandex = []\n",
    "for text in tqdm(test_inputs):\n",
    "    test_src_en_yandex.append(translate_yandex(text, 'ru-en')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ukropidorg lavrinov! communicate better with ai - so at least you look like a person!',\n",
       " \"I can't stand females who are ready to sell their homeland for a man's dick\",\n",
       " 'fse for faggots like you']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_src_en_yandex[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb6166171cd430aafd990b609e299b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_trg_en_yandex = []\n",
    "for text in tqdm(test_src_en_yandex):\n",
    "    test_trg_en_yandex.append(paraphrase(text, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lavrinov! communicate better with ai - so at least you look like a person!',\n",
       " \"I can't stand females who are ready to sell their homeland for a man's.\",\n",
       " 'fse for those who dislike it']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trg_en_yandex[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60ccff9e4f64d75a5d1577bb7e1f152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_trg_ru_yandex = []\n",
    "for text in tqdm(test_trg_en_yandex):\n",
    "    test_trg_ru_yandex.append(translate_yandex(text, 'en-ru')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['лавринов! лучше общайтесь с искусственным интеллектом - так вы хотя бы будете выглядеть как человек!',\n",
       " 'Я терпеть не могу женщин, которые готовы продать свою родину за мужскую.',\n",
       " 'fse для тех, кому это не нравится']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trg_ru_yandex[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/backtranslate-yandex/'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'translated_ru2en.txt', 'w') as f:\n",
    "    for line in test_src_en_yandex:\n",
    "        f.write(line + '\\n')\n",
    "        \n",
    "with open(path + 'translated_ru2en_paraphrased.txt', 'w') as f:\n",
    "    for line in test_trg_en_yandex:\n",
    "        f.write(line + '\\n')\n",
    "        \n",
    "with open(path + 'results_ru.txt', 'w') as f:\n",
    "    for line in test_trg_ru_yandex:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python evaluate_ru.py \\\n",
    "    --result_filename scores \\\n",
    "    --input_dir results/backtranslate-marianmt \\\n",
    "    --output_dir results\n",
    "    \n",
    "Style accuracy:       0.6040729880332947\n",
    "Meaning preservation: 0.722809910774231\n",
    "Joint fluency:        -0.22462719678878784\n",
    "Joint score:          -0.08952497690916061\n",
    "Scores after calibration:\n",
    "Style accuracy:       0.6436656713485718\n",
    "Meaning preservation: 0.5851079821586609\n",
    "Joint fluency:        0.7416787147521973\n",
    "Joint score:          0.28991788625717163\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python evaluate_ru.py \\\n",
    "    --result_filename scores \\\n",
    "    --input_dir results/backtranslate-yandex \\\n",
    "    --output_dir results\n",
    "\n",
    "Style accuracy:       0.6853741407394409\n",
    "Meaning preservation: 0.8279372453689575\n",
    "Joint fluency:        -0.1530541628599167\n",
    "Joint score:          -0.085952028632164\n",
    "Scores after calibration:\n",
    "Style accuracy:       0.7168368101119995\n",
    "Meaning preservation: 0.7420499324798584\n",
    "Joint fluency:        0.8239876627922058\n",
    "Joint score:          0.4313623607158661\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Russian to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/home/dale/models/detox-parallel/mbart_5000_RU'\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/mbart-large-50')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(\n",
    "    text, model, tokenizer, n=None, max_length=\"auto\", beams=5,\n",
    "):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    if max_length == \"auto\":\n",
    "        max_length = inputs.shape[1] + 10\n",
    "\n",
    "    result = model.generate(\n",
    "        inputs,\n",
    "        num_return_sequences=n or 1,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        repetition_penalty=10.0,\n",
    "        max_length=max_length,\n",
    "        min_length=int(0.5 * max_length),\n",
    "        num_beams=beams,\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Мне все равно на это всё!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase('В пизду это всё!', model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MariaNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"facebook/wmt19-en-ru\"\n",
    "model_name1 = 'Helsinki-NLP/opus-mt-en-ru'\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
    "model1 = AutoModelForSeq2SeqLM.from_pretrained(model_name1).to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name2 = 'Helsinki-NLP/opus-mt-ru-en'\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "model2 = AutoModelForSeq2SeqLM.from_pretrained(model_name2).to('cuda:1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def detokenize(text):\n",
    "    for symbol in \",.?'!\":\n",
    "        text = text.replace(' ' + symbol, symbol)\n",
    "    return text\n",
    "\n",
    "\n",
    "def translate(texts, model, tokenizer, num_beams=5, max_length='auto', repetition_penalty=16.0, **kwargs):\n",
    "    sentences = []\n",
    "    sent_sizes = []\n",
    "    for text in texts:\n",
    "        sents = sent_tokenize(detokenize(text))\n",
    "        sentences.extend(sents)\n",
    "        sent_sizes.append(len(sents))\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    if max_length == 'auto': \n",
    "        max_length = int(10 + inputs.input_ids.shape[1] * 1.5)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs.to(model.device), num_beams=num_beams, repetition_penalty=repetition_penalty, **kwargs)\n",
    "    out_sents = [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]\n",
    "    out_texts = []\n",
    "    cursor = 0\n",
    "    for size in sent_sizes:\n",
    "        out_texts.append(' '.join(out_sents[cursor: cursor+size]))\n",
    "        cursor += size\n",
    "    return out_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/english_data/test_toxic_parallel.txt', 'r') as f:\n",
    "    test_inputs = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. or the loud ass one - thousand ton beast roaring towards you howling its horn .',\n",
       " 'mandated  and \" right fucking now \" would be good .',\n",
       " '* neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8  # 15 minutes to translate the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58563ffca434622af15677fba0ddad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_src_ru_marianmt = []\n",
    "texts = test_inputs\n",
    "for i in trange(0, len(texts), batch_size):\n",
    "    test_src_ru_marianmt.extend(translate(texts[i:i+batch_size], model1, tokenizer1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. или громкая задница тысяча тонн зверя, рычащего на тебя и вопящего его рога.',\n",
       " 'Это было бы неплохо.',\n",
       " 'Моим коллегам тоже было насрать, когда пришло время бросить Митч. Ух.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_src_ru_marianmt[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5939a1cc0ab46f097d01646a90b9e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_trg_ru_marianmt = []\n",
    "for text in tqdm(test_src_ru_marianmt):\n",
    "    test_trg_ru_marianmt.append(paraphrase(text, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['или громкая задница тысяча тонн зверя, рычащего на тебя и вопящего его рога.',\n",
       " 'Это было бы неплохо..',\n",
       " 'Моим коллегам тоже было насрать, когда пришло время бросить Митч. Ух.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trg_ru_marianmt[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a3bac1b52345dfaad4073891a87ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_trg_en_marianmt = []\n",
    "texts = test_trg_ru_marianmt\n",
    "for i in trange(0, len(texts), batch_size):\n",
    "    test_trg_en_marianmt.extend(translate(texts[i:i+batch_size], model2, tokenizer2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Or the loud ass of a thousand tons of beast roaring at you and crying out his horn.',\n",
       " 'That would be nice.',\n",
       " \"My colleagues didn't give a shit when it was time to leave Mitch. Wow.\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trg_en_marianmt[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/backtranslate-marianmt/'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'translated_en2ru.txt', 'w') as f:\n",
    "    for line in test_src_ru_marianmt:\n",
    "        f.write(line + '\\n')\n",
    "        \n",
    "with open(path + 'translated_en2ru_paraphrased.txt', 'w') as f:\n",
    "    for line in test_trg_ru_marianmt:\n",
    "        f.write(line + '\\n')\n",
    "        \n",
    "with open(path + 'results_en.txt', 'w') as f:\n",
    "    for line in test_trg_en_marianmt:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yandex translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_by_symbol(text, symbol=',', max_len=400):\n",
    "    if len(text) <= max_len:\n",
    "        return [text]\n",
    "    chunks = re.split(symbol, text)\n",
    "    if len(chunks) <= 1:\n",
    "        return [text]\n",
    "    result = [chunks[0]]\n",
    "    for chunk in chunks[1:]:\n",
    "        result.append(symbol)\n",
    "        result.append(chunk)\n",
    "    return result\n",
    "\n",
    "def join_texts(texts, max_len=400):\n",
    "    result = []\n",
    "    prev_text = ''\n",
    "    for text in texts:\n",
    "        if len(text) + len(prev_text) > max_len:\n",
    "            result.append(prev_text)\n",
    "            prev_text = text\n",
    "        else:\n",
    "            prev_text = prev_text + text\n",
    "    result.append(prev_text)\n",
    "    return result\n",
    "\n",
    "def hard_split(text, max_len=300):\n",
    "    parts = list(sent_tokenize(text))\n",
    "    result = []\n",
    "    for part in parts:\n",
    "        chunks = [part]\n",
    "        for symbol in [',', '-', ' ']:\n",
    "            chunks = [c2 for c in chunks for c2 in split_by_symbol(c, symbol, max_len=max_len)]\n",
    "        result.extend(chunks)\n",
    "    result = join_texts(result, max_len=max_len)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to obtain a fresh SID:\n",
    "* go to translate.yandex.ru\n",
    "* open the \"network\" panel of the developers console\n",
    "* enter any text in the translation form\n",
    "* find the request to \"https://translate.yandex.net/api/v1/tr.json/translate\" and copy its first parameter (\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "SID = '7d9e19fd.62960895.db4a787d.74722d74657874-8-0'\n",
    "\n",
    "def translate_yandex(search_str, direction='en-ru', full_response=False):\n",
    "    try:\n",
    "        url = f'https://translate.yandex.net/api/v1/tr.json/translate?id={SID}&srv=tr-text&lang={direction}&reason=auto&format=text'\n",
    "\n",
    "        post_header = {}\n",
    "        post_header['Accept'] = '*/*'\n",
    "        post_header['Accept-Encoding'] = 'gzip, deflate'\n",
    "        post_header['Accept-Language'] = 'en-US,en;q=0.9'\n",
    "        post_header['Cache-Control'] = 'no-cache'\n",
    "        post_header['Connection'] = 'keep-alive'\n",
    "        post_header['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "        post_header['Host'] = 'translate.yandex.com'\n",
    "        post_header['Referer'] = 'https://translate.yandex.com/'\n",
    "        post_header['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 YaBrowser/21.8.2.383 Yowser/2.5 Safari/537.36'\n",
    "\n",
    "\n",
    "        data_payload = {'text': search_str, 'options': '4'}\n",
    "\n",
    "        resp = requests.post(url, headers=post_header, data=data_payload).json()\n",
    "        #print(resp)\n",
    "        if full_response:\n",
    "            return resp\n",
    "\n",
    "        if resp.get('message') == 'The text size exceeds the maximum':\n",
    "            parts = hard_split(search_str)\n",
    "            if len(parts) > 1:\n",
    "                return 200, ' '.join([translate_yandex(part, dir=dir)[1] for part in parts])\n",
    "\n",
    "        return resp['code'], resp['text'][0]\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        return 0, ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 'Fuck it all!')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_yandex('В пизду это всё!', 'ru-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. or the loud ass one - thousand ton beast roaring towards you howling its horn .',\n",
       " 'mandated  and \" right fucking now \" would be good .',\n",
       " '* neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4351bdebba464f99080d5f7cc29f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_src_ru_yandex = []\n",
    "for text in tqdm(test_inputs):\n",
    "    test_src_ru_yandex.append(translate_yandex(text, 'en-ru')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. или громкоголосый тысячетонный зверь , ревущий к вам , воя своим рогом .',\n",
       " '\" и \" прямо сейчас , блядь \" было бы неплохо .',\n",
       " '* никому * из моих коллег не было дела, когда пришло время бросить Митча. фу.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_src_ru_yandex[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd192a16a7f437d9f8fb56a149010f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_trg_ru_yandex = []\n",
    "for text in tqdm(test_src_ru_yandex):\n",
    "    test_trg_ru_yandex.append(paraphrase(text, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['или громкоголосый тысячетонный зверь, ревущий к вам, воя своим рогом',\n",
       " '\" и \" прямо сейчас \" было бы неплохо....',\n",
       " 'Никому из моих подчиненных не было дела, когда пришло время бросить Митча']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trg_ru_yandex[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5eeeb1d07a4b97bad5a89c892243da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_trg_en_yandex = []\n",
    "for text in tqdm(test_trg_ru_yandex):\n",
    "    test_trg_en_yandex.append(translate_yandex(text, 'ru-en')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['or a loud-voiced thousand-ton beast roaring towards you, howling with its horn',\n",
       " '\" and \\'right now\\' would be nice....',\n",
       " 'None of my subordinates cared when it was time to dump Mitch.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trg_en_yandex[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/backtranslate-yandex/'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'translated_en2ru.txt', 'w') as f:\n",
    "    for line in test_src_ru_yandex:\n",
    "        f.write(line + '\\n')\n",
    "        \n",
    "with open(path + 'translated_en2ru_paraphrased.txt', 'w') as f:\n",
    "    for line in test_trg_ru_yandex:\n",
    "        f.write(line + '\\n')\n",
    "        \n",
    "with open(path + 'results_en.txt', 'w') as f:\n",
    "    for line in test_trg_en_yandex:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://github.com/skoltech-nlp/paradetox/tree/main/evaluation_detox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /home/dale/projects/paradetox2/evaluation_detox\n",
    "python metric.py --inputs /home/dale/projects/multilingual_detox/data/english_data/test_toxic_parallel.txt \\\n",
    "    --preds /home/dale/projects/multilingual_detox/results/backtranslate-marianmt/results_en.txt \\\n",
    "    --cola_classifier_path /home/dale/models/cola_classifier_fairseq \\\n",
    "    --wieting_model_path /home/dale/models/wieting_similarity/sim.pt \\\n",
    "    --wieting_tokenizer_path /home/dale/models/wieting_similarity/sim.sp.30k.model \\\n",
    "    --batch_size 32\n",
    "cat results.md\n",
    "```\n",
    "\n",
    "| Model | ACC | EMB_SIM | SIM | CharPPL | TokenPPL | FL | GM | J | BLEU |\n",
    "| ----- | --- | ------- | --- | ------- | -------- | -- | -- | - | ---- |\n",
    "results_en.txt|0.6766|0.7574|0.7180|6.5078|86.1255|0.8942|0.0000|0.4116|0.4885|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python metric.py --inputs /home/dale/projects/multilingual_detox/data/english_data/test_toxic_parallel.txt \\\n",
    "    --preds /home/dale/projects/multilingual_detox/results/backtranslate-yandex/results_en.txt \\\n",
    "    --cola_classifier_path /home/dale/models/cola_classifier_fairseq \\\n",
    "    --wieting_model_path /home/dale/models/wieting_similarity/sim.pt \\\n",
    "    --wieting_tokenizer_path /home/dale/models/wieting_similarity/sim.sp.30k.model \\\n",
    "    --batch_size 32\n",
    "cat results.md\n",
    "```\n",
    "| Model | ACC | EMB_SIM | SIM | CharPPL | TokenPPL | FL | GM | J | BLEU |\n",
    "| ----- | --- | ------- | --- | ------- | -------- | -- | -- | - | ---- |\n",
    "results_en.txt|0.6662|0.7985|0.7741|6.1724|89.3471|0.9553|0.0000|0.4674|0.5799|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2M100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tried the https://huggingface.co/facebook/m2m100_1.2B model for machine translation, and the small (418M) model translated poorly in both directions. \n",
    "\n",
    "The medium one (1.2B) works decently (but not better than marianmt), and the large one (12B) just does not fit our hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf8157e68934ac799a4ca6078d32ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/909 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1171961002d4b75877ad441f57142b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d10aa9897694994867ae63037c199f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.54M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957433733e394fe49d0e5aaabdcc7c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a070bbeb8546848e1e4875e7a9dff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5280c865de624bdb94a96b6b1585381a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "hi_text = \"जीवन एक चॉकलेट बॉक्स की तरह है।\"\n",
    "chinese_text = \"生活就像一盒巧克力。\"\n",
    "\n",
    "model_name = 'facebook/m2m100_418M'\n",
    "model_name = 'facebook/m2m100_1.2B'\n",
    "\n",
    "\n",
    "model2 = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer2 = M2M100Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['La vie est comme une boîte de chocolat.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translate Hindi to French\n",
    "tokenizer2.src_lang = \"hi\"\n",
    "encoded_hi = tokenizer2(hi_text, return_tensors=\"pt\")\n",
    "generated_tokens = model2.generate(**encoded_hi, forced_bos_token_id=tokenizer2.get_lang_id(\"fr\"))\n",
    "tokenizer2.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "# => \"La vie est comme une boîte de chocolat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_m2m(texts, model, tokenizer, src, trg, num_beams=5, max_length='auto', repetition_penalty=16.0, **kwargs):\n",
    "    tokenizer2.src_lang = src\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "    if max_length == 'auto': \n",
    "        max_length = int(10 + inputs.input_ids.shape[1] * 1.5)\n",
    "    generated_tokens = model.generate(\n",
    "        **inputs, \n",
    "        forced_bos_token_id=tokenizer.get_lang_id(trg),\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_length,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Или громкий осел — тысяча тонн зверей, ругающихся к тебе и шумлящих своим рогом.']\n",
      "['«Правда и проклятие» было бы хорошо.']\n",
      "['* ни один из моих коллег не давал дерьма, когда пришло время отбросить митч. ugh.']\n"
     ]
    }
   ],
   "source": [
    "for text in ['. or the loud ass one - thousand ton beast roaring towards you howling its horn .',\n",
    " 'mandated  and \" right fucking now \" would be good .',\n",
    " '* neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .']:\n",
    "    print(translate_m2m(text, model2, tokenizer2, 'en', 'ru'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "marianmt:\n",
    "['. или громкая задница тысяча тонн зверя, рычащего на тебя и вопящего его рога.',\n",
    " 'Это было бы неплохо.',\n",
    " 'Моим коллегам тоже было насрать, когда пришло время бросить Митч. Ух.']\n",
    "yandex:\n",
    "['. или громкоголосый тысячетонный зверь , ревущий к вам , воя своим рогом .',\n",
    " '\" и \" прямо сейчас , блядь \" было бы неплохо .',\n",
    " '* никому * из моих коллег не было дела, когда пришло время бросить Митча. фу.']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ukropidog laurines! communicate better with IA - so at least it looks like a man!']\n",
      "['I can’t tolerate females who are willing to sell their homeland for a man.']\n",
      "['for peddlers like you.']\n"
     ]
    }
   ],
   "source": [
    "for text in ['укропидорг лавринов! общайся лучше с ией - так хоть на человека похож!',\n",
    " 'терпеть не могу самок, которые за мужской хер готовы родину продать',\n",
    " 'фсё для таких пидарасоф как ты']:\n",
    "    print(translate_m2m(text, model2, tokenizer2, 'ru', 'en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "marianmt:\n",
    "[\"The laurin crimp! You'd better talk to the Yi, at least you look like a man!\",\n",
    " \"I hate females who are willing to sell their homeland for a man's dick.\",\n",
    " 'For fuckers like you.']\n",
    "yandex:\n",
    "['ukropidorg lavrinov! communicate better with ai - so at least you look like a person!',\n",
    " \"I can't stand females who are ready to sell their homeland for a man's dick\",\n",
    " 'fse for faggots like you']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3k",
   "language": "python",
   "name": "p3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
