{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment the English training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/home/dale/models/detox-parallel/mbart_5000_EN'\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/mbart-large-50')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(\n",
    "    text, model, tokenizer, \n",
    "    n=None, \n",
    "    max_length=\"auto\", \n",
    "    min_length='auto',\n",
    "    beams=5,\n",
    "    repetition_penalty=16.0,\n",
    "):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    if max_length == \"auto\":\n",
    "        max_length = int(inputs.shape[1] * 1.2 + 10 ) \n",
    "    if min_length == 'auto':\n",
    "        for i in range(inputs.shape[1]):\n",
    "            min_length = i + 1\n",
    "            if (inputs[:, i] == tokenizer.eos_token_id).any().item():\n",
    "                break\n",
    "    min_length = int(min_length * 0.5 + 1) \n",
    "\n",
    "    result = model.generate(\n",
    "        inputs,\n",
    "        num_return_sequences=n or 1,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        num_beams=beams,\n",
    "    )\n",
    "    results = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "\n",
    "    if not n and isinstance(text, str):\n",
    "        return results[0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't like this.\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase('fuck this', model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82691, 6)\n",
      "['Unnamed: 0', 'sentence', 'dataset', 'toxicity_score', 'iteration', 'length']\n"
     ]
    }
   ],
   "source": [
    "twitter_unmarked = pd.read_csv('/home/dale/data/toxic_corpora/en-parallel/input_twitter_unmarked.csv')\n",
    "print(twitter_unmarked.shape)\n",
    "print(twitter_unmarked.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97968, 6)\n",
      "['Unnamed: 0', 'sentence', 'dataset', 'toxicity_score', 'iteration', 'length']\n"
     ]
    }
   ],
   "source": [
    "jigsaw_unmarked = pd.read_csv('/home/dale/data/toxic_corpora/en-parallel/input_jigsaw_unmarked.csv')\n",
    "print(jigsaw_unmarked.shape)\n",
    "print(jigsaw_unmarked.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232347, 6)\n",
      "['Unnamed: 0', 'sentence', 'dataset', 'toxicity_score', 'iteration', 'length']\n"
     ]
    }
   ],
   "source": [
    "reddit_unmarked = pd.read_csv('/home/dale/data/toxic_corpora/en-parallel/input_reddit_unmarked.csv')\n",
    "print(reddit_unmarked.shape)\n",
    "print(reddit_unmarked.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(413006, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>jigsaw</th>\n",
       "      <td>97968.0</td>\n",
       "      <td>0.984946</td>\n",
       "      <td>0.030152</td>\n",
       "      <td>0.800043</td>\n",
       "      <td>0.987282</td>\n",
       "      <td>0.996570</td>\n",
       "      <td>0.998868</td>\n",
       "      <td>0.999677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit</th>\n",
       "      <td>232347.0</td>\n",
       "      <td>0.991976</td>\n",
       "      <td>0.016702</td>\n",
       "      <td>0.800127</td>\n",
       "      <td>0.992619</td>\n",
       "      <td>0.996761</td>\n",
       "      <td>0.998475</td>\n",
       "      <td>0.999670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter</th>\n",
       "      <td>82691.0</td>\n",
       "      <td>0.991451</td>\n",
       "      <td>0.018476</td>\n",
       "      <td>0.800028</td>\n",
       "      <td>0.992899</td>\n",
       "      <td>0.996755</td>\n",
       "      <td>0.998302</td>\n",
       "      <td>0.999658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count      mean       std       min       25%       50%       75%  \\\n",
       "dataset                                                                         \n",
       "jigsaw    97968.0  0.984946  0.030152  0.800043  0.987282  0.996570  0.998868   \n",
       "reddit   232347.0  0.991976  0.016702  0.800127  0.992619  0.996761  0.998475   \n",
       "twitter   82691.0  0.991451  0.018476  0.800028  0.992899  0.996755  0.998302   \n",
       "\n",
       "              max  \n",
       "dataset            \n",
       "jigsaw   0.999677  \n",
       "reddit   0.999670  \n",
       "twitter  0.999658  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unmarked = pd.concat([twitter_unmarked, jigsaw_unmarked, reddit_unmarked]).reset_index(drop=True)\n",
    "print(all_unmarked.shape)\n",
    "all_unmarked.groupby('dataset').toxicity_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en_train = pd.read_csv('../data/english_data/en_data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/english_data/test_toxic_parallel.txt', 'r') as f:\n",
    "    test_inputs = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12598\n"
     ]
    }
   ],
   "source": [
    "already = set(test_inputs).union(set(detox_en_train.toxic_comment))\n",
    "print(len(already))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412540, 6)\n"
     ]
    }
   ],
   "source": [
    "all_unmarked = all_unmarked[all_unmarked.sentence.apply(lambda x: x not in already)]\n",
    "print(all_unmarked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210000, 6)\n"
     ]
    }
   ],
   "source": [
    "unmarked_balanced = all_unmarked.groupby('dataset').sample(70_000, random_state=1)\n",
    "print(unmarked_balanced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>dataset</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>iteration</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152519</th>\n",
       "      <td>26474</td>\n",
       "      <td>meeting with this lame duck .</td>\n",
       "      <td>jigsaw</td>\n",
       "      <td>0.994770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17365</th>\n",
       "      <td>83386</td>\n",
       "      <td>shit our spring break next week so imma be bac...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0.973974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143822</th>\n",
       "      <td>49367</td>\n",
       "      <td>step away from the computer and the ridiculous...</td>\n",
       "      <td>jigsaw</td>\n",
       "      <td>0.984254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                           sentence  \\\n",
       "152519       26474                      meeting with this lame duck .   \n",
       "17365        83386  shit our spring break next week so imma be bac...   \n",
       "143822       49367  step away from the computer and the ridiculous...   \n",
       "\n",
       "        dataset  toxicity_score  iteration  length  \n",
       "152519   jigsaw        0.994770        NaN       5  \n",
       "17365   twitter        0.973974        NaN      14  \n",
       "143822   jigsaw        0.984254        NaN      18  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmarked_balanced.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(text):\n",
    "    for symbol in \",.?'\":\n",
    "        text = text.replace(' ' + symbol, symbol)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d695f10a94459abd16493ba4fd9f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "detoxified_unmarked = []\n",
    "inputs = unmarked_balanced.sentence.tolist()\n",
    "for i in trange(0, len(inputs), bs):\n",
    "    try:\n",
    "        results = paraphrase(inputs[i:i+bs], model, tokenizer, beams=5)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        results = [None for t in inputs[i:i+bs]]\n",
    "    detoxified_unmarked.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmarked_balanced['neutral'] = detoxified_unmarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>dataset</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>iteration</th>\n",
       "      <th>length</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158871</th>\n",
       "      <td>77849</td>\n",
       "      <td>he boobytrapped his apartment , to kill anyone coming in .</td>\n",
       "      <td>jigsaw</td>\n",
       "      <td>0.926426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>He boobytrapped his apartment to kill anyone coming in.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133699</th>\n",
       "      <td>100197</td>\n",
       "      <td>perhaps the people of taif saw that this fellow was a con artist , thief and predator .</td>\n",
       "      <td>jigsaw</td>\n",
       "      <td>0.983709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>perhaps the people of taif saw that this fellow was a thief and predator.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137827</th>\n",
       "      <td>3917</td>\n",
       "      <td>and not , au revoir , do call again and leave more crap .</td>\n",
       "      <td>jigsaw</td>\n",
       "      <td>0.993696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>Do not call again and leave more problems.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  \\\n",
       "158871       77849   \n",
       "133699      100197   \n",
       "137827        3917   \n",
       "\n",
       "                                                                                       sentence  \\\n",
       "158871                               he boobytrapped his apartment , to kill anyone coming in .   \n",
       "133699  perhaps the people of taif saw that this fellow was a con artist , thief and predator .   \n",
       "137827                                and not , au revoir , do call again and leave more crap .   \n",
       "\n",
       "       dataset  toxicity_score  iteration  length  \\\n",
       "158871  jigsaw        0.926426        NaN       9   \n",
       "133699  jigsaw        0.983709        NaN      16   \n",
       "137827  jigsaw        0.993696        NaN      11   \n",
       "\n",
       "                                                                          neutral  \n",
       "158871                    He boobytrapped his apartment to kill anyone coming in.  \n",
       "133699  perhaps the people of taif saw that this fellow was a thief and predator.  \n",
       "137827                                 Do not call again and leave more problems.  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmarked_balanced.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmarked_balanced[['sentence', 'neutral', 'dataset']].to_csv('detox_en_augmented.tsv', index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate the English augmented data to Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en_augmented = pd.read_csv('detox_en_augmented.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# model_name = \"facebook/wmt19-en-ru\"\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-ru'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(text):\n",
    "    for symbol in \",.?'\":\n",
    "        text = text.replace(' ' + symbol, symbol)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def detokenize(text):\n",
    "    for symbol in \",.?'!\":\n",
    "        text = text.replace(' ' + symbol, symbol)\n",
    "    return text\n",
    "\n",
    "\n",
    "def translate(texts, model, tokenizer, num_beams=5, max_length='auto', repetition_penalty=16.0, **kwargs):\n",
    "    sentences = []\n",
    "    sent_sizes = []\n",
    "    for text in texts:\n",
    "        sents = sent_tokenize(detokenize(text))\n",
    "        sentences.extend(sents)\n",
    "        sent_sizes.append(len(sents))\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    if max_length == 'auto': \n",
    "        max_length = int(10 + inputs.input_ids.shape[1] * 1.5)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs.to(model.device), num_beams=num_beams, repetition_penalty=repetition_penalty, **kwargs)\n",
    "    out_sents = [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]\n",
    "    out_texts = []\n",
    "    cursor = 0\n",
    "    for size in sent_sizes:\n",
    "        out_texts.append(' '.join(out_sents[cursor: cursor+size]))\n",
    "        cursor += size\n",
    "    return out_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>neutral</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the demonization of the china and the chinese ...</td>\n",
       "      <td>the demonization of the china and the chinese ...</td>\n",
       "      <td>jigsaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hatred of those white supremacist nazis was pr...</td>\n",
       "      <td>hatred of those white supremacist nazis was pr...</td>\n",
       "      <td>jigsaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if a person is black , deadly force is general...</td>\n",
       "      <td>If a person is black, deadly force is generall...</td>\n",
       "      <td>jigsaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what a shitty upbringing that would be .</td>\n",
       "      <td>what a bad upbringing that would be.</td>\n",
       "      <td>jigsaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trudeau is nothing but corruptions .</td>\n",
       "      <td>Trudeau is nothing but corruptions.</td>\n",
       "      <td>jigsaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and dont give me that bs about those priests b...</td>\n",
       "      <td>and dont give me that bs about those priests b...</td>\n",
       "      <td>jigsaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>theyve been feeding this rubbish to their citi...</td>\n",
       "      <td>theyve been feeding this to their citizens sin...</td>\n",
       "      <td>jigsaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the reason white people have that advantage is...</td>\n",
       "      <td>the reason white people have that advantage is...</td>\n",
       "      <td>jigsaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>thats why i donr blame trump for the present m...</td>\n",
       "      <td>That's why I don't blame Trump for the present...</td>\n",
       "      <td>jigsaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>one thing harper was not is a lapdog that crav...</td>\n",
       "      <td>one thing harper was not is a lapdog that crav...</td>\n",
       "      <td>jigsaw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  the demonization of the china and the chinese ...   \n",
       "1  hatred of those white supremacist nazis was pr...   \n",
       "2  if a person is black , deadly force is general...   \n",
       "3           what a shitty upbringing that would be .   \n",
       "4               trudeau is nothing but corruptions .   \n",
       "5  and dont give me that bs about those priests b...   \n",
       "6  theyve been feeding this rubbish to their citi...   \n",
       "7  the reason white people have that advantage is...   \n",
       "8  thats why i donr blame trump for the present m...   \n",
       "9  one thing harper was not is a lapdog that crav...   \n",
       "\n",
       "                                             neutral dataset  \n",
       "0  the demonization of the china and the chinese ...  jigsaw  \n",
       "1  hatred of those white supremacist nazis was pr...  jigsaw  \n",
       "2  If a person is black, deadly force is generall...  jigsaw  \n",
       "3               what a bad upbringing that would be.  jigsaw  \n",
       "4                Trudeau is nothing but corruptions.  jigsaw  \n",
       "5  and dont give me that bs about those priests b...  jigsaw  \n",
       "6  theyve been feeding this to their citizens sin...  jigsaw  \n",
       "7  the reason white people have that advantage is...  jigsaw  \n",
       "8  That's why I don't blame Trump for the present...  jigsaw  \n",
       "9  one thing harper was not is a lapdog that crav...  jigsaw  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_en_augmented.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484092f7b992409c83e2f3c08b6b7be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toxic_ru = []\n",
    "neutral_ru = []\n",
    "for i in trange(0, len(detox_en_augmented), batch_size):\n",
    "    toxic_ru.extend(translate(detox_en_augmented.sentence[i:i+batch_size].tolist(), model, tokenizer))\n",
    "    neutral_ru.extend(translate(detox_en_augmented.neutral[i:i+batch_size].tolist(), model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en_augmented['toxic_ru'] = toxic_ru\n",
    "detox_en_augmented['neutral_ru'] = neutral_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en_augmented.to_csv('detox_en_augmented.tsv', index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en_augmented = pd.read_csv('detox_en_augmented.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en2ru = pd.read_csv('detox_en2ru.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>neutral_comment</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>toxic</th>\n",
       "      <th>confidence_toxic</th>\n",
       "      <th>is_match</th>\n",
       "      <th>confidence_is_match</th>\n",
       "      <th>toxic_ru</th>\n",
       "      <th>neutral_ru</th>\n",
       "      <th>edit_distance_en</th>\n",
       "      <th>edit_distance_ru</th>\n",
       "      <th>edit_sim_en</th>\n",
       "      <th>edit_sim_ru</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>similarity</th>\n",
       "      <th>fluency</th>\n",
       "      <th>joint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9885</th>\n",
       "      <td>69927</td>\n",
       "      <td>no one gives a fuck about the music on ur ipod</td>\n",
       "      <td>No one cares about the music on your iPod</td>\n",
       "      <td>0.900609</td>\n",
       "      <td>twitter</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9777</td>\n",
       "      <td>Никого не волнует музыка на айподе.</td>\n",
       "      <td>Никого не волнует музыка на твоем iPod</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.479064</td>\n",
       "      <td>0.680840</td>\n",
       "      <td>0.521709</td>\n",
       "      <td>0.170164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>9271</td>\n",
       "      <td>i think we have better things to care about in...</td>\n",
       "      <td>I think we have better things to care about in...</td>\n",
       "      <td>0.998004</td>\n",
       "      <td>reddit</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9824</td>\n",
       "      <td>Думаю, в Париже есть вещи поважнее, чем показы...</td>\n",
       "      <td>Думаю, в Париже есть вещи поважнее, чем показы...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.938272</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.997856</td>\n",
       "      <td>0.934951</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.932946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>22998</td>\n",
       "      <td>it was directed towards callin me a bitch</td>\n",
       "      <td>it was directed towards callin me a bad name</td>\n",
       "      <td>0.991726</td>\n",
       "      <td>twitter</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9727</td>\n",
       "      <td>Он был направлен на то, чтобы называть меня су...</td>\n",
       "      <td>Он был направлен на то, чтобы называть меня пл...</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.961407</td>\n",
       "      <td>0.775528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.745598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                      toxic_comment  \\\n",
       "9885   69927     no one gives a fuck about the music on ur ipod   \n",
       "15999   9271  i think we have better things to care about in...   \n",
       "4559   22998          it was directed towards callin me a bitch   \n",
       "\n",
       "                                         neutral_comment  toxicity_score  \\\n",
       "9885           No one cares about the music on your iPod        0.900609   \n",
       "15999  I think we have better things to care about in...        0.998004   \n",
       "4559        it was directed towards callin me a bad name        0.991726   \n",
       "\n",
       "       dataset  toxic  confidence_toxic  is_match  confidence_is_match  \\\n",
       "9885   twitter  False            0.9968      True               0.9777   \n",
       "15999   reddit  False            0.9997      True               0.9824   \n",
       "4559   twitter  False            0.9896      True               0.9727   \n",
       "\n",
       "                                                toxic_ru  \\\n",
       "9885                 Никого не волнует музыка на айподе.   \n",
       "15999  Думаю, в Париже есть вещи поважнее, чем показы...   \n",
       "4559   Он был направлен на то, чтобы называть меня су...   \n",
       "\n",
       "                                              neutral_ru  edit_distance_en  \\\n",
       "9885              Никого не волнует музыка на твоем iPod                14   \n",
       "15999  Думаю, в Париже есть вещи поважнее, чем показы...                 5   \n",
       "4559   Он был направлен на то, чтобы называть меня пл...                 7   \n",
       "\n",
       "       edit_distance_ru  edit_sim_en  edit_sim_ru  accuracy  similarity  \\\n",
       "9885                 10     0.695652     0.736842  0.479064    0.680840   \n",
       "15999                 7     0.938272     0.887097  0.997856    0.934951   \n",
       "4559                 13     0.840909     0.775862  0.961407    0.775528   \n",
       "\n",
       "        fluency     joint  \n",
       "9885   0.521709  0.170164  \n",
       "15999  1.000000  0.932946  \n",
       "4559   1.000000  0.745598  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_en2ru.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>neutral</th>\n",
       "      <th>dataset</th>\n",
       "      <th>toxic_ru</th>\n",
       "      <th>neutral_ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169718</th>\n",
       "      <td>don 't get too close bitch , tuh !</td>\n",
       "      <td>don 't get too close, tuh!</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Не подбирайся слишком близко, сука!</td>\n",
       "      <td>Не подходи слишком близко, Тихо!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196111</th>\n",
       "      <td>in bed with my pussy</td>\n",
       "      <td>I am in love with this woman.</td>\n",
       "      <td>twitter</td>\n",
       "      <td>в постели с моей киской</td>\n",
       "      <td>Я люблю эту женщину.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36806</th>\n",
       "      <td>the name mt mckinley sucked !</td>\n",
       "      <td>the name mt mckinley sucked!</td>\n",
       "      <td>jigsaw</td>\n",
       "      <td>имя МакКинли было отстойным!</td>\n",
       "      <td>имя МакКинли было отстойным!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  sentence                        neutral  \\\n",
       "169718  don 't get too close bitch , tuh !     don 't get too close, tuh!   \n",
       "196111                in bed with my pussy  I am in love with this woman.   \n",
       "36806        the name mt mckinley sucked !   the name mt mckinley sucked!   \n",
       "\n",
       "        dataset                             toxic_ru  \\\n",
       "169718  twitter  Не подбирайся слишком близко, сука!   \n",
       "196111  twitter              в постели с моей киской   \n",
       "36806    jigsaw         имя МакКинли было отстойным!   \n",
       "\n",
       "                              neutral_ru  \n",
       "169718  Не подходи слишком близко, Тихо!  \n",
       "196111              Я люблю эту женщину.  \n",
       "36806       имя МакКинли было отстойным!  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_en_augmented.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textdistance import levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en_augmented['edit_distance_ru'] = [levenshtein.distance(*row) for row in detox_en_augmented[['toxic_ru', 'neutral_ru']].values]\n",
    "detox_en_augmented['edit_sim_ru'] = [levenshtein.normalized_similarity(*row) for row in detox_en_augmented[['toxic_ru', 'neutral_ru']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en_augmented['edit_distance_en'] = [levenshtein.distance(*row) for row in detox_en_augmented[['sentence', 'neutral']].values]\n",
    "detox_en_augmented['edit_sim_en'] = [levenshtein.normalized_similarity(*row) for row in detox_en_augmented[['sentence', 'neutral']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>confidence_toxic</th>\n",
       "      <th>confidence_is_match</th>\n",
       "      <th>edit_distance_en</th>\n",
       "      <th>edit_distance_ru</th>\n",
       "      <th>edit_sim_en</th>\n",
       "      <th>edit_sim_ru</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>similarity</th>\n",
       "      <th>fluency</th>\n",
       "      <th>joint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19766.000000</td>\n",
       "      <td>19766.000000</td>\n",
       "      <td>19766.000000</td>\n",
       "      <td>19766.000000</td>\n",
       "      <td>19766.000000</td>\n",
       "      <td>19766.00000</td>\n",
       "      <td>19766.000000</td>\n",
       "      <td>19766.000000</td>\n",
       "      <td>19766.000000</td>\n",
       "      <td>19766.000000</td>\n",
       "      <td>19766.000000</td>\n",
       "      <td>19766.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>78035.783416</td>\n",
       "      <td>0.990181</td>\n",
       "      <td>0.984174</td>\n",
       "      <td>0.972585</td>\n",
       "      <td>15.705757</td>\n",
       "      <td>21.46327</td>\n",
       "      <td>0.696492</td>\n",
       "      <td>0.602780</td>\n",
       "      <td>0.838157</td>\n",
       "      <td>0.711838</td>\n",
       "      <td>0.869159</td>\n",
       "      <td>0.525728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>60839.351686</td>\n",
       "      <td>0.019532</td>\n",
       "      <td>0.025847</td>\n",
       "      <td>0.025094</td>\n",
       "      <td>10.989535</td>\n",
       "      <td>14.26659</td>\n",
       "      <td>0.186762</td>\n",
       "      <td>0.209683</td>\n",
       "      <td>0.250159</td>\n",
       "      <td>0.216294</td>\n",
       "      <td>0.172396</td>\n",
       "      <td>0.261383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.800983</td>\n",
       "      <td>0.523700</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31839.000000</td>\n",
       "      <td>0.991065</td>\n",
       "      <td>0.981100</td>\n",
       "      <td>0.961100</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.793939</td>\n",
       "      <td>0.604774</td>\n",
       "      <td>0.788633</td>\n",
       "      <td>0.309918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>63895.000000</td>\n",
       "      <td>0.996133</td>\n",
       "      <td>0.994100</td>\n",
       "      <td>0.980500</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>18.00000</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.974691</td>\n",
       "      <td>0.764827</td>\n",
       "      <td>0.944624</td>\n",
       "      <td>0.554619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>101255.250000</td>\n",
       "      <td>0.998179</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.991100</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>28.00000</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.994927</td>\n",
       "      <td>0.873708</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.745741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>238836.000000</td>\n",
       "      <td>0.999647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>271.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 idx  toxicity_score  confidence_toxic  confidence_is_match  \\\n",
       "count   19766.000000    19766.000000      19766.000000         19766.000000   \n",
       "mean    78035.783416        0.990181          0.984174             0.972585   \n",
       "std     60839.351686        0.019532          0.025847             0.025094   \n",
       "min         7.000000        0.800983          0.523700             0.800000   \n",
       "25%     31839.000000        0.991065          0.981100             0.961100   \n",
       "50%     63895.000000        0.996133          0.994100             0.980500   \n",
       "75%    101255.250000        0.998179          0.998500             0.991100   \n",
       "max    238836.000000        0.999647          1.000000             0.999900   \n",
       "\n",
       "       edit_distance_en  edit_distance_ru   edit_sim_en   edit_sim_ru  \\\n",
       "count      19766.000000       19766.00000  19766.000000  19766.000000   \n",
       "mean          15.705757          21.46327      0.696492      0.602780   \n",
       "std           10.989535          14.26659      0.186762      0.209683   \n",
       "min            0.000000           0.00000      0.000000      0.000000   \n",
       "25%            8.000000          11.00000      0.590909      0.451613   \n",
       "50%           12.000000          18.00000      0.741935      0.626667   \n",
       "75%           20.000000          28.00000      0.838710      0.765957   \n",
       "max          105.000000         271.00000      1.000000      1.000000   \n",
       "\n",
       "           accuracy    similarity       fluency         joint  \n",
       "count  19766.000000  19766.000000  19766.000000  19766.000000  \n",
       "mean       0.838157      0.711838      0.869159      0.525728  \n",
       "std        0.250159      0.216294      0.172396      0.261383  \n",
       "min        0.103904      0.000000      0.000000      0.000000  \n",
       "25%        0.793939      0.604774      0.788633      0.309918  \n",
       "50%        0.974691      0.764827      0.944624      0.554619  \n",
       "75%        0.994927      0.873708      1.000000      0.745741  \n",
       "max        0.999766      1.000000      1.000000      0.999527  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_en2ru.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edit_distance_ru</th>\n",
       "      <th>edit_sim_ru</th>\n",
       "      <th>edit_distance_en</th>\n",
       "      <th>edit_sim_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>210000.000000</td>\n",
       "      <td>210000.000000</td>\n",
       "      <td>210000.000000</td>\n",
       "      <td>210000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.198533</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>11.559924</td>\n",
       "      <td>0.766713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.668022</td>\n",
       "      <td>0.221797</td>\n",
       "      <td>8.207094</td>\n",
       "      <td>0.166556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.806452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.887097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       edit_distance_ru    edit_sim_ru  edit_distance_en    edit_sim_en\n",
       "count     210000.000000  210000.000000     210000.000000  210000.000000\n",
       "mean          17.198533       0.663594         11.559924       0.766713\n",
       "std           12.668022       0.221797          8.207094       0.166556\n",
       "min            0.000000       0.000000          0.000000       0.000000\n",
       "25%            8.000000       0.510204          6.000000       0.684211\n",
       "50%           15.000000       0.681818         10.000000       0.806452\n",
       "75%           24.000000       0.828571         15.000000       0.887097\n",
       "max          260.000000       1.000000        245.000000       1.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_en_augmented.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19766, 19)\n",
      "(18239, 19)\n"
     ]
    }
   ],
   "source": [
    "detox_en2ru_filtered = detox_en2ru[\n",
    "    (detox_en2ru.edit_distance_ru >= detox_en2ru.edit_distance_en.quantile(0.01)) \n",
    "    & (detox_en2ru.edit_distance_ru <= detox_en2ru.edit_distance_en.quantile(0.99)) \n",
    "    & (detox_en2ru.edit_sim_ru >= detox_en2ru.edit_sim_en.quantile(0.01)) \n",
    "    & (detox_en2ru.edit_sim_ru <= detox_en2ru.edit_sim_en.quantile(0.99))\n",
    "]\n",
    "\n",
    "print(detox_en2ru.shape)\n",
    "print(detox_en2ru_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210000, 9)\n",
      "(178112, 9)\n"
     ]
    }
   ],
   "source": [
    "detox_augmented_filtered = detox_en_augmented[\n",
    "    (detox_en_augmented.edit_distance_ru >= detox_en2ru.edit_distance_en.quantile(0.01)) \n",
    "    & (detox_en_augmented.edit_distance_ru <= detox_en2ru.edit_distance_en.quantile(0.99)) \n",
    "    & (detox_en_augmented.edit_sim_ru >= detox_en2ru.edit_sim_en.quantile(0.01)) \n",
    "    & (detox_en_augmented.edit_sim_ru <= detox_en2ru.edit_sim_en.quantile(0.99))\n",
    "]\n",
    "\n",
    "print(detox_en_augmented.shape)\n",
    "print(detox_augmented_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>neutral</th>\n",
       "      <th>dataset</th>\n",
       "      <th>toxic_ru</th>\n",
       "      <th>neutral_ru</th>\n",
       "      <th>edit_distance_ru</th>\n",
       "      <th>edit_sim_ru</th>\n",
       "      <th>edit_distance_en</th>\n",
       "      <th>edit_sim_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170589</th>\n",
       "      <td>it 's bitch follow me pleeease ?</td>\n",
       "      <td>follow me pleeease?</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Это сучка за моей плейезой?</td>\n",
       "      <td>Следуешь за мной?</td>\n",
       "      <td>18</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>13</td>\n",
       "      <td>0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88547</th>\n",
       "      <td>i get the idea of goodie bags or whatever to an extent , but entire fucking gifts ! ?</td>\n",
       "      <td>i get the idea of goodie bags or whatever to an extent, but entire gifts!?</td>\n",
       "      <td>reddit</td>\n",
       "      <td>Я понимаю, что такое шикарные сумки или в какой-то мере. Но целые чертовы подарки! ?</td>\n",
       "      <td>Я понимаю, что такое шикарные сумки или вроде того. Но целые подарки! ?</td>\n",
       "      <td>20</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>11</td>\n",
       "      <td>0.870588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73805</th>\n",
       "      <td>they build and program that shit .</td>\n",
       "      <td>They build and program that.</td>\n",
       "      <td>reddit</td>\n",
       "      <td>Они строят и программируют это дерьмо.</td>\n",
       "      <td>Они строят и программируют это.</td>\n",
       "      <td>7</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>7</td>\n",
       "      <td>0.794118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     sentence  \\\n",
       "170589                                                       it 's bitch follow me pleeease ?   \n",
       "88547   i get the idea of goodie bags or whatever to an extent , but entire fucking gifts ! ?   \n",
       "73805                                                      they build and program that shit .   \n",
       "\n",
       "                                                                           neutral  \\\n",
       "170589                                                         follow me pleeease?   \n",
       "88547   i get the idea of goodie bags or whatever to an extent, but entire gifts!?   \n",
       "73805                                                 They build and program that.   \n",
       "\n",
       "        dataset  \\\n",
       "170589  twitter   \n",
       "88547    reddit   \n",
       "73805    reddit   \n",
       "\n",
       "                                                                                    toxic_ru  \\\n",
       "170589                                                           Это сучка за моей плейезой?   \n",
       "88547   Я понимаю, что такое шикарные сумки или в какой-то мере. Но целые чертовы подарки! ?   \n",
       "73805                                                 Они строят и программируют это дерьмо.   \n",
       "\n",
       "                                                                     neutral_ru  \\\n",
       "170589                                                        Следуешь за мной?   \n",
       "88547   Я понимаю, что такое шикарные сумки или вроде того. Но целые подарки! ?   \n",
       "73805                                           Они строят и программируют это.   \n",
       "\n",
       "        edit_distance_ru  edit_sim_ru  edit_distance_en  edit_sim_en  \n",
       "170589                18     0.333333                13     0.593750  \n",
       "88547                 20     0.761905                11     0.870588  \n",
       "73805                  7     0.815789                 7     0.794118  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_augmented_filtered.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(detox_en2ru_filtered, random_state=1, test_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import CHRF\n",
    "chrfpp = CHRF(word_order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.6204444270353"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrfpp.corpus_score(val.toxic_ru.tolist(), [val.neutral_ru.tolist()]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(\n",
    "    text, model, tokenizer, n=None, max_length=\"auto\", beams=5,\n",
    "):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    if max_length == \"auto\":\n",
    "        max_length = inputs.shape[1] + 10\n",
    "\n",
    "    result = model.generate(\n",
    "        inputs,\n",
    "        num_return_sequences=n or 1,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        repetition_penalty=10.0,\n",
    "        max_length=max_length,\n",
    "        min_length=int(0.5 * max_length),\n",
    "        num_beams=beams,\n",
    "        #forced_bos_token_id=tokenizer.lang_code_to_id[tokenizer.tgt_lang],\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/russian_data/test.tsv', sep='\\t')\n",
    "test_inputs = test_data[\"toxic_comment\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add special `<s>` token to indicate augmented data (it is not used otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_en: Dataset({\n",
       "        features: ['text', 'target'],\n",
       "        num_rows: 195851\n",
       "    })\n",
       "    train_ru: Dataset({\n",
       "        features: ['text', 'target'],\n",
       "        num_rows: 195851\n",
       "    })\n",
       "    train_ru_clean: Dataset({\n",
       "        features: ['text', 'target'],\n",
       "        num_rows: 17739\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text', 'target'],\n",
       "        num_rows: 391702\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['text', 'target'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = DatasetDict({\n",
    "    'train_en': Dataset.from_dict({\n",
    "        'text': train.toxic_comment.tolist() + ['<s>' + t for t in detox_augmented_filtered.sentence.tolist()], \n",
    "        'target': train.neutral_comment.tolist() + ['<s>' + t for t in detox_augmented_filtered.neutral.tolist()], \n",
    "    }),\n",
    "    'train_ru': Dataset.from_dict({\n",
    "        'text': train.toxic_ru.tolist() + ['<s>' + t for t in detox_augmented_filtered.toxic_ru.tolist() ], \n",
    "        'target': train.neutral_ru.tolist()  + ['<s>' + t for t in detox_augmented_filtered.neutral_ru.tolist()], \n",
    "    }),\n",
    "    'train_ru_clean': Dataset.from_dict({\n",
    "        'text': train.toxic_ru.tolist(),\n",
    "        'target': train.neutral_ru.tolist(),\n",
    "    }),\n",
    "    'train': Dataset.from_dict({\n",
    "        'text': train.toxic_ru.tolist() + train.toxic_comment.tolist() \\\n",
    "            + ['<s>' + t for t in detox_augmented_filtered.toxic_ru.tolist() + detox_augmented_filtered.sentence.tolist()], \n",
    "        'target': train.neutral_ru.tolist() + train.neutral_comment.tolist() \\\n",
    "            + ['<s>' + t for t in detox_augmented_filtered.neutral_ru.tolist() + detox_augmented_filtered.neutral.tolist()], \n",
    "    }),\n",
    "    'dev': Dataset.from_dict({'text': val.toxic_ru, 'target': val.neutral_ru}),\n",
    "})\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<s>she might not make it me fuck you nursey',\n",
       " 'target': '<s>she might not make it me mess with you nursey'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['train'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'facebook/mbart-large-50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, padding=True)\n",
    "    labels = tokenizer(examples[\"target\"], padding=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e749400c834a49868dcda22450050344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810cde95b7ff42cdb3389ec7198599f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9f5ac594cb4b3cb919e57afd0f3c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b44f72025248779cd659102e81387e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/392 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de1e80fa4ba46fcaf54fdbc6138bfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_data = raw_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=1, # 8 is too much \n",
    "    weight_decay=1e-5,\n",
    "    max_steps=50_000,\n",
    "    learning_rate=1e-5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    eval_steps=1000, \n",
    "    save_steps=1000,\n",
    "    logging_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    # trying to save memory: see https://huggingface.co/docs/transformers/performance\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adafactor\",\n",
    "    gradient_accumulation_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tok_data[\"train\"],\n",
    "    eval_dataset=tok_data[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 4 hours of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 391702\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50000\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50000' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50000/50000 5:02:05, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>1.082111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.343400</td>\n",
       "      <td>0.490913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.455586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.448654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.297800</td>\n",
       "      <td>0.431600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.295900</td>\n",
       "      <td>0.417865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.293400</td>\n",
       "      <td>0.413381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.289800</td>\n",
       "      <td>0.410315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.288900</td>\n",
       "      <td>0.403366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.277100</td>\n",
       "      <td>0.399524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.284800</td>\n",
       "      <td>0.396950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.282900</td>\n",
       "      <td>0.395608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.283500</td>\n",
       "      <td>0.397444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>0.381915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.381281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>0.381450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.379178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.376760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.377113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.266300</td>\n",
       "      <td>0.376828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.369066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.364376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.364574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.365111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.259400</td>\n",
       "      <td>0.366895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.263400</td>\n",
       "      <td>0.360185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.261100</td>\n",
       "      <td>0.361061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.253300</td>\n",
       "      <td>0.359434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.358973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.356408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.254100</td>\n",
       "      <td>0.356395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.353459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.251300</td>\n",
       "      <td>0.357476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>0.350901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>0.351087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.255800</td>\n",
       "      <td>0.348495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.348119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.251400</td>\n",
       "      <td>0.348897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.348263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>0.344734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.249700</td>\n",
       "      <td>0.344448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.343547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.343610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.342934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.239200</td>\n",
       "      <td>0.342768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.249100</td>\n",
       "      <td>0.342873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.249100</td>\n",
       "      <td>0.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.248600</td>\n",
       "      <td>0.339958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.340295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.217600</td>\n",
       "      <td>0.341599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-1000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-1000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-2000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-2000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-3000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-3000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-4000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-4000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-5000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-5000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-3000] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-6000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-6000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-4000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-7000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-7000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-8000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-8000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-6000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-9000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-9000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-7000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-10000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-10000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-8000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-11000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-11000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-9000] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-12000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-12000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-10000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-13000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-13000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-11000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-14000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-14000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-12000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-15000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-15000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-13000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-16000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-16000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-14000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-17000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-17000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-15000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-18000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-18000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-16000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-19000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-19000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-17000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-20000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-20000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-19000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-21000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-21000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-18000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-22000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-22000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-20000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-23000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-23000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-23000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-21000] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-24000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-24000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-24000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-23000] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-25000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-25000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-25000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-24000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-26000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-26000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-26000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-22000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-27000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-27000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-27000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-25000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-28000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-28000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-28000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-26000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-29000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-29000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-29000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-27000] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-30000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-30000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-30000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-28000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-31000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-31000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-31000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-29000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-32000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-32000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-32000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-30000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-33000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-33000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-33000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-31000] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-34000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-34000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-34000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-32000] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-35000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-35000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-35000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-33000] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-36000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-36000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-36000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-34000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-37000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-37000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-37000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-35000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-38000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-38000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-38000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-36000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-39000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-39000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-39000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-38000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-40000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-40000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-40000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-37000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-41000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-41000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-41000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-41000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-39000] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-42000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-42000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-42000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-40000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-43000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-43000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-43000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-43000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-43000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-41000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-44000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-44000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-44000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-44000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-44000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-42000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-45000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-45000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-45000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-45000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-45000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-43000] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-46000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-46000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-46000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-46000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-46000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-44000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-47000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-47000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-47000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-47000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-47000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-45000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-48000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-48000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-48000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-48000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-48000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-46000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-49000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-49000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-49000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-49000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-49000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-47000] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-50000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-50000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-50000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-49000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart/checkpoint-48000 (score: 0.33995795249938965).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50000, training_loss=0.28522207397460936, metrics={'train_runtime': 18125.3899, 'train_samples_per_second': 22.068, 'train_steps_per_second': 2.759, 'total_flos': 3.915323299735142e+16, 'train_loss': 0.28522207397460936, 'epoch': 1.02})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e047b35632254668bee1dc00c5e55082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "model.eval()\n",
    "for text in tqdm(val.toxic_ru):\n",
    "    with torch.inference_mode():\n",
    "        out = tokenizer.decode(\n",
    "            model.generate(**tokenizer(text, return_tensors='pt').to(model.device), num_beams=5, max_length=256)[0], \n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        preds.append(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "61.4175 was the previous model score. With augmentation, it is higher: 61.9785. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.97856261830022\n"
     ]
    }
   ],
   "source": [
    "print(chrfpp.corpus_score(preds, [val.neutral_ru.tolist()]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2b31c474054038a7d71beaa5af2b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs = [paraphrase(text, model, tokenizer) for text in tqdm(test_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../results/translate-train-full_augmented_bilingual-mbart/'\n",
    "os.makedirs(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(p + 'results_ru.txt', 'w') as f:\n",
    "    for text in test_outputs:\n",
    "        f.write(text+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python evaluate_ru.py \\\n",
    "    --result_filename scores \\\n",
    "    --input_dir results/translate-train-full_augmented_bilingual-mbart \\\n",
    "    --output_dir results\n",
    "```\n",
    "\n",
    "```\n",
    "Style accuracy:       0.47033900022506714\n",
    "Meaning preservation: 0.8848435282707214\n",
    "Joint fluency:        -0.10494334995746613\n",
    "Joint score:          -0.04591362178325653\n",
    "Scores after calibration:\n",
    "Style accuracy:       0.5233051180839539\n",
    "Meaning preservation: 0.8273530006408691\n",
    "Joint fluency:        0.879315197467804\n",
    "Joint score:          0.36825796961784363\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=1, # 8 is too much \n",
    "    weight_decay=1e-5,\n",
    "    max_steps=5_000,\n",
    "    learning_rate=1e-5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    eval_steps=1000, \n",
    "    save_steps=1000,\n",
    "    logging_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    # trying to save memory: see https://huggingface.co/docs/transformers/performance\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adafactor\",\n",
    "    gradient_accumulation_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tok_data[\"train_ru_clean\"],\n",
    "    eval_dataset=tok_data[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 17739\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 32:51, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.344882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.341600</td>\n",
       "      <td>0.336609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.283300</td>\n",
       "      <td>0.340101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.335528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.246900</td>\n",
       "      <td>0.345450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-1000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-1000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-2000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-2000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-3000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-3000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-4000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-4000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-2000] due to args.save_total_limit\n",
      "/home/dale/p3/lib/python3.7/site-packages/transformers/trainer.py:1599: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: text, target. If text, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-5000\n",
      "Configuration saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-5000/config.json\n",
      "Model weights saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-3000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/dale/models/detox-parallel/translate-en2ru-full_aug_bilingual-mbart-finetune/checkpoint-4000 (score: 0.33552777767181396).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5000, training_loss=0.2969224395751953, metrics={'train_runtime': 1972.0811, 'train_samples_per_second': 20.283, 'train_steps_per_second': 2.535, 'total_flos': 5273016131518464.0, 'train_loss': 0.2969224395751953, 'epoch': 2.25})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f46ecb46bcd428bb1c12f42277a9211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "model.eval()\n",
    "for text in tqdm(val.toxic_ru):\n",
    "    with torch.inference_mode():\n",
    "        out = tokenizer.decode(\n",
    "            model.generate(**tokenizer(text, return_tensors='pt').to(model.device), num_beams=5, max_length=256)[0], \n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        preds.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.257692944801725\n"
     ]
    }
   ],
   "source": [
    "print(chrfpp.corpus_score(preds, [val.neutral_ru.tolist()]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7948289d718d4136aed37130e2d765e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs = [paraphrase(text, model, tokenizer) for text in tqdm(test_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../results/translate-train-full_augmented_bilingual-mbart-finetune/'\n",
    "os.makedirs(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(p + 'results_ru.txt', 'w') as f:\n",
    "    for text in test_outputs:\n",
    "        f.write(text+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python evaluate_ru.py \\\n",
    "    --result_filename scores \\\n",
    "    --input_dir results/translate-train-full_augmented_bilingual-mbart-finetune \\\n",
    "    --output_dir results\n",
    "```\n",
    "\n",
    "```\n",
    "Style accuracy:       0.5113834142684937\n",
    "Meaning preservation: 0.8692513704299927\n",
    "Joint fluency:        -0.11415145546197891\n",
    "Joint score:          -0.04862915724515915\n",
    "Scores after calibration:\n",
    "Style accuracy:       0.5602450370788574\n",
    "Meaning preservation: 0.8039894700050354\n",
    "Joint fluency:        0.8687258362770081\n",
    "Joint score:          0.3829019069671631\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3k",
   "language": "python",
   "name": "p3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
