{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Paraphrasing: https://huggingface.co/datasets/GEM/opusparcus\n",
    "* Translation: https://huggingface.co/datasets/open_subtitles + news_commentary? + tatoeba?\n",
    "* Detox: ordinary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opusparcus (/home/dale/.cache/huggingface/datasets/GEM___opusparcus/en.80/1.0.0/79d36ae4eced4f3c2c5a2ab9f94a584de7adca957186408d33798d0d87b018f2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f20a6b7625c4b08869f92bfd0bfdc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 982\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1015\n",
       "    })\n",
       "    test.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1445\n",
       "    })\n",
       "    validation.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1455\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 5200000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opus_para_en = load_dataset(\"GEM/opusparcus\", \"en.80\")\n",
    "opus_para_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opusparcus (/home/dale/.cache/huggingface/datasets/GEM___opusparcus/ru.80/1.0.0/79d36ae4eced4f3c2c5a2ab9f94a584de7adca957186408d33798d0d87b018f2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7dfd9e57cd4cd2832923c763bc7444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1068\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1020\n",
       "    })\n",
       "    test.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1855\n",
       "    })\n",
       "    validation.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1854\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 2300000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opus_para_ru = load_dataset(\"GEM/opusparcus\", \"ru.80\")\n",
    "opus_para_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lang': 'ru',\n",
       " 'input': 'Сделай, и всё.',\n",
       " 'target': 'Просто сделай это.',\n",
       " 'annot_score': 0.0,\n",
       " 'gem_id': 'gem-opusparcus-train-78766378',\n",
       " 'references': ['Просто сделай это.']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(opus_para_ru['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-ru-lang1=en,lang2=ru\n",
      "Reusing dataset open_subtitles (/home/dale/.cache/huggingface/datasets/open_subtitles/en-ru-lang1=en,lang2=ru/0.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ada40d75ec4407be28ed874e81ba24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'meta', 'translation'],\n",
       "        num_rows: 25910105\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opensub = load_dataset(\"open_subtitles\", lang1=\"en\", lang2='ru')\n",
    "opensub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '16790810',\n",
       " 'meta': {'year': 2013,\n",
       "  'imdbId': 2808666,\n",
       "  'subtitleId': {'en': 5081422, 'ru': 5084906},\n",
       "  'sentenceIds': {'en': [449], 'ru': [445]}},\n",
       " 'translation': {'en': 'Tell me who she is.', 'ru': 'Скажи мне, кто она!'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(opensub['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-ru-lang1=en,lang2=ru\n",
      "Reusing dataset news_commentary (/home/dale/.cache/huggingface/datasets/news_commentary/en-ru-lang1=en,lang2=ru/0.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3083f64a2354442d8446563e3627f03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 190104\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_commentary = load_dataset(\"news_commentary\", lang1=\"en\", lang2='ru')\n",
    "news_commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '31817',\n",
       " 'translation': {'en': 'Such measures aim to reduce the risk of conflict by increasing trust among participating OSCE states, and by contributing to greater transparency in the field of military planning and other activities.',\n",
       "  'ru': 'Такие меры направлены на снижение риска возникновения конфликтов за счет увеличения доверия между государствами-участниками ОБСЕ, а также они будут способствовать повышению прозрачности в области военного планирования и других мероприятий.'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(news_commentary['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-ru-lang1=en,lang2=ru\n",
      "Reusing dataset tatoeba (/home/dale/.cache/huggingface/datasets/tatoeba/en-ru-lang1=en,lang2=ru/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc4c0d64d414dbea7a3d1f7159f683d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 523656\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tatoeba = load_dataset(\"tatoeba\", lang1=\"en\", lang2='ru')\n",
    "tatoeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '404703',\n",
       " 'translation': {'en': \"Tom is from the same village that I'm from.\",\n",
       "  'ru': 'Мы с Томом из одной деревни.'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(tatoeba['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['paraphrase: Again, he wins.'], ['He won again.'], 'en_XX', 'en_XX')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_paraphrase_task(batch_size=1):\n",
    "    task = 'paraphrase: '\n",
    "    if random.random() < 0.5:\n",
    "        src = opus_para_en\n",
    "        src_id, tgt_id = 'en_XX', 'en_XX'\n",
    "    else:\n",
    "        src = opus_para_ru\n",
    "        src_id, tgt_id = 'ru_RU', 'ru_RU'\n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(batch_size):\n",
    "        item = random.choice(src['train'])\n",
    "        xx, yy = item['input'], item['target']\n",
    "        if random.random() < 0.5:\n",
    "            xx, yy = yy, xx\n",
    "        x.append(task + xx)\n",
    "        y.append(yy)\n",
    "    return x, y, src_id, tgt_id\n",
    "        \n",
    "get_paraphrase_task(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['translate: Джон сказал: «Это красивая мелодия».',\n",
       "  'translate: Не кидай камни в собаку.'],\n",
       " ['John said, \"That\\'s a beautiful tune.\"', \"Don't throw stones at the dog.\"],\n",
       " 'ru_RU',\n",
       " 'en_XX')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_translate_task(batch_size=1):\n",
    "    task = 'translate: '\n",
    "    src = random.choice([tatoeba, opensub, news_commentary])\n",
    "    src_id, tgt_id = 'ru_RU', 'en_XX'\n",
    "    if random.random() < 0.5:\n",
    "        src_id, tgt_id = tgt_id, src_id\n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(batch_size):\n",
    "        item = random.choice(src['train'])\n",
    "        x.append(task + item['translation'][src_id[:2]])\n",
    "        y.append(item['translation'][tgt_id[:2]])\n",
    "    return x, y, src_id, tgt_id\n",
    "\n",
    "get_translate_task(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/home/dale/models/detox-parallel/mbart_5000_EN'\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/mbart-large-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size=4):\n",
    "    x = []\n",
    "    y = []\n",
    "    bs = batch_size // 2\n",
    "    for generator in [get_paraphrase_task, get_translate_task]:\n",
    "        for _ in range(bs):\n",
    "            xx, yy, src_id, tgt_id = generator(1)\n",
    "            tokenizer.src_lang, tokenizer.tgt_lang = src_id, tgt_id\n",
    "            x.append(tokenizer(xx))\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                y.append(tokenizer(yy))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'input_ids': [[250004, 121, 179665, 184, 12, 3293, 83, 10, 50208, 126, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 121, 179665, 184, 12, 82310, 98676, 31766, 62, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 121, 179665, 184, 12, 24080, 4, 3884, 442, 16065, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 3900, 19309, 12, 992, 4398, 299, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 3900, 19309, 12, 20, 10160, 4, 51544, 2355, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 3900, 19309, 12, 56227, 26574, 77, 69, 33364, 41984, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}],\n",
       " [{'input_ids': [[250004, 442, 25, 7, 10, 50208, 126, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 2583, 34614, 11, 1002, 775, 111, 10, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 89726, 7086, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 901, 4968, 80502, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 20, 166891, 70177, 38, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 1401, 25, 107, 8306, 132294, 2046, 10, 200, 71, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batch(6)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_XX paraphrase: This is a bullet.</s>\n",
      "en_XX it's a bullet.</s>\n",
      "en_XX paraphrase: YOU SON OF A.</s>\n",
      "en_XX You sneaky son of a.</s>\n",
      "en_XX paraphrase: Nah, put it away.</s>\n",
      "en_XX Never mind.</s>\n",
      "en_XX translate: Uncanny.</s>\n",
      "ru_RU Поразительно.</s>\n",
      "en_XX translate: - Oh, thank god.</s>\n",
      "ru_RU - Слава Богу!</s>\n",
      "ru_RU translate: Нам никогда не дадут ребенка.</s>\n",
      "en_XX We're never gonna get a kid.</s>\n"
     ]
    }
   ],
   "source": [
    "for xx, yy in zip(x, y):\n",
    "    print(tokenizer.decode(xx['input_ids'][0]))\n",
    "    print(tokenizer.decode(yy['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[250021, 121, 179665, 184, 12, 16434, 4042, 206677, 15856, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [250004, 121, 179665, 184, 12, 175254, 23, 70, 131, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [250004, 3900, 19309, 12, 62, 29954, 509, 7228, 23, 903, 48225, 23, 18237, 88922, 3229, 70, 66069, 111, 240596, 4, 172915, 25842, 1286, 3501, 23552, 111, 70, 32786, 117249, 111, 70, 14098, 145704, 4, 509, 75935, 23, 34210, 15519, 5, 2], [250004, 3900, 19309, 12, 6561, 199823, 43606, 75281, 136, 59671, 939, 9, 13, 22553, 47, 186, 24763, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad([{k: v[0] for k, v in item.items()} for item in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor\n",
    "optimizer = Adafactor(model.parameters(), scale_parameter=False, relative_step=False, lr=1e-5, clip_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "num_warmup_steps = 1000\n",
    "\n",
    "def lr_lambda(current_step: int):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1.0, num_warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_steps = 4\n",
    "batch_size = 6\n",
    "window = 1000\n",
    "report_steps = 1000\n",
    "cleanup_steps = 100\n",
    "save_path = '/home/dale/models/detox-parallel/bart-multitask-pretrain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewm_loss = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ~6 iterations per second, 1M iteration takes 1000000/6/60/60=45 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b9004fe93c4a34b92854224c8a8b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/839148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train()\n",
    "tq = trange(160852 , 1_000_000)\n",
    "cleanup()\n",
    "\n",
    "for i in tq:\n",
    "    x, y = get_batch(batch_size)\n",
    "    xpad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in x])\n",
    "    ypad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in y])\n",
    "    \n",
    "    try:\n",
    "        labels = torch.tensor(ypad['input_ids'], device=model.device)\n",
    "        labels[labels==tokenizer.pad_token_id] = -100\n",
    "        loss = model(\n",
    "            input_ids=torch.tensor(xpad['input_ids'], device=model.device),\n",
    "            attention_mask=torch.tensor(xpad['attention_mask'], device=model.device),\n",
    "            labels=labels,\n",
    "        ).loss\n",
    "        loss.backward()\n",
    "        if i % gradient_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "    except RuntimeError as e:\n",
    "        print('error', i, e)\n",
    "        loss = None\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        cleanup()\n",
    "        continue\n",
    "\n",
    "    w = 1 / max(1, min(i, window))\n",
    "    ewm_loss = ewm_loss * (1-w) + loss.item() * w\n",
    "    tq.set_description(f'{ewm_loss:3.4f}')\n",
    "\n",
    "    if i > 0 and i % report_steps == 0:\n",
    "        print('step', i, 'loss', ewm_loss, )\n",
    "        if i > 0:\n",
    "            model.save_pretrained(save_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "    if i % cleanup_steps == 0:\n",
    "        cleanup()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999999 1.076907632012897\n"
     ]
    }
   ],
   "source": [
    "print(i, ewm_loss)\n",
    "# step 141000 loss 0.9336458999525007\n",
    "# 598943 0.7913156676625156\n",
    "# 999999 0.7528545096685079"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "step 1000 loss 4.030285649758136\n",
    "step 2000 loss 2.924966513852863\n",
    "step 4000 loss 2.0756006959416595\n",
    "step 5000 loss 1.8793864612213047\n",
    "step 6000 loss 1.7691763829453662\n",
    "step 7000 loss 1.7182416955328477\n",
    "step 8000 loss 1.6834452027635922\n",
    "step 9000 loss 1.663543205332096\n",
    "step 10000 loss 1.663113163950107\n",
    "step 11000 loss 1.6201348936227629\n",
    "step 12000 loss 1.6082521446914142\n",
    "step 13000 loss 1.5896520319789016\n",
    "step 14000 loss 1.5847235759135747\n",
    "step 15000 loss 1.5505381041495554\n",
    "step 16000 loss 1.543097093158245\n",
    "step 17000 loss 1.5379276086863993\n",
    "step 18000 loss 1.5277597218784424\n",
    "step 19000 loss 1.5407952705743255\n",
    "step 20000 loss 1.536612467038952\n",
    "step 21000 loss 1.5191543397968388\n",
    "step 22000 loss 1.518949046247585\n",
    "step 23000 loss 1.503215299959398\n",
    "step 24000 loss 1.5010942820045503\n",
    "step 25000 loss 1.4981686735060622\n",
    "step 26000 loss 1.4965668479089238\n",
    "step 27000 loss 1.488605661614572\n",
    "step 28000 loss 1.4758588817081453\n",
    "step 29000 loss 1.4726483236694177\n",
    "step 30000 loss 1.4814640238725008\n",
    "step 31000 loss 1.47830836848471\n",
    "step 32000 loss 1.4589154542656102\n",
    "step 33000 loss 1.4417377788981833\n",
    "step 34000 loss 1.4494910439710555\n",
    "step 35000 loss 1.4426278926886749\n",
    "step 36000 loss 1.44148649541007\n",
    "step 37000 loss 1.4374428204636993\n",
    "step 38000 loss 1.4578149832296017\n",
    "step 39000 loss 1.4471952635777157\n",
    "step 40000 loss 1.441363376735978\n",
    "step 41000 loss 1.4358235478151893\n",
    "step 42000 loss 1.4276501513380877\n",
    "step 43000 loss 1.430725375910283\n",
    "step 44000 loss 1.4313272358920095\n",
    "step 45000 loss 1.432307047691588\n",
    "step 46000 loss 1.4248935302674965\n",
    "step 47000 loss 1.428975425487932\n",
    "step 48000 loss 1.4367560202463607\n",
    "step 49000 loss 1.4240793456122969\n",
    "step 50000 loss 1.4081390656764776\n",
    "step 51000 loss 1.4091096928337647\n",
    "step 52000 loss 1.3977898006943437\n",
    "\n",
    "step 53000 loss 1.408167125875017\n",
    "step 54000 loss 1.411923371306938\n",
    "step 55000 loss 1.394591920184885\n",
    "step 56000 loss 1.4101064364472955\n",
    "step 57000 loss 1.4045947166181658\n",
    "step 58000 loss 1.4117634408999225\n",
    "step 60000 loss 1.4005583499928356\n",
    "step 61000 loss 1.3778290190471294\n",
    "step 62000 loss 1.3916115126198214\n",
    "step 63000 loss 1.3835347688222643\n",
    "step 64000 loss 1.3984618459599827\n",
    "step 65000 loss 1.3976124273679278\n",
    "step 66000 loss 1.381784886532697\n",
    "step 67000 loss 1.3648037205587196\n",
    "step 68000 loss 1.374033853056438\n",
    "\n",
    "step 69000 loss 1.3733464772640473\n",
    "step 70000 loss 1.368408598845629\n",
    "step 71000 loss 1.36885491135711\n",
    "step 72000 loss 1.365724270276775\n",
    "step 73000 loss 1.3748838410436879\n",
    "step 74000 loss 1.3669939964287745\n",
    "step 75000 loss 1.368752935215937\n",
    "step 76000 loss 1.3712254828303787\n",
    "step 77000 loss 1.3707613699794057\n",
    "step 78000 loss 1.3723687654880743\n",
    "step 79000 loss 1.3704682869621896\n",
    "step 80000 loss 1.3620819050889972\n",
    "step 81000 loss 1.365885952894489\n",
    "step 82000 loss 1.3521919751292009\n",
    "step 83000 loss 1.3528008621156438\n",
    "step 84000 loss 1.3573856373971744\n",
    "step 85000 loss 1.357475243249886\n",
    "step 86000 loss 1.355344262885632\n",
    "step 87000 loss 1.3396657697283283\n",
    "step 88000 loss 1.3530322961749577\n",
    "step 89000 loss 1.351423998173461\n",
    "step 90000 loss 1.345621896752212\n",
    "step 91000 loss 1.3491330424397554\n",
    "step 92000 loss 1.3495505573811666\n",
    "step 93000 loss 1.3458939547429194\n",
    "step 94000 loss 1.3620523511085512\n",
    "step 95000 loss 1.3511449763697705\n",
    "step 96000 loss 1.3331114196031584\n",
    "step 97000 loss 1.3442420122214624\n",
    "step 98000 loss 1.3533528126214864\n",
    "step 99000 loss 1.3576928244196738\n",
    "step 100000 loss 1.3535247897620117\n",
    "step 101000 loss 1.3387256920724813\n",
    "step 102000 loss 1.3452958104057957\n",
    "step 103000 loss 1.3356340940417883\n",
    "step 104000 loss 1.333309402278774\n",
    "step 105000 loss 1.3258874055995518\n",
    "step 106000 loss 1.3370539755310682\n",
    "step 107000 loss 1.343880505123775\n",
    "step 108000 loss 1.341739257448781\n",
    "step 109000 loss 1.3395123455102544\n",
    "step 110000 loss 1.3279594339818224\n",
    "step 111000 loss 1.3226150264791396\n",
    "step 112000 loss 1.3206542441713167\n",
    "step 113000 loss 1.3236204271528507\n",
    "step 114000 loss 1.3193116127056803\n",
    "step 115000 loss 1.3152623717347338\n",
    "step 116000 loss 1.3086302777171588\n",
    "step 117000 loss 1.3310852806315456\n",
    "step 118000 loss 1.3327323135754303\n",
    "step 119000 loss 1.3145623184089887\n",
    "step 120000 loss 1.3062492088541324\n",
    "step 121000 loss 1.3141538967149964\n",
    "step 122000 loss 1.3160020090388171\n",
    "step 123000 loss 1.321558573313111\n",
    "step 124000 loss 1.3127376859450441\n",
    "step 125000 loss 1.3138053120861193\n",
    "step 126000 loss 1.2949118680851988\n",
    "step 127000 loss 1.3034073680619607\n",
    "step 128000 loss 1.3183906576789546\n",
    "step 129000 loss 1.3019162163215015\n",
    "step 130000 loss 1.3010734672083777\n",
    "step 131000 loss 1.3191547748294705\n",
    "step 132000 loss 1.3149730288162342\n",
    "step 133000 loss 1.3171935941087558\n",
    "step 134000 loss 1.3153115811646312\n",
    "step 135000 loss 1.3181873168703298\n",
    "step 136000 loss 1.3133259883003798\n",
    "step 137000 loss 1.32182021592838\n",
    "step 138000 loss 1.3079991855624757\n",
    "step 139000 loss 1.2964033815500906\n",
    "step 140000 loss 1.303071548959342\n",
    "step 141000 loss 1.3062257196002987\n",
    "step 142000 loss 1.2999398341278006\n",
    "step 143000 loss 1.2982098372683213\n",
    "step 144000 loss 1.3052906077292914\n",
    "step 145000 loss 1.3063556414884758\n",
    "step 146000 loss 1.3070851643857095\n",
    "step 147000 loss 1.2984685859014025\n",
    "step 148000 loss 1.3053904894859323\n",
    "step 149000 loss 1.304871654869325\n",
    "step 150000 loss 1.3008903165969232\n",
    "step 151000 loss 1.303887735036512\n",
    "step 152000 loss 1.2935637518539662\n",
    "step 153000 loss 1.3060935613096878\n",
    "step 154000 loss 1.302060990732172\n",
    "step 155000 loss 1.2962453502285138\n",
    "step 156000 loss 1.2921562535555324\n",
    "step 157000 loss 1.2871806450898\n",
    "step 158000 loss 1.2995706042796882\n",
    "step 159000 loss 1.3095689668868038\n",
    "step 160000 loss 1.2946405708000173\n",
    "...\n",
    "step 999999 loss 1.076907632012897\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang = 'ru_RU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang = 'en_XX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [250004, 146038, 2], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('привет')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250004"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('en_XX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(\n",
    "    text, model, tokenizer, \n",
    "    n=None, \n",
    "    max_length=\"auto\", \n",
    "    beams=5,\n",
    "    src_lang='en_XX',\n",
    "    tgt_lang='en_XX',\n",
    "    **kwargs\n",
    "):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    tokenizer.tgt_lang = tgt_lang\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    tokenizer.tgt_lang\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    if max_length == \"auto\":\n",
    "        max_length = inputs.shape[1] + 10\n",
    "\n",
    "    result = model.generate(\n",
    "        inputs,\n",
    "        num_return_sequences=n or 1,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        repetition_penalty=10.0,\n",
    "        max_length=max_length,\n",
    "        min_length=int(0.5 * max_length),\n",
    "        num_beams=beams,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to play with my dog, you know.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('paraphrase: I like to play with my nice dog.', model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мне нравится играть со своей милой собакой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: I like to play with my nice dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ненавижу играть со своей собачкой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate playing with my fuckin' dog, man.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: Ненавижу играть со своей долбаной собакой.', model, tokenizer, src_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't like playing with my dog.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('paraphrase: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='en_XX'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer from English to Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en2ru = pd.read_csv('detox_en2ru_yandex.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19766, 15)\n",
      "(18488, 15)\n"
     ]
    }
   ],
   "source": [
    "detox_en2ru_filtered = detox_en2ru[\n",
    "    (detox_en2ru.edit_distance_ru >= detox_en2ru.edit_distance_en.quantile(0.01)) \n",
    "    & (detox_en2ru.edit_distance_ru <= detox_en2ru.edit_distance_en.quantile(0.99)) \n",
    "    & (detox_en2ru.edit_sim_ru >= detox_en2ru.edit_sim_en.quantile(0.01)) \n",
    "    & (detox_en2ru.edit_sim_ru <= detox_en2ru.edit_sim_en.quantile(0.99))\n",
    "]\n",
    "\n",
    "print(detox_en2ru.shape)\n",
    "print(detox_en2ru_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>neutral_comment</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>toxic</th>\n",
       "      <th>confidence_toxic</th>\n",
       "      <th>is_match</th>\n",
       "      <th>confidence_is_match</th>\n",
       "      <th>toxic_ru</th>\n",
       "      <th>neutral_ru</th>\n",
       "      <th>edit_distance_en</th>\n",
       "      <th>edit_distance_ru</th>\n",
       "      <th>edit_sim_en</th>\n",
       "      <th>edit_sim_ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16628</th>\n",
       "      <td>111246</td>\n",
       "      <td>where the fuck did that tidbit come from ?</td>\n",
       "      <td>Where did this obscenity come from?</td>\n",
       "      <td>0.996669</td>\n",
       "      <td>reddit</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9536</td>\n",
       "      <td>откуда, черт возьми, взялся этот лакомый кусочек?</td>\n",
       "      <td>Откуда взялась эта непристойность?</td>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.326531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13005</th>\n",
       "      <td>209188</td>\n",
       "      <td>\" but they give a shit if you say you drive a ...</td>\n",
       "      <td>\" but they care if you say you drive a mazda w...</td>\n",
       "      <td>0.963981</td>\n",
       "      <td>reddit</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9727</td>\n",
       "      <td>\" но им насрать , если ты говоришь, что водишь...</td>\n",
       "      <td>\"но им не все равно, если ты говоришь, что вод...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.872093</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13545</th>\n",
       "      <td>100151</td>\n",
       "      <td>should be fucking common sense .</td>\n",
       "      <td>Should be common sense</td>\n",
       "      <td>0.988590</td>\n",
       "      <td>reddit</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9906</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>должен быть гребаный здравый смысл .</td>\n",
       "      <td>Должен быть здравый смысл</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          idx                                      toxic_comment  \\\n",
       "16628  111246         where the fuck did that tidbit come from ?   \n",
       "13005  209188  \" but they give a shit if you say you drive a ...   \n",
       "13545  100151                   should be fucking common sense .   \n",
       "\n",
       "                                         neutral_comment  toxicity_score  \\\n",
       "16628                Where did this obscenity come from?        0.996669   \n",
       "13005  \" but they care if you say you drive a mazda w...        0.963981   \n",
       "13545                             Should be common sense        0.988590   \n",
       "\n",
       "      dataset  toxic  confidence_toxic  is_match  confidence_is_match  \\\n",
       "16628  reddit  False            0.9999      True               0.9536   \n",
       "13005  reddit  False            0.9990      True               0.9727   \n",
       "13545  reddit  False            0.9906      True               0.9565   \n",
       "\n",
       "                                                toxic_ru  \\\n",
       "16628  откуда, черт возьми, взялся этот лакомый кусочек?   \n",
       "13005  \" но им насрать , если ты говоришь, что водишь...   \n",
       "13545               должен быть гребаный здравый смысл .   \n",
       "\n",
       "                                              neutral_ru  edit_distance_en  \\\n",
       "16628                 Откуда взялась эта непристойность?                20   \n",
       "13005  \"но им не все равно, если ты говоришь, что вод...                11   \n",
       "13545                          Должен быть здравый смысл                11   \n",
       "\n",
       "       edit_distance_ru  edit_sim_en  edit_sim_ru  \n",
       "16628                33     0.523810     0.326531  \n",
       "13005                11     0.872093     0.890000  \n",
       "13545                12     0.656250     0.666667  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_en2ru_filtered.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(detox_en2ru_filtered, random_state=1, test_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['detox: this , again , needs context no , this fucking shit does not need any context at all .',\n",
       "  'detox: it always is funny to me when someone religious talks shit about another religions views'],\n",
       " ['this , again , needs context no , this  does not need any context at all .',\n",
       "  'it always is funny to me when someone religious talks bad about another religions views.'],\n",
       " 'en_XX',\n",
       " 'en_XX')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_detox_task_en(batch_size=1):\n",
    "    task = 'detox: '\n",
    "    sample = train.sample(batch_size)\n",
    "    src_id, tgt_id = 'en_XX', 'en_XX'\n",
    "    x = []\n",
    "    y = []\n",
    "    for row in sample.itertuples():\n",
    "        x.append(task + row.toxic_comment)\n",
    "        y.append(row.neutral_comment)\n",
    "    return x, y, src_id, tgt_id\n",
    "\n",
    "get_detox_task_en(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['detox:  и все они были говноедами .',\n",
       "  'detox: я никогда не знал, что они богаты, просто глупые туристы в кричащих рубашках алоха.'],\n",
       " ['все они были любителями поговорить',\n",
       "  'Я никогда не знал, что они богаты, просто туристы в кричащих рубашках алоха.'],\n",
       " 'ru_RU',\n",
       " 'ru_RU')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_detox_task_ru(batch_size=1):\n",
    "    task = 'detox: '\n",
    "    sample = train.sample(batch_size)\n",
    "    src_id, tgt_id = 'ru_RU', 'ru_RU'\n",
    "    x = []\n",
    "    y = []\n",
    "    for row in sample.itertuples():\n",
    "        x.append(task + row.toxic_ru)\n",
    "        y.append(row.neutral_ru)\n",
    "    return x, y, src_id, tgt_id\n",
    "\n",
    "get_detox_task_ru(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size=4):\n",
    "    x = []\n",
    "    y = []\n",
    "    bs = batch_size // 4\n",
    "    for generator in [get_detox_task_en, get_detox_task_ru, get_paraphrase_task, get_translate_task]:\n",
    "        for _ in range(bs):\n",
    "            xx, yy, src_id, tgt_id = generator(1)\n",
    "            tokenizer.src_lang, tokenizer.tgt_lang = src_id, tgt_id\n",
    "            x.append(tokenizer(xx))\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                y.append(tokenizer(yy))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart-multitask-pretrain\n",
      "bart-multitask-v1\n",
      "bart-multitask-v2\n",
      "mbart_5000_EN\n",
      "mbart_5000_RU\n",
      "translate-en2ru-filter75best-rut5\n",
      "translate-en2ru-full_aug_bilingual-mbart\n",
      "translate-en2ru-full_aug_bilingual-mbart-finetune\n",
      "translate-en2ru-full_bilingual-mbart\n",
      "translate-en2ru-full-mbart\n",
      "translate-en2ru-full-rut5\n",
      "translate-en2ru_yandex-full-mbart\n",
      "translate-ru2en-full-mbart\n",
      "translate-ru2en_yandex-full_bilingual-mbart\n"
     ]
    }
   ],
   "source": [
    "!ls /home/dale/models/detox-parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "save_path = '/home/dale/models/detox-parallel/bart-multitask-v3-ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e41338a2854ca79d4e54c61ead07fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000 loss 1.0244991126060508\n",
      "error 1644 CUDA out of memory. Tried to allocate 1.07 GiB (GPU 0; 10.76 GiB total capacity; 7.93 GiB already allocated; 845.69 MiB free; 8.81 GiB reserved in total by PyTorch)\n",
      "step 2000 loss 0.9926564970535389\n",
      "step 3000 loss 0.9637185616676812\n",
      "step 4000 loss 0.9397538064847123\n",
      "step 5000 loss 0.9433629860885504\n",
      "error 5234 CUDA out of memory. Tried to allocate 1.48 GiB (GPU 0; 10.76 GiB total capacity; 8.26 GiB already allocated; 701.69 MiB free; 8.95 GiB reserved in total by PyTorch)\n",
      "error 5238 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 6.83 GiB already allocated; 953.69 MiB free; 8.70 GiB reserved in total by PyTorch)\n",
      "error 5366 CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 10.76 GiB total capacity; 7.74 GiB already allocated; 735.69 MiB free; 8.92 GiB reserved in total by PyTorch)\n",
      "error 5484 CUDA out of memory. Tried to allocate 810.00 MiB (GPU 0; 10.76 GiB total capacity; 7.59 GiB already allocated; 747.69 MiB free; 8.90 GiB reserved in total by PyTorch)\n",
      "error 5930 CUDA out of memory. Tried to allocate 1.03 GiB (GPU 0; 10.76 GiB total capacity; 8.21 GiB already allocated; 759.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "step 6000 loss 0.931296804310276\n",
      "error 6020 CUDA out of memory. Tried to allocate 786.00 MiB (GPU 0; 10.76 GiB total capacity; 7.63 GiB already allocated; 723.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
      "error 6092 CUDA out of memory. Tried to allocate 1016.00 MiB (GPU 0; 10.76 GiB total capacity; 8.41 GiB already allocated; 455.69 MiB free; 9.19 GiB reserved in total by PyTorch)\n",
      "error 6846 CUDA out of memory. Tried to allocate 940.00 MiB (GPU 0; 10.76 GiB total capacity; 8.18 GiB already allocated; 721.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
      "step 7000 loss 0.9247424328486749\n",
      "error 7073 CUDA out of memory. Tried to allocate 894.00 MiB (GPU 0; 10.76 GiB total capacity; 8.23 GiB already allocated; 549.69 MiB free; 9.10 GiB reserved in total by PyTorch)\n",
      "error 7102 CUDA out of memory. Tried to allocate 878.00 MiB (GPU 0; 10.76 GiB total capacity; 8.83 GiB already allocated; 691.69 MiB free; 8.96 GiB reserved in total by PyTorch)\n",
      "step 8000 loss 0.9069169458721802\n",
      "step 9000 loss 0.9114406364817549\n",
      "error 9572 CUDA out of memory. Tried to allocate 1.01 GiB (GPU 0; 10.76 GiB total capacity; 8.68 GiB already allocated; 79.69 MiB free; 9.56 GiB reserved in total by PyTorch)\n",
      "step 10000 loss 0.8935421050869667\n",
      "error 10068 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.19 GiB already allocated; 943.69 MiB free; 8.71 GiB reserved in total by PyTorch)\n",
      "step 11000 loss 0.8908625486680094\n",
      "error 11403 CUDA out of memory. Tried to allocate 1000.00 MiB (GPU 0; 10.76 GiB total capacity; 8.48 GiB already allocated; 477.69 MiB free; 9.17 GiB reserved in total by PyTorch)\n",
      "step 12000 loss 0.8906173676462869\n",
      "step 13000 loss 0.8767853705166644\n",
      "step 14000 loss 0.8808037240341139\n",
      "error 14124 CUDA out of memory. Tried to allocate 1.37 GiB (GPU 0; 10.76 GiB total capacity; 7.79 GiB already allocated; 1.25 GiB free; 8.39 GiB reserved in total by PyTorch)\n",
      "error 14447 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.67 GiB already allocated; 441.69 MiB free; 9.20 GiB reserved in total by PyTorch)\n",
      "error 14963 CUDA out of memory. Tried to allocate 818.00 MiB (GPU 0; 10.76 GiB total capacity; 8.14 GiB already allocated; 619.69 MiB free; 9.03 GiB reserved in total by PyTorch)\n",
      "step 15000 loss 0.859462540557372\n",
      "error 15334 CUDA out of memory. Tried to allocate 826.00 MiB (GPU 0; 10.76 GiB total capacity; 8.56 GiB already allocated; 235.69 MiB free; 9.40 GiB reserved in total by PyTorch)\n",
      "step 16000 loss 0.8605306205445222\n",
      "error 16114 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.20 GiB already allocated; 893.69 MiB free; 8.76 GiB reserved in total by PyTorch)\n",
      "error 16340 CUDA out of memory. Tried to allocate 1.63 GiB (GPU 0; 10.76 GiB total capacity; 8.62 GiB already allocated; 277.69 MiB free; 9.36 GiB reserved in total by PyTorch)\n",
      "error 16355 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.07 GiB already allocated; 943.69 MiB free; 8.71 GiB reserved in total by PyTorch)\n",
      "error 16784 CUDA out of memory. Tried to allocate 902.00 MiB (GPU 0; 10.76 GiB total capacity; 8.15 GiB already allocated; 733.69 MiB free; 8.92 GiB reserved in total by PyTorch)\n",
      "step 17000 loss 0.8554034307169552\n",
      "error 17124 CUDA out of memory. Tried to allocate 2.44 GiB (GPU 0; 10.76 GiB total capacity; 8.06 GiB already allocated; 1.13 GiB free; 8.51 GiB reserved in total by PyTorch)\n",
      "error 17256 CUDA out of memory. Tried to allocate 840.00 MiB (GPU 0; 10.76 GiB total capacity; 7.91 GiB already allocated; 595.69 MiB free; 9.05 GiB reserved in total by PyTorch)\n",
      "error 17428 CUDA out of memory. Tried to allocate 1.59 GiB (GPU 0; 10.76 GiB total capacity; 8.56 GiB already allocated; 899.69 MiB free; 8.76 GiB reserved in total by PyTorch)\n",
      "error 17438 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.05 GiB already allocated; 339.69 MiB free; 9.30 GiB reserved in total by PyTorch)\n",
      "error 17740 CUDA out of memory. Tried to allocate 612.00 MiB (GPU 0; 10.76 GiB total capacity; 7.92 GiB already allocated; 137.69 MiB free; 9.50 GiB reserved in total by PyTorch)\n",
      "step 18000 loss 0.8523610175867705\n",
      "error 18195 CUDA out of memory. Tried to allocate 756.00 MiB (GPU 0; 10.76 GiB total capacity; 7.60 GiB already allocated; 741.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
      "error 18627 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 8.45 GiB already allocated; 739.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
      "error 18663 CUDA out of memory. Tried to allocate 910.00 MiB (GPU 0; 10.76 GiB total capacity; 8.10 GiB already allocated; 499.69 MiB free; 9.15 GiB reserved in total by PyTorch)\n",
      "step 19000 loss 0.8500660683596294\n",
      "error 19455 CUDA out of memory. Tried to allocate 1.03 GiB (GPU 0; 10.76 GiB total capacity; 8.42 GiB already allocated; 525.69 MiB free; 9.12 GiB reserved in total by PyTorch)\n",
      "error 19967 CUDA out of memory. Tried to allocate 810.00 MiB (GPU 0; 10.76 GiB total capacity; 7.68 GiB already allocated; 693.69 MiB free; 8.96 GiB reserved in total by PyTorch)\n",
      "step 20000 loss 0.8443091445981161\n",
      "step 21000 loss 0.8399447210702233\n",
      "error 21535 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.56 GiB already allocated; 695.69 MiB free; 8.96 GiB reserved in total by PyTorch)\n",
      "error 21576 CUDA out of memory. Tried to allocate 902.00 MiB (GPU 0; 10.76 GiB total capacity; 8.00 GiB already allocated; 795.69 MiB free; 8.86 GiB reserved in total by PyTorch)\n",
      "error 21707 CUDA out of memory. Tried to allocate 1.93 GiB (GPU 0; 10.76 GiB total capacity; 8.49 GiB already allocated; 445.69 MiB free; 9.20 GiB reserved in total by PyTorch)\n",
      "error 21722 CUDA out of memory. Tried to allocate 970.00 MiB (GPU 0; 10.76 GiB total capacity; 8.34 GiB already allocated; 561.69 MiB free; 9.09 GiB reserved in total by PyTorch)\n",
      "error 21891 CUDA out of memory. Tried to allocate 1.39 GiB (GPU 0; 10.76 GiB total capacity; 7.81 GiB already allocated; 1.07 GiB free; 8.57 GiB reserved in total by PyTorch)\n",
      "step 22000 loss 0.8369275387818947\n",
      "error 22930 CUDA out of memory. Tried to allocate 764.00 MiB (GPU 0; 10.76 GiB total capacity; 7.66 GiB already allocated; 729.69 MiB free; 8.92 GiB reserved in total by PyTorch)\n",
      "step 23000 loss 0.8199173013360819\n",
      "error 23134 CUDA out of memory. Tried to allocate 1.19 GiB (GPU 0; 10.76 GiB total capacity; 8.96 GiB already allocated; 455.69 MiB free; 9.19 GiB reserved in total by PyTorch)\n",
      "step 24000 loss 0.8179586398337628\n",
      "error 24926 CUDA out of memory. Tried to allocate 1.66 GiB (GPU 0; 10.76 GiB total capacity; 9.04 GiB already allocated; 269.69 MiB free; 9.37 GiB reserved in total by PyTorch)\n",
      "step 25000 loss 0.8118961373924071\n",
      "error 25370 CUDA out of memory. Tried to allocate 910.00 MiB (GPU 0; 10.76 GiB total capacity; 8.39 GiB already allocated; 723.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
      "error 25458 CUDA out of memory. Tried to allocate 1.16 GiB (GPU 0; 10.76 GiB total capacity; 8.60 GiB already allocated; 361.69 MiB free; 9.28 GiB reserved in total by PyTorch)\n",
      "step 26000 loss 0.8053143102273502\n",
      "error 26090 CUDA out of memory. Tried to allocate 818.00 MiB (GPU 0; 10.76 GiB total capacity; 8.18 GiB already allocated; 779.69 MiB free; 8.87 GiB reserved in total by PyTorch)\n",
      "error 26147 CUDA out of memory. Tried to allocate 832.00 MiB (GPU 0; 10.76 GiB total capacity; 7.97 GiB already allocated; 765.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "error 26570 CUDA out of memory. Tried to allocate 932.00 MiB (GPU 0; 10.76 GiB total capacity; 8.49 GiB already allocated; 723.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
      "step 27000 loss 0.8000209590607844\n",
      "error 27086 CUDA out of memory. Tried to allocate 1.37 GiB (GPU 0; 10.76 GiB total capacity; 8.24 GiB already allocated; 1.18 GiB free; 8.45 GiB reserved in total by PyTorch)\n",
      "error 27272 CUDA out of memory. Tried to allocate 924.00 MiB (GPU 0; 10.76 GiB total capacity; 8.55 GiB already allocated; 605.69 MiB free; 9.04 GiB reserved in total by PyTorch)\n",
      "error 27363 CUDA out of memory. Tried to allocate 1008.00 MiB (GPU 0; 10.76 GiB total capacity; 8.24 GiB already allocated; 949.69 MiB free; 8.71 GiB reserved in total by PyTorch)\n",
      "step 28000 loss 0.7989337151313199\n",
      "error 28556 CUDA out of memory. Tried to allocate 878.00 MiB (GPU 0; 10.76 GiB total capacity; 7.82 GiB already allocated; 813.69 MiB free; 8.84 GiB reserved in total by PyTorch)\n",
      "error 28584 CUDA out of memory. Tried to allocate 932.00 MiB (GPU 0; 10.76 GiB total capacity; 7.97 GiB already allocated; 659.69 MiB free; 8.99 GiB reserved in total by PyTorch)\n",
      "error 28608 CUDA out of memory. Tried to allocate 986.00 MiB (GPU 0; 10.76 GiB total capacity; 8.65 GiB already allocated; 549.69 MiB free; 9.10 GiB reserved in total by PyTorch)\n",
      "step 29000 loss 0.8001074789617054\n",
      "error 29492 CUDA out of memory. Tried to allocate 718.00 MiB (GPU 0; 10.76 GiB total capacity; 7.62 GiB already allocated; 665.69 MiB free; 8.98 GiB reserved in total by PyTorch)\n",
      "step 30000 loss 0.793068536195024\n",
      "error 30687 CUDA out of memory. Tried to allocate 2.44 GiB (GPU 0; 10.76 GiB total capacity; 8.06 GiB already allocated; 565.69 MiB free; 9.08 GiB reserved in total by PyTorch)\n",
      "error 30862 CUDA out of memory. Tried to allocate 994.00 MiB (GPU 0; 10.76 GiB total capacity; 7.69 GiB already allocated; 617.69 MiB free; 9.03 GiB reserved in total by PyTorch)\n",
      "error 30974 CUDA out of memory. Tried to allocate 780.00 MiB (GPU 0; 10.76 GiB total capacity; 7.82 GiB already allocated; 565.69 MiB free; 9.08 GiB reserved in total by PyTorch)\n",
      "step 31000 loss 0.7904995776209937\n",
      "error 31167 CUDA out of memory. Tried to allocate 772.00 MiB (GPU 0; 10.76 GiB total capacity; 7.53 GiB already allocated; 743.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
      "error 31854 CUDA out of memory. Tried to allocate 924.00 MiB (GPU 0; 10.76 GiB total capacity; 8.04 GiB already allocated; 811.69 MiB free; 8.84 GiB reserved in total by PyTorch)\n",
      "error 31902 CUDA out of memory. Tried to allocate 1.42 GiB (GPU 0; 10.76 GiB total capacity; 8.32 GiB already allocated; 965.69 MiB free; 8.69 GiB reserved in total by PyTorch)\n",
      "step 32000 loss 0.789260617279361\n",
      "error 32167 CUDA out of memory. Tried to allocate 1.39 GiB (GPU 0; 10.76 GiB total capacity; 8.62 GiB already allocated; 131.69 MiB free; 9.51 GiB reserved in total by PyTorch)\n",
      "error 32215 CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.76 GiB total capacity; 8.14 GiB already allocated; 615.69 MiB free; 9.03 GiB reserved in total by PyTorch)\n",
      "step 33000 loss 0.7820524797665493\n",
      "error 33092 CUDA out of memory. Tried to allocate 1.66 GiB (GPU 0; 10.76 GiB total capacity; 8.81 GiB already allocated; 689.69 MiB free; 8.96 GiB reserved in total by PyTorch)\n",
      "error 33603 CUDA out of memory. Tried to allocate 864.00 MiB (GPU 0; 10.76 GiB total capacity; 7.85 GiB already allocated; 845.69 MiB free; 8.81 GiB reserved in total by PyTorch)\n",
      "error 33987 CUDA out of memory. Tried to allocate 664.00 MiB (GPU 0; 10.76 GiB total capacity; 7.56 GiB already allocated; 607.69 MiB free; 9.04 GiB reserved in total by PyTorch)\n",
      "step 34000 loss 0.7832612387671116\n",
      "error 34746 CUDA out of memory. Tried to allocate 772.00 MiB (GPU 0; 10.76 GiB total capacity; 7.82 GiB already allocated; 569.69 MiB free; 9.08 GiB reserved in total by PyTorch)\n",
      "error 34975 CUDA out of memory. Tried to allocate 1.19 GiB (GPU 0; 10.76 GiB total capacity; 7.78 GiB already allocated; 1.11 GiB free; 8.53 GiB reserved in total by PyTorch)\n",
      "error 34992 CUDA out of memory. Tried to allocate 1.05 GiB (GPU 0; 10.76 GiB total capacity; 8.96 GiB already allocated; 459.69 MiB free; 9.19 GiB reserved in total by PyTorch)\n",
      "step 35000 loss 0.7764514238221701\n",
      "error 35446 CUDA out of memory. Tried to allocate 886.00 MiB (GPU 0; 10.76 GiB total capacity; 8.39 GiB already allocated; 783.69 MiB free; 8.87 GiB reserved in total by PyTorch)\n",
      "error 35452 CUDA out of memory. Tried to allocate 1.04 GiB (GPU 0; 10.76 GiB total capacity; 8.52 GiB already allocated; 303.69 MiB free; 9.34 GiB reserved in total by PyTorch)\n",
      "step 36000 loss 0.7796234257103892\n",
      "error 36234 CUDA out of memory. Tried to allocate 1.10 GiB (GPU 0; 10.76 GiB total capacity; 8.66 GiB already allocated; 683.69 MiB free; 8.97 GiB reserved in total by PyTorch)\n",
      "error 36378 CUDA out of memory. Tried to allocate 1.25 GiB (GPU 0; 10.76 GiB total capacity; 8.95 GiB already allocated; 355.69 MiB free; 9.29 GiB reserved in total by PyTorch)\n",
      "step 37000 loss 0.7755592864149082\n",
      "error 37952 CUDA out of memory. Tried to allocate 1.11 GiB (GPU 0; 10.76 GiB total capacity; 8.49 GiB already allocated; 455.69 MiB free; 9.19 GiB reserved in total by PyTorch)\n",
      "step 38000 loss 0.7690174856175005\n",
      "step 39000 loss 0.7576416058233201\n",
      "error 39223 CUDA out of memory. Tried to allocate 1.41 GiB (GPU 0; 10.76 GiB total capacity; 7.91 GiB already allocated; 855.69 MiB free; 8.80 GiB reserved in total by PyTorch)\n",
      "error 39298 CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 10.76 GiB total capacity; 7.98 GiB already allocated; 183.69 MiB free; 9.46 GiB reserved in total by PyTorch)\n",
      "error 39718 CUDA out of memory. Tried to allocate 916.00 MiB (GPU 0; 10.76 GiB total capacity; 7.86 GiB already allocated; 803.69 MiB free; 8.85 GiB reserved in total by PyTorch)\n",
      "step 40000 loss 0.7528746082341623\n",
      "error 40220 CUDA out of memory. Tried to allocate 1000.00 MiB (GPU 0; 10.76 GiB total capacity; 8.48 GiB already allocated; 699.69 MiB free; 8.95 GiB reserved in total by PyTorch)\n",
      "error 40882 CUDA out of memory. Tried to allocate 1.03 GiB (GPU 0; 10.76 GiB total capacity; 7.70 GiB already allocated; 1.02 GiB free; 8.62 GiB reserved in total by PyTorch)\n",
      "step 41000 loss 0.7465217740081879\n",
      "error 41159 CUDA out of memory. Tried to allocate 840.00 MiB (GPU 0; 10.76 GiB total capacity; 7.64 GiB already allocated; 731.69 MiB free; 8.92 GiB reserved in total by PyTorch)\n",
      "error 41194 CUDA out of memory. Tried to allocate 704.00 MiB (GPU 0; 10.76 GiB total capacity; 8.11 GiB already allocated; 485.69 MiB free; 9.16 GiB reserved in total by PyTorch)\n",
      "error 41584 CUDA out of memory. Tried to allocate 826.00 MiB (GPU 0; 10.76 GiB total capacity; 7.88 GiB already allocated; 551.69 MiB free; 9.10 GiB reserved in total by PyTorch)\n",
      "step 42000 loss 0.7484442370524438\n",
      "error 42551 CUDA out of memory. Tried to allocate 940.00 MiB (GPU 0; 10.76 GiB total capacity; 8.38 GiB already allocated; 737.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
      "error 42883 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.47 GiB already allocated; 635.69 MiB free; 9.01 GiB reserved in total by PyTorch)\n",
      "error 42934 CUDA out of memory. Tried to allocate 1.10 GiB (GPU 0; 10.76 GiB total capacity; 8.85 GiB already allocated; 703.69 MiB free; 8.95 GiB reserved in total by PyTorch)\n",
      "step 43000 loss 0.7543446287842915\n",
      "error 43843 CUDA out of memory. Tried to allocate 878.00 MiB (GPU 0; 10.76 GiB total capacity; 8.40 GiB already allocated; 261.69 MiB free; 9.38 GiB reserved in total by PyTorch)\n",
      "step 44000 loss 0.7454149801897323\n",
      "error 44695 CUDA out of memory. Tried to allocate 1.11 GiB (GPU 0; 10.76 GiB total capacity; 8.49 GiB already allocated; 127.69 MiB free; 9.51 GiB reserved in total by PyTorch)\n",
      "step 45000 loss 0.7410682477039187\n",
      "error 45094 CUDA out of memory. Tried to allocate 1.06 GiB (GPU 0; 10.76 GiB total capacity; 7.89 GiB already allocated; 897.69 MiB free; 8.76 GiB reserved in total by PyTorch)\n",
      "error 45972 CUDA out of memory. Tried to allocate 948.00 MiB (GPU 0; 10.76 GiB total capacity; 8.16 GiB already allocated; 759.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "step 46000 loss 0.7233194315090705\n",
      "step 47000 loss 0.723059124236168\n",
      "error 47723 CUDA out of memory. Tried to allocate 802.00 MiB (GPU 0; 10.76 GiB total capacity; 8.62 GiB already allocated; 115.69 MiB free; 9.52 GiB reserved in total by PyTorch)\n",
      "step 48000 loss 0.7243239084520445\n",
      "error 48818 CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 10.76 GiB total capacity; 8.22 GiB already allocated; 649.69 MiB free; 9.00 GiB reserved in total by PyTorch)\n",
      "step 49000 loss 0.7227016722296297\n",
      "error 49302 CUDA out of memory. Tried to allocate 1.42 GiB (GPU 0; 10.76 GiB total capacity; 8.32 GiB already allocated; 1.12 GiB free; 8.51 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "tq = trange(0, 50_000)\n",
    "cleanup()\n",
    "\n",
    "for i in tq:\n",
    "    x, y = get_batch(batch_size)\n",
    "    xpad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in x])\n",
    "    ypad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in y])\n",
    "    \n",
    "    try:\n",
    "        labels = torch.tensor(ypad['input_ids'], device=model.device)\n",
    "        labels[labels==tokenizer.pad_token_id] = -100\n",
    "        loss = model(\n",
    "            input_ids=torch.tensor(xpad['input_ids'], device=model.device),\n",
    "            attention_mask=torch.tensor(xpad['attention_mask'], device=model.device),\n",
    "            labels=labels,\n",
    "        ).loss\n",
    "        loss.backward()\n",
    "        if i % gradient_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "    except RuntimeError as e:\n",
    "        print('error', i, e)\n",
    "        loss = None\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        cleanup()\n",
    "        continue\n",
    "\n",
    "    w = 1 / max(1, min(i, window))\n",
    "    ewm_loss = ewm_loss * (1-w) + loss.item() * w\n",
    "    tq.set_description(f'{ewm_loss:3.4f}')\n",
    "\n",
    "    if i > 0 and i % report_steps == 0:\n",
    "        print('step', i, 'loss', ewm_loss, )\n",
    "        if i > 0:\n",
    "            model.save_pretrained(save_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "    if i % cleanup_steps == 0:\n",
    "        cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd like to play with my dog.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('paraphrase: I like to play with my nice dog.', model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я люблю играть со своей милой собакой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: I like to play with my nice dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я ненавижу играть со своей собачкой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate playing with my fucking dog. Oh, no!\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: Ненавижу играть со своей долбаной собакой.', model, tokenizer, src_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't like to play with my dog.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('paraphrase: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='en_XX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't like to play with my dog.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('detox: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='en_XX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ненавижу играть со своей собакой, черт возьми.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('detox: Ненавижу играть со своей долбаной собакой.', model, tokenizer, src_lang='ru_RU', tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/russian_data/test.tsv', sep='\\t')\n",
    "test_inputs = test_data[\"toxic_comment\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73a2d97474c4f9db098e174dd52745f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs = [paraphrase('detox: ' + text, model, tokenizer, src_lang='ru_RU', tgt_lang='ru_RU') for text in tqdm(test_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/multitask-1M-translate-yandex-v3/'  # partial training results; need to train further.\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'results_ru.txt', 'w') as f:\n",
    "    for line in test_outputs:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full results (1m steps)  + 50K with Russian translated data\n",
    "```\n",
    "python evaluate_ru.py \\\n",
    "    --result_filename scores \\\n",
    "    --input_dir results/multitask-1M-translate-yandex-v3 \\\n",
    "    --output_dir results\n",
    "    \n",
    "Style accuracy:       0.6365973949432373\n",
    "Meaning preservation: 0.861542284488678\n",
    "Joint fluency:        -0.09916463494300842\n",
    "Joint score:          -0.052923914045095444\n",
    "Scores after calibration:\n",
    "Style accuracy:       0.6729376912117004\n",
    "Meaning preservation: 0.792417585849762\n",
    "Joint fluency:        0.8859606981277466\n",
    "Joint score:          0.4621787667274475\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3k",
   "language": "python",
   "name": "p3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
