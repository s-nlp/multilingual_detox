{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Paraphrasing: https://huggingface.co/datasets/GEM/opusparcus\n",
    "* Translation: https://huggingface.co/datasets/open_subtitles + news_commentary? + tatoeba?\n",
    "* Detox: ordinary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opusparcus (/home/dale/.cache/huggingface/datasets/GEM___opusparcus/en.80/1.0.0/79d36ae4eced4f3c2c5a2ab9f94a584de7adca957186408d33798d0d87b018f2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715e9c67ad6c44beb38e1a8ee24795ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 982\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1015\n",
       "    })\n",
       "    test.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1445\n",
       "    })\n",
       "    validation.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1455\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 5200000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opus_para_en = load_dataset(\"GEM/opusparcus\", \"en.80\")\n",
    "opus_para_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opusparcus (/home/dale/.cache/huggingface/datasets/GEM___opusparcus/ru.80/1.0.0/79d36ae4eced4f3c2c5a2ab9f94a584de7adca957186408d33798d0d87b018f2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda72dff423f4ae2a8919e246ab7d527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1068\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1020\n",
       "    })\n",
       "    test.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1855\n",
       "    })\n",
       "    validation.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1854\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 2300000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opus_para_ru = load_dataset(\"GEM/opusparcus\", \"ru.80\")\n",
    "opus_para_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lang': 'ru',\n",
       " 'input': 'Ты прости.',\n",
       " 'target': 'Сожалею.',\n",
       " 'annot_score': 0.0,\n",
       " 'gem_id': 'gem-opusparcus-train-80334819',\n",
       " 'references': ['Сожалею.']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(opus_para_ru['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-ru-lang1=en,lang2=ru\n",
      "Reusing dataset open_subtitles (/home/dale/.cache/huggingface/datasets/open_subtitles/en-ru-lang1=en,lang2=ru/0.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a9699cae0f4831a7dd7be9983b3fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'meta', 'translation'],\n",
       "        num_rows: 25910105\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opensub = load_dataset(\"open_subtitles\", lang1=\"en\", lang2='ru')\n",
    "opensub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '22222006',\n",
       " 'meta': {'year': 2015,\n",
       "  'imdbId': 4516410,\n",
       "  'subtitleId': {'en': 6571424, 'ru': 6126244},\n",
       "  'sentenceIds': {'en': [543], 'ru': [522]}},\n",
       " 'translation': {'en': '- Look here.', 'ru': '- Посмотри'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(opensub['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-ru-lang1=en,lang2=ru\n",
      "Reusing dataset news_commentary (/home/dale/.cache/huggingface/datasets/news_commentary/en-ru-lang1=en,lang2=ru/0.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a685cea2296470e8057de9c6baddffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 190104\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_commentary = load_dataset(\"news_commentary\", lang1=\"en\", lang2='ru')\n",
    "news_commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '96821',\n",
       " 'translation': {'en': 'For the first time in its troubled history, Romania has nothing to fear from its neighbours.',\n",
       "  'ru': 'Впервые за всю свою беспокойную историю Румынии нечего опасаться своих соседей.'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(news_commentary['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-ru-lang1=en,lang2=ru\n",
      "Reusing dataset tatoeba (/home/dale/.cache/huggingface/datasets/tatoeba/en-ru-lang1=en,lang2=ru/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb1167eb2b64bfa83adfb4022489cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 523656\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tatoeba = load_dataset(\"tatoeba\", lang1=\"en\", lang2='ru')\n",
    "tatoeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '166210',\n",
       " 'translation': {'en': \"It's beautiful here.\", 'ru': 'Здесь красиво.'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(tatoeba['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['paraphrase: Я главный агент Уорен Расс.'],\n",
       " ['Здесь я главный.'],\n",
       " 'ru_RU',\n",
       " 'ru_RU')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_paraphrase_task(batch_size=1):\n",
    "    task = 'paraphrase: '\n",
    "    if random.random() < 0.5:\n",
    "        src = opus_para_en\n",
    "        src_id, tgt_id = 'en_XX', 'en_XX'\n",
    "    else:\n",
    "        src = opus_para_ru\n",
    "        src_id, tgt_id = 'ru_RU', 'ru_RU'\n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(batch_size):\n",
    "        item = random.choice(src['train'])\n",
    "        xx, yy = item['input'], item['target']\n",
    "        if random.random() < 0.5:\n",
    "            xx, yy = yy, xx\n",
    "        x.append(task + xx)\n",
    "        y.append(yy)\n",
    "    return x, y, src_id, tgt_id\n",
    "        \n",
    "get_paraphrase_task(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['translate: Давайте начнем отсюда.',\n",
       "  'translate: Как давно вы здесь живёте?'],\n",
       " [\"Let's start here.\", 'How long have you been living here?'],\n",
       " 'ru_RU',\n",
       " 'en_XX')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_translate_task(batch_size=1):\n",
    "    task = 'translate: '\n",
    "    src = random.choice([tatoeba, opensub, news_commentary])\n",
    "    src_id, tgt_id = 'ru_RU', 'en_XX'\n",
    "    if random.random() < 0.5:\n",
    "        src_id, tgt_id = tgt_id, src_id\n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(batch_size):\n",
    "        item = random.choice(src['train'])\n",
    "        x.append(task + item['translation'][src_id[:2]])\n",
    "        y.append(item['translation'][tgt_id[:2]])\n",
    "    return x, y, src_id, tgt_id\n",
    "\n",
    "get_translate_task(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en_train = pd.read_csv('../data/english_data/en_data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "detox_train, detox_val = train_test_split(detox_en_train, random_state=1, test_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.6 ms, sys: 288 µs, total: 1.89 ms\n",
      "Wall time: 1.77 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>neutral_comment</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>toxic</th>\n",
       "      <th>confidence_toxic</th>\n",
       "      <th>is_match</th>\n",
       "      <th>confidence_is_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6395</th>\n",
       "      <td>83656</td>\n",
       "      <td>don 't bother yourself with that shit .</td>\n",
       "      <td>don 't bother yourself with that situation</td>\n",
       "      <td>0.998556</td>\n",
       "      <td>twitter</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9786</th>\n",
       "      <td>54986</td>\n",
       "      <td>u just gon recite the whole damn song huh ? ?</td>\n",
       "      <td>u just gon recite the whole  song</td>\n",
       "      <td>0.996870</td>\n",
       "      <td>twitter</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6291</th>\n",
       "      <td>7659</td>\n",
       "      <td>no respect from the bitches ?</td>\n",
       "      <td>no respect from the girls ?</td>\n",
       "      <td>0.996808</td>\n",
       "      <td>twitter</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        idx                                  toxic_comment  \\\n",
       "6395  83656        don 't bother yourself with that shit .   \n",
       "9786  54986  u just gon recite the whole damn song huh ? ?   \n",
       "6291   7659                  no respect from the bitches ?   \n",
       "\n",
       "                                 neutral_comment  toxicity_score  dataset  \\\n",
       "6395  don 't bother yourself with that situation        0.998556  twitter   \n",
       "9786           u just gon recite the whole  song        0.996870  twitter   \n",
       "6291                 no respect from the girls ?        0.996808  twitter   \n",
       "\n",
       "      toxic  confidence_toxic  is_match  confidence_is_match  \n",
       "6395  False            0.9985      True               0.9862  \n",
       "9786  False            0.9985      True               0.9900  \n",
       "6291  False            0.9996      True               0.9302  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "detox_train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['detox: you rap like a fucking 40 year old bitch .',\n",
       "  'detox: the shit you just said !'],\n",
       " ['Your rap is old.', 'What did you just say?'],\n",
       " 'en_XX',\n",
       " 'en_XX')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_detox_task(batch_size=1):\n",
    "    task = 'detox: '\n",
    "    sample = detox_train.sample(batch_size)\n",
    "    src_id, tgt_id = 'en_XX', 'en_XX'\n",
    "    x = []\n",
    "    y = []\n",
    "    for row in sample.itertuples():\n",
    "        x.append(task + row.toxic_comment)\n",
    "        y.append(row.neutral_comment)\n",
    "    return x, y, src_id, tgt_id\n",
    "\n",
    "get_detox_task(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/home/dale/models/detox-parallel/mbart_5000_EN'\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/mbart-large-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size=3):\n",
    "    x = []\n",
    "    y = []\n",
    "    bs = batch_size // 3\n",
    "    for generator in [get_detox_task, get_paraphrase_task, get_translate_task]:\n",
    "        for _ in range(bs):\n",
    "            xx, yy, src_id, tgt_id = generator(1)\n",
    "            tokenizer.src_lang, tokenizer.tgt_lang = src_id, tgt_id\n",
    "            x.append(tokenizer(xx))\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                y.append(tokenizer(yy))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'input_ids': [[250004, 162770, 12, 1836, 1221, 765, 70, 3428, 68818, 21771, 2685, 47, 63284, 100592, 13, 831, 242, 18, 54, 93669, 1672, 442, 6, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 121, 179665, 184, 12, 1509, 81880, 21136, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 3900, 19309, 12, 138521, 5844, 538, 4, 642, 621, 35782, 214, 23, 70, 78431, 26548, 8999, 1926, 1505, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}],\n",
       " [{'input_ids': [[250004, 10660, 1221, 765, 70, 3428, 68818, 21771, 2685, 47, 63284, 82739, 831, 242, 18, 54, 23937, 1672, 442, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 1509, 81880, 21136, 4, 112613, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 417, 130390, 419, 71459, 103, 2176, 591, 130525, 92890, 203981, 135, 11373, 55994, 100252, 419, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batch(3)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_XX detox: they will have the troops already there to ensure ukraine can 't do shit about it.</s>\n",
      "en_XX They will have the troops already there to ensure Ukraine can 't do anything about it.</s>\n",
      "ru_RU paraphrase: Я люблю тебя.</s>\n",
      "ru_RU Я люблю тебя, Мама.</s>\n",
      "en_XX translate: Collectively, we are failing in the fight against world hunger.</s>\n",
      "ru_RU В глобальном масштабе мы проигрываем войну с мировым голодом.</s>\n"
     ]
    }
   ],
   "source": [
    "for xx, yy in zip(x, y):\n",
    "    print(tokenizer.decode(xx['input_ids'][0]))\n",
    "    print(tokenizer.decode(yy['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[250004, 162770, 12, 764, 1556, 11343, 194663, 1884, 142, 68649, 3853, 20271, 70, 4568, 99656, 28021, 6, 5, 2], [250004, 121, 179665, 184, 12, 10868, 32, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [250021, 3900, 19309, 12, 20, 16434, 77, 67008, 4, 414, 784, 111379, 38, 2, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad([{k: v[0] for k, v in item.items()} for item in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor\n",
    "optimizer = Adafactor(model.parameters(), scale_parameter=False, relative_step=False, lr=1e-5, clip_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "num_warmup_steps = 1000\n",
    "\n",
    "def lr_lambda(current_step: int):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1.0, num_warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_steps = 4\n",
    "batch_size = 6\n",
    "window = 1000\n",
    "report_steps = 1000\n",
    "cleanup_steps = 100\n",
    "save_path = '/home/dale/models/detox-parallel/bart-multitask-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewm_loss = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ~6 iterations per second, 1M iteration takes 1000000/6/60/60=45 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2177e771ac3c45c0864948b0fba3f648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/401057 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 599000 loss 0.791350449541119\n",
      "step 600000 loss 0.7908788145488161\n",
      "error 600904 CUDA out of memory. Tried to allocate 906.00 MiB (GPU 0; 10.76 GiB total capacity; 8.02 GiB already allocated; 785.69 MiB free; 8.87 GiB reserved in total by PyTorch)\n",
      "step 601000 loss 0.7923423596750206\n",
      "step 602000 loss 0.7940523141718425\n",
      "error 602326 CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 10.76 GiB total capacity; 8.14 GiB already allocated; 799.69 MiB free; 8.85 GiB reserved in total by PyTorch)\n",
      "step 603000 loss 0.790953011557408\n",
      "error 603566 CUDA out of memory. Tried to allocate 870.00 MiB (GPU 0; 10.76 GiB total capacity; 8.08 GiB already allocated; 815.69 MiB free; 8.84 GiB reserved in total by PyTorch)\n",
      "step 604000 loss 0.8040739516727109\n",
      "step 605000 loss 0.7983863036420874\n",
      "step 606000 loss 0.8064560542508245\n",
      "step 607000 loss 0.8109014954943938\n",
      "step 608000 loss 0.8060132506587476\n",
      "step 609000 loss 0.8090561751829025\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "tq = trange(598943, 1_000_000)\n",
    "cleanup()\n",
    "\n",
    "for i in tq:\n",
    "    x, y = get_batch(batch_size)\n",
    "    xpad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in x])\n",
    "    ypad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in y])\n",
    "    \n",
    "    try:\n",
    "        labels = torch.tensor(ypad['input_ids'], device=model.device)\n",
    "        labels[labels==tokenizer.pad_token_id] = -100\n",
    "        loss = model(\n",
    "            input_ids=torch.tensor(xpad['input_ids'], device=model.device),\n",
    "            attention_mask=torch.tensor(xpad['attention_mask'], device=model.device),\n",
    "            labels=labels,\n",
    "        ).loss\n",
    "        loss.backward()\n",
    "        if i % gradient_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "    except RuntimeError as e:\n",
    "        print('error', i, e)\n",
    "        loss = None\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        cleanup()\n",
    "        continue\n",
    "\n",
    "    w = 1 / max(1, min(i, window))\n",
    "    ewm_loss = ewm_loss * (1-w) + loss.item() * w\n",
    "    tq.set_description(f'{ewm_loss:3.4f}')\n",
    "\n",
    "    if i > 0 and i % report_steps == 0:\n",
    "        print('step', i, 'loss', ewm_loss, )\n",
    "        if i > 0:\n",
    "            model.save_pretrained(save_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "    if i % cleanup_steps == 0:\n",
    "        cleanup()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999999 0.7528545096685079\n"
     ]
    }
   ],
   "source": [
    "print(i, ewm_loss)\n",
    "# step 141000 loss 0.9336458999525007\n",
    "# 598943 0.7913156676625156\n",
    "# 999999 0.7528545096685079"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "```\n",
    "step 1000 loss 1.5705595690906056\n",
    "error 1146 CUDA out of memory. Tried to allocate 1.35 GiB (GPU 0; 10.76 GiB total capacity; 8.85 GiB already allocated; 475.69 MiB free; 9.17 GiB reserved in total by PyTorch)\n",
    "step 2000 loss 1.5289358124251258\n",
    "step 3000 loss 1.4603064465819475\n",
    "step 4000 loss 1.4280587719245064\n",
    "error 4731 CUDA out of memory. Tried to allocate 916.00 MiB (GPU 0; 10.76 GiB total capacity; 8.11 GiB already allocated; 801.69 MiB free; 8.85 GiB reserved in total by PyTorch)\n",
    "error 4840 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.16 GiB already allocated; 867.69 MiB free; 8.79 GiB reserved in total by PyTorch)\n",
    "step 5000 loss 1.3723411050480274\n",
    "step 6000 loss 1.3470262561429978\n",
    "step 7000 loss 1.3337626059557242\n",
    "step 8000 loss 1.3178974970698947\n",
    "error 8569 CUDA out of memory. Tried to allocate 3.58 GiB (GPU 0; 10.76 GiB total capacity; 6.39 GiB already allocated; 3.11 GiB free; 6.53 GiB reserved in total by PyTorch)\n",
    "step 9000 loss 1.3109540848422108\n",
    "step 10000 loss 1.3099768834329912\n",
    "step 11000 loss 1.2894572959109634\n",
    "error 11954 CUDA out of memory. Tried to allocate 1.26 GiB (GPU 0; 10.76 GiB total capacity; 8.24 GiB already allocated; 633.69 MiB free; 9.02 GiB reserved in total by PyTorch)\n",
    "step 12000 loss 1.2661666174786903\n",
    "step 13000 loss 1.2544034908444262\n",
    "step 14000 loss 1.2537943159628\n",
    "step 15000 loss 1.2417901334227581\n",
    "error 15970 CUDA out of memory. Tried to allocate 780.00 MiB (GPU 0; 10.76 GiB total capacity; 7.67 GiB already allocated; 767.69 MiB free; 8.88 GiB reserved in total by PyTorch)\n",
    "step 16000 loss 1.2500597551439607\n",
    "step 17000 loss 1.2311876436254687\n",
    "step 18000 loss 1.2300539663708037\n",
    "step 19000 loss 1.214656587177018\n",
    "step 20000 loss 1.209378143381565\n",
    "step 21000 loss 1.2033179850564317\n",
    "step 22000 loss 1.1968057551352824\n",
    "step 23000 loss 1.176260400882675\n",
    "error 23584 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.28 GiB already allocated; 829.69 MiB free; 8.82 GiB reserved in total by PyTorch)\n",
    "step 24000 loss 1.1754935866907865\n",
    "error 24896 CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 10.76 GiB total capacity; 9.44 GiB already allocated; 59.69 MiB free; 9.58 GiB reserved in total by PyTorch)\n",
    "step 25000 loss 1.1754272553817264\n",
    "step 26000 loss 1.1656607958886298\n",
    "step 27000 loss 1.1570394344844919\n",
    "step 28000 loss 1.1558873855161609\n",
    "step 29000 loss 1.165974972071494\n",
    "step 30000 loss 1.1599093132613605\n",
    "step 31000 loss 1.1541310660451913\n",
    "error 31282 CUDA out of memory. Tried to allocate 1.19 GiB (GPU 0; 10.76 GiB total capacity; 8.76 GiB already allocated; 287.69 MiB free; 9.35 GiB reserved in total by PyTorch)\n",
    "step 32000 loss 1.1364641991741162\n",
    "step 33000 loss 1.132973628288517\n",
    "error 33624 CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 10.76 GiB total capacity; 9.44 GiB already allocated; 79.69 MiB free; 9.56 GiB reserved in total by PyTorch)\n",
    "step 34000 loss 1.1286950876108275\n",
    "step 35000 loss 1.1291851942061415\n",
    "step 36000 loss 1.1235886658742347\n",
    "step 37000 loss 1.1246682009640214\n",
    "step 38000 loss 1.1135467563562322\n",
    "step 39000 loss 1.1149872643497183\n",
    "step 40000 loss 1.1017778124035793\n",
    "step 41000 loss 1.1060003889083563\n",
    "error 41735 CUDA out of memory. Tried to allocate 870.00 MiB (GPU 0; 10.76 GiB total capacity; 8.07 GiB already allocated; 859.69 MiB free; 8.79 GiB reserved in total by PyTorch)\n",
    "step 42000 loss 1.1078476631524794\n",
    "step 43000 loss 1.100026474680707\n",
    "step 44000 loss 1.095670024500316\n",
    "step 45000 loss 1.109476816996912\n",
    "step 46000 loss 1.1119795778634898\n",
    "step 47000 loss 1.1311553555564193\n",
    "step 48000 loss 1.113347318553216\n",
    "step 49000 loss 1.1016645401565517\n",
    "step 50000 loss 1.1220686658058219\n",
    "error 50454 CUDA out of memory. Tried to allocate 916.00 MiB (GPU 0; 10.76 GiB total capacity; 7.88 GiB already allocated; 821.69 MiB free; 8.83 GiB reserved in total by PyTorch)\n",
    "step 51000 loss 1.1214904008655675\n",
    "step 52000 loss 1.0854113000333645\n",
    "step 53000 loss 1.088154120166232\n",
    "step 54000 loss 1.093514120254794\n",
    "step 55000 loss 1.0922690118309737\n",
    "error 55998 CUDA out of memory. Tried to allocate 952.00 MiB (GPU 0; 10.76 GiB total capacity; 8.01 GiB already allocated; 695.69 MiB free; 8.96 GiB reserved in total by PyTorch)\n",
    "step 56000 loss 1.0800373847786937\n",
    "step 57000 loss 1.0920900005165737\n",
    "step 58000 loss 1.0736909533657972\n",
    "step 59000 loss 1.0752592100786067\n",
    "step 60000 loss 1.0637662286778657\n",
    "step 61000 loss 1.0654742857820474\n",
    "error 61504 CUDA out of memory. Tried to allocate 946.00 MiB (GPU 0; 10.76 GiB total capacity; 7.89 GiB already allocated; 757.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
    "step 62000 loss 1.0565901908696624\n",
    "step 63000 loss 1.057395690647219\n",
    "step 64000 loss 1.0477948828239614\n",
    "step 65000 loss 1.0296297263268868\n",
    "error 65595 CUDA out of memory. Tried to allocate 1.07 GiB (GPU 0; 10.76 GiB total capacity; 8.30 GiB already allocated; 781.69 MiB free; 8.87 GiB reserved in total by PyTorch)\n",
    "step 66000 loss 1.0449434271303466\n",
    "step 67000 loss 1.0265650949758665\n",
    "step 68000 loss 1.025196188228675\n",
    "error 68646 CUDA out of memory. Tried to allocate 1.03 GiB (GPU 0; 10.76 GiB total capacity; 8.18 GiB already allocated; 1011.69 MiB free; 8.65 GiB reserved in total by PyTorch)\n",
    "step 69000 loss 1.040014132410023\n",
    "step 70000 loss 1.0401423453181597\n",
    "step 71000 loss 1.0255363661922454\n",
    "step 72000 loss 1.025135515281328\n",
    "step 73000 loss 1.0365278080704476\n",
    "step 74000 loss 1.0183107856778406\n",
    "error 74386 CUDA out of memory. Tried to allocate 894.00 MiB (GPU 0; 10.76 GiB total capacity; 8.19 GiB already allocated; 739.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
    "step 75000 loss 1.0290407528180614\n",
    "step 76000 loss 1.028850245665216\n",
    "step 77000 loss 1.0279383226408405\n",
    "step 78000 loss 1.022980517921964\n",
    "step 79000 loss 1.0177867619738628\n",
    "step 80000 loss 1.0196264535815993\n",
    "step 81000 loss 1.0091859735396909\n",
    "step 82000 loss 1.0045839575710827\n",
    "step 83000 loss 0.9986900074678753\n",
    "step 84000 loss 0.9907349173959362\n",
    "error 84856 CUDA out of memory. Tried to allocate 934.00 MiB (GPU 0; 10.76 GiB total capacity; 7.84 GiB already allocated; 743.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
    "step 85000 loss 1.002218034634077\n",
    "step 86000 loss 0.9975985926845609\n",
    "error 86519 CUDA out of memory. Tried to allocate 1.04 GiB (GPU 0; 10.76 GiB total capacity; 8.31 GiB already allocated; 475.69 MiB free; 9.17 GiB reserved in total by PyTorch)\n",
    "error 86791 CUDA out of memory. Tried to allocate 946.00 MiB (GPU 0; 10.76 GiB total capacity; 8.18 GiB already allocated; 531.69 MiB free; 9.12 GiB reserved in total by PyTorch)\n",
    "step 87000 loss 0.999982904219524\n",
    "error 87064 CUDA out of memory. Tried to allocate 968.00 MiB (GPU 0; 10.76 GiB total capacity; 8.10 GiB already allocated; 763.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
    "error 87252 CUDA out of memory. Tried to allocate 1.14 GiB (GPU 0; 10.76 GiB total capacity; 8.50 GiB already allocated; 175.69 MiB free; 9.46 GiB reserved in total by PyTorch)\n",
    "step 88000 loss 1.0042518996963314\n",
    "error 88427 CUDA out of memory. Tried to allocate 912.00 MiB (GPU 0; 10.76 GiB total capacity; 7.85 GiB already allocated; 861.69 MiB free; 8.79 GiB reserved in total by PyTorch)\n",
    "step 89000 loss 0.9853234168833125\n",
    "step 90000 loss 0.9924546283292504\n",
    "step 91000 loss 0.9888159920285438\n",
    "step 92000 loss 0.9908827874992283\n",
    "step 93000 loss 0.9790343402052905\n",
    "error 93459 CUDA out of memory. Tried to allocate 922.00 MiB (GPU 0; 10.76 GiB total capacity; 7.89 GiB already allocated; 769.69 MiB free; 8.88 GiB reserved in total by PyTorch)\n",
    "step 94000 loss 0.9948122169551259\n",
    "error 94568 CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 10.76 GiB total capacity; 8.75 GiB already allocated; 93.69 MiB free; 9.54 GiB reserved in total by PyTorch)\n",
    "error 94798 CUDA out of memory. Tried to allocate 734.00 MiB (GPU 0; 10.76 GiB total capacity; 7.62 GiB already allocated; 599.69 MiB free; 9.05 GiB reserved in total by PyTorch)\n",
    "step 95000 loss 0.9982311906577338\n",
    "step 96000 loss 0.9977218772548391\n",
    "step 97000 loss 0.9998095549900895\n",
    "step 98000 loss 0.9892420303142296\n",
    "step 99000 loss 1.0010712033357734\n",
    "error 99226 CUDA out of memory. Tried to allocate 968.00 MiB (GPU 0; 10.76 GiB total capacity; 8.47 GiB already allocated; 797.69 MiB free; 8.86 GiB reserved in total by PyTorch)\n",
    "step 100000 loss 0.993382955842513\n",
    "step 101000 loss 0.989972051978362\n",
    "step 102000 loss 0.9816771483956006\n",
    "error 102955 CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 10.76 GiB total capacity; 7.56 GiB already allocated; 777.69 MiB free; 8.88 GiB reserved in total by PyTorch)\n",
    "step 103000 loss 0.9892074373843217\n",
    "error 103353 CUDA out of memory. Tried to allocate 448.00 MiB (GPU 0; 10.76 GiB total capacity; 8.67 GiB already allocated; 367.69 MiB free; 9.28 GiB reserved in total by PyTorch)\n",
    "error 103360 CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 10.76 GiB total capacity; 7.79 GiB already allocated; 827.69 MiB free; 8.83 GiB reserved in total by PyTorch)\n",
    "error 103459 CUDA out of memory. Tried to allocate 1.45 GiB (GPU 0; 10.76 GiB total capacity; 9.45 GiB already allocated; 81.69 MiB free; 9.55 GiB reserved in total by PyTorch)\n",
    "step 104000 loss 0.9956771865461124\n",
    "step 105000 loss 0.9826513917788725\n",
    "step 106000 loss 0.9809061708308737\n",
    "step 107000 loss 0.9690858024975915\n",
    "error 107880 CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 10.76 GiB total capacity; 7.89 GiB already allocated; 749.69 MiB free; 8.90 GiB reserved in total by PyTorch)\n",
    "step 108000 loss 0.9768115332263623\n",
    "step 109000 loss 0.9730555710098093\n",
    "error 109867 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.98 GiB already allocated; 55.69 MiB free; 9.58 GiB reserved in total by PyTorch)\n",
    "step 110000 loss 0.9588704389592169\n",
    "step 111000 loss 0.96905091931659\n",
    "step 112000 loss 0.9584886363872099\n",
    "step 113000 loss 0.9626811534210103\n",
    "error 113832 CUDA out of memory. Tried to allocate 986.00 MiB (GPU 0; 10.76 GiB total capacity; 8.36 GiB already allocated; 681.69 MiB free; 8.97 GiB reserved in total by PyTorch)\n",
    "step 114000 loss 0.9652874172750078\n",
    "error 114256 CUDA out of memory. Tried to allocate 740.00 MiB (GPU 0; 10.76 GiB total capacity; 7.62 GiB already allocated; 725.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
    "step 115000 loss 0.9702503596144891\n",
    "step 116000 loss 0.9834469998620671\n",
    "step 117000 loss 0.9565716304994859\n",
    "error 117802 CUDA out of memory. Tried to allocate 1.05 GiB (GPU 0; 10.76 GiB total capacity; 8.48 GiB already allocated; 115.69 MiB free; 9.52 GiB reserved in total by PyTorch)\n",
    "step 118000 loss 0.9472308477471018\n",
    "step 119000 loss 0.9432499939710902\n",
    "step 120000 loss 0.9487864710832014\n",
    "error 120352 CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 10.76 GiB total capacity; 7.89 GiB already allocated; 765.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
    "step 121000 loss 0.9565588829385917\n",
    "error 121728 CUDA out of memory. Tried to allocate 894.00 MiB (GPU 0; 10.76 GiB total capacity; 8.00 GiB already allocated; 775.69 MiB free; 8.88 GiB reserved in total by PyTorch)\n",
    "step 122000 loss 0.9532983707522799\n",
    "error 122227 CUDA out of memory. Tried to allocate 980.00 MiB (GPU 0; 10.76 GiB total capacity; 8.09 GiB already allocated; 605.69 MiB free; 9.04 GiB reserved in total by PyTorch)\n",
    "step 123000 loss 0.9446579631148261\n",
    "error 123827 CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 10.76 GiB total capacity; 9.21 GiB already allocated; 227.69 MiB free; 9.41 GiB reserved in total by PyTorch)\n",
    "step 124000 loss 0.9559603861394274\n",
    "step 125000 loss 0.9535240063586068\n",
    "error 125740 CUDA out of memory. Tried to allocate 916.00 MiB (GPU 0; 10.76 GiB total capacity; 7.93 GiB already allocated; 835.69 MiB free; 8.82 GiB reserved in total by PyTorch)\n",
    "error 125771 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.13 GiB already allocated; 937.69 MiB free; 8.72 GiB reserved in total by PyTorch)\n",
    "step 126000 loss 0.9491991817012265\n",
    "error 126478 CUDA out of memory. Tried to allocate 980.00 MiB (GPU 0; 10.76 GiB total capacity; 8.05 GiB already allocated; 749.69 MiB free; 8.90 GiB reserved in total by PyTorch)\n",
    "step 127000 loss 0.9462021857845146\n",
    "step 128000 loss 0.9454268055951242\n",
    "step 129000 loss 0.9485337868782098\n",
    "step 130000 loss 0.9403961820295327\n",
    "step 131000 loss 0.9420713849797546\n",
    "step 132000 loss 0.9559321375595082\n",
    "error 132104 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 6.98 GiB already allocated; 919.69 MiB free; 8.74 GiB reserved in total by PyTorch)\n",
    "step 133000 loss 0.9514717640636141\n",
    "step 134000 loss 0.9486987142211305\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "step 599000 loss 0.791350449541119\n",
    "step 600000 loss 0.7908788145488161\n",
    "error 600904 CUDA out of memory. Tried to allocate 906.00 MiB (GPU 0; 10.76 GiB total capacity; 8.02 GiB already allocated; 785.69 MiB free; 8.87 GiB reserved in total by PyTorch)\n",
    "step 601000 loss 0.7923423596750206\n",
    "step 602000 loss 0.7940523141718425\n",
    "error 602326 CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 10.76 GiB total capacity; 8.14 GiB already allocated; 799.69 MiB free; 8.85 GiB reserved in total by PyTorch)\n",
    "step 603000 loss 0.790953011557408\n",
    "error 603566 CUDA out of memory. Tried to allocate 870.00 MiB (GPU 0; 10.76 GiB total capacity; 8.08 GiB already allocated; 815.69 MiB free; 8.84 GiB reserved in total by PyTorch)\n",
    "step 604000 loss 0.8040739516727109\n",
    "step 605000 loss 0.7983863036420874\n",
    "step 606000 loss 0.8064560542508245\n",
    "step 607000 loss 0.8109014954943938\n",
    "step 608000 loss 0.8060132506587476\n",
    "step 609000 loss 0.8090561751829025\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang = 'ru_RU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang = 'en_XX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [250004, 146038, 2], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('привет')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250004"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('en_XX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(\n",
    "    text, model, tokenizer, \n",
    "    n=None, \n",
    "    max_length=\"auto\", \n",
    "    beams=5,\n",
    "    src_lang='en_XX',\n",
    "    tgt_lang='en_XX',\n",
    "    **kwargs\n",
    "):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    tokenizer.tgt_lang = tgt_lang\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    tokenizer.tgt_lang\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    if max_length == \"auto\":\n",
    "        max_length = inputs.shape[1] + 10\n",
    "\n",
    "    result = model.generate(\n",
    "        inputs,\n",
    "        num_return_sequences=n or 1,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        repetition_penalty=10.0,\n",
    "        max_length=max_length,\n",
    "        min_length=int(0.5 * max_length),\n",
    "        num_beams=beams,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to play with my dog, too.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('paraphrase: I like to play with my nice dog.', model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мне нравится играть со своей миленькой собакой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: I like to play with my nice dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ненавижу играть со своей чертовой собакой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate playing with my fucking dog. It's not a good thing.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: Ненавижу играть со своей долбаной собакой.', model, tokenizer, src_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't like to play with my dog.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('paraphrase: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='en_XX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't like to play with my dog.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('detox: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='en_XX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Терпеть не могу играть со своей собакой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('detox: Ненавижу играть со своей долбаной собакой.', model, tokenizer, src_lang='ru_RU', tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/russian_data/test.tsv', sep='\\t')\n",
    "test_inputs = test_data[\"toxic_comment\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5535509145748c988a068f2e39985e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs = [paraphrase('detox: ' + text, model, tokenizer, src_lang='ru_RU', tgt_lang='ru_RU') for text in tqdm(test_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/multitask-1M/'  # partial training results; need to train further.\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'results_ru.txt', 'w') as f:\n",
    "    for line in test_outputs:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary results\n",
    "```\n",
    "\n",
    "python evaluate_ru.py \\\n",
    "    --result_filename scores \\\n",
    "    --input_dir results/multitask-135K \\\n",
    "    --output_dir results\n",
    "    \n",
    "Style accuracy:       0.22049465775489807\n",
    "Meaning preservation: 0.9518442153930664\n",
    "Joint fluency:        -0.0548839345574379\n",
    "Joint score:          -0.01705724559724331\n",
    "Scores after calibration:\n",
    "Style accuracy:       0.2984451949596405\n",
    "Meaning preservation: 0.9277663826942444\n",
    "Joint fluency:        0.9368835091590881\n",
    "Joint score:          0.24435113370418549\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full results (1m steps)\n",
    "```\n",
    "python evaluate_ru.py \\\n",
    "    --result_filename scores \\\n",
    "    --input_dir results/multitask-1M \\\n",
    "    --output_dir results\n",
    "    \n",
    "Style accuracy:       0.25191155076026917\n",
    "Meaning preservation: 0.948351263999939\n",
    "Joint fluency:        -0.06296750158071518\n",
    "Joint score:          -0.021249013021588326\n",
    "Scores after calibration:\n",
    "Style accuracy:       0.32672038674354553\n",
    "Meaning preservation: 0.9225268363952637\n",
    "Joint fluency:        0.9275873899459839\n",
    "Joint score:          0.261173278093338\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../results/copypaste/results_ru.txt', 'w') as f:\n",
    "#    for line in test_inputs:\n",
    "#        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a Russian (translated) detox dataset as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en2ru = pd.read_csv('detox_en2ru_yandex.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19766, 15)\n",
      "(18488, 15)\n"
     ]
    }
   ],
   "source": [
    "detox_en2ru_filtered = detox_en2ru[\n",
    "    (detox_en2ru.edit_distance_ru >= detox_en2ru.edit_distance_en.quantile(0.01)) \n",
    "    & (detox_en2ru.edit_distance_ru <= detox_en2ru.edit_distance_en.quantile(0.99)) \n",
    "    & (detox_en2ru.edit_sim_ru >= detox_en2ru.edit_sim_en.quantile(0.01)) \n",
    "    & (detox_en2ru.edit_sim_ru <= detox_en2ru.edit_sim_en.quantile(0.99))\n",
    "]\n",
    "\n",
    "print(detox_en2ru.shape)\n",
    "print(detox_en2ru_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>neutral_comment</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>toxic</th>\n",
       "      <th>confidence_toxic</th>\n",
       "      <th>is_match</th>\n",
       "      <th>confidence_is_match</th>\n",
       "      <th>toxic_ru</th>\n",
       "      <th>neutral_ru</th>\n",
       "      <th>edit_distance_en</th>\n",
       "      <th>edit_distance_ru</th>\n",
       "      <th>edit_sim_en</th>\n",
       "      <th>edit_sim_ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16852</th>\n",
       "      <td>195236</td>\n",
       "      <td>that 's why you never say a fucking word until...</td>\n",
       "      <td>that 's why you never say a word until you spe...</td>\n",
       "      <td>0.995958</td>\n",
       "      <td>reddit</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9981</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9679</td>\n",
       "      <td>вот почему ты никогда не произносишь ни единог...</td>\n",
       "      <td>вот почему ты никогда не произносишь ни слова,...</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>0.890411</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>79073</td>\n",
       "      <td>of course , the only majority loser dem is cli...</td>\n",
       "      <td>Of course, the only majority loser is clinton</td>\n",
       "      <td>0.911208</td>\n",
       "      <td>jigsaw</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>Конечно, единственная демократка, проигравшая ...</td>\n",
       "      <td>Конечно, единственным проигравшим большинством...</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.575342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14601</th>\n",
       "      <td>127468</td>\n",
       "      <td>how in the fuck did bird flu get to antarctica .</td>\n",
       "      <td>How did Bird Flu get to Antarctica?</td>\n",
       "      <td>0.989469</td>\n",
       "      <td>reddit</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>как, черт возьми, птичий грипп попал в Антаркт...</td>\n",
       "      <td>Как птичий грипп попал в Антарктиду?</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          idx                                      toxic_comment  \\\n",
       "16852  195236  that 's why you never say a fucking word until...   \n",
       "1439    79073  of course , the only majority loser dem is cli...   \n",
       "14601  127468   how in the fuck did bird flu get to antarctica .   \n",
       "\n",
       "                                         neutral_comment  toxicity_score  \\\n",
       "16852  that 's why you never say a word until you spe...        0.995958   \n",
       "1439       Of course, the only majority loser is clinton        0.911208   \n",
       "14601                How did Bird Flu get to Antarctica?        0.989469   \n",
       "\n",
       "      dataset  toxic  confidence_toxic  is_match  confidence_is_match  \\\n",
       "16852  reddit  False            0.9981      True               0.9679   \n",
       "1439   jigsaw  False            0.9387      True               0.9611   \n",
       "14601  reddit  False            0.9955      True               0.9901   \n",
       "\n",
       "                                                toxic_ru  \\\n",
       "16852  вот почему ты никогда не произносишь ни единог...   \n",
       "1439   Конечно, единственная демократка, проигравшая ...   \n",
       "14601  как, черт возьми, птичий грипп попал в Антаркт...   \n",
       "\n",
       "                                              neutral_ru  edit_distance_en  \\\n",
       "16852  вот почему ты никогда не произносишь ни слова,...                 8   \n",
       "1439   Конечно, единственным проигравшим большинством...                 8   \n",
       "14601               Как птичий грипп попал в Антарктиду?                18   \n",
       "\n",
       "       edit_distance_ru  edit_sim_en  edit_sim_ru  \n",
       "16852                18     0.890411     0.812500  \n",
       "1439                 31     0.846154     0.575342  \n",
       "14601                15     0.625000     0.700000  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_en2ru_filtered.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(detox_en2ru_filtered, random_state=1, test_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['detox: зачем нам вообще хотеть видеть его свадьбу? мне на это наплевать',\n",
       "  'detox: некоторые идеи настолько нагло глупы , что только интеллектуал будет следовать им и верить в них.'],\n",
       " ['зачем нам вообще хотеть видеть его свадьбу? Мне все равно',\n",
       "  'Некоторые идеи настолько нагло невообразимы, что только интеллектуал будет следовать им и верить в них.'],\n",
       " 'ru_RU',\n",
       " 'ru_RU')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_detox_task_ru(batch_size=1):\n",
    "    task = 'detox: '\n",
    "    sample = detox_en2ru_filtered.sample(batch_size)\n",
    "    src_id, tgt_id = 'ru_RU', 'ru_RU'\n",
    "    x = []\n",
    "    y = []\n",
    "    for row in sample.itertuples():\n",
    "        x.append(task + row.toxic_ru)\n",
    "        y.append(row.neutral_ru)\n",
    "    return x, y, src_id, tgt_id\n",
    "\n",
    "get_detox_task_ru(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size=4):\n",
    "    x = []\n",
    "    y = []\n",
    "    bs = batch_size // 4\n",
    "    for generator in [get_detox_task, get_detox_task_ru, get_paraphrase_task, get_translate_task]:\n",
    "        for _ in range(bs):\n",
    "            xx, yy, src_id, tgt_id = generator(1)\n",
    "            tokenizer.src_lang, tokenizer.tgt_lang = src_id, tgt_id\n",
    "            x.append(tokenizer(xx))\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                y.append(tokenizer(yy))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "save_path = '/home/dale/models/detox-parallel/bart-multitask-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93560805e3a4669a024069b3fac4385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 74 CUDA out of memory. Tried to allocate 658.00 MiB (GPU 0; 10.76 GiB total capacity; 7.49 GiB already allocated; 605.69 MiB free; 9.04 GiB reserved in total by PyTorch)\n",
      "error 544 CUDA out of memory. Tried to allocate 1.21 GiB (GPU 0; 10.76 GiB total capacity; 7.60 GiB already allocated; 1.01 GiB free; 8.63 GiB reserved in total by PyTorch)\n",
      "error 748 CUDA out of memory. Tried to allocate 810.00 MiB (GPU 0; 10.76 GiB total capacity; 7.57 GiB already allocated; 747.69 MiB free; 8.90 GiB reserved in total by PyTorch)\n",
      "step 1000 loss 0.8314551791351633\n",
      "error 1260 CUDA out of memory. Tried to allocate 894.00 MiB (GPU 0; 10.76 GiB total capacity; 8.03 GiB already allocated; 781.69 MiB free; 8.87 GiB reserved in total by PyTorch)\n",
      "step 2000 loss 0.8092259806353226\n",
      "error 2935 CUDA out of memory. Tried to allocate 1.21 GiB (GPU 0; 10.76 GiB total capacity; 8.94 GiB already allocated; 413.69 MiB free; 9.23 GiB reserved in total by PyTorch)\n",
      "error 2948 CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 10.76 GiB total capacity; 7.94 GiB already allocated; 683.69 MiB free; 8.97 GiB reserved in total by PyTorch)\n",
      "step 3000 loss 0.7937554402146534\n",
      "error 3079 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.77 GiB already allocated; 181.69 MiB free; 9.46 GiB reserved in total by PyTorch)\n",
      "error 3186 CUDA out of memory. Tried to allocate 742.00 MiB (GPU 0; 10.76 GiB total capacity; 7.54 GiB already allocated; 679.69 MiB free; 8.97 GiB reserved in total by PyTorch)\n",
      "step 4000 loss 0.7974604768399046\n",
      "error 4706 CUDA out of memory. Tried to allocate 1008.00 MiB (GPU 0; 10.76 GiB total capacity; 8.43 GiB already allocated; 613.69 MiB free; 9.04 GiB reserved in total by PyTorch)\n",
      "step 5000 loss 0.7800834597484653\n",
      "step 6000 loss 0.7784381403957221\n",
      "error 6315 CUDA out of memory. Tried to allocate 1.83 GiB (GPU 0; 10.76 GiB total capacity; 8.55 GiB already allocated; 203.69 MiB free; 9.44 GiB reserved in total by PyTorch)\n",
      "error 6378 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 6.74 GiB already allocated; 407.69 MiB free; 9.24 GiB reserved in total by PyTorch)\n",
      "error 6630 CUDA out of memory. Tried to allocate 916.00 MiB (GPU 0; 10.76 GiB total capacity; 8.35 GiB already allocated; 715.69 MiB free; 8.94 GiB reserved in total by PyTorch)\n",
      "error 6952 CUDA out of memory. Tried to allocate 1.25 GiB (GPU 0; 10.76 GiB total capacity; 9.28 GiB already allocated; 201.69 MiB free; 9.44 GiB reserved in total by PyTorch)\n",
      "error 6988 CUDA out of memory. Tried to allocate 902.00 MiB (GPU 0; 10.76 GiB total capacity; 8.25 GiB already allocated; 365.69 MiB free; 9.28 GiB reserved in total by PyTorch)\n",
      "step 7000 loss 0.78068018917068\n",
      "error 7767 CUDA out of memory. Tried to allocate 780.00 MiB (GPU 0; 10.76 GiB total capacity; 7.53 GiB already allocated; 719.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
      "step 8000 loss 0.7763817786440819\n",
      "error 8975 CUDA out of memory. Tried to allocate 1.10 GiB (GPU 0; 10.76 GiB total capacity; 9.11 GiB already allocated; 5.69 MiB free; 9.63 GiB reserved in total by PyTorch)\n",
      "step 9000 loss 0.7648057962117366\n",
      "error 9194 CUDA out of memory. Tried to allocate 986.00 MiB (GPU 0; 10.76 GiB total capacity; 8.18 GiB already allocated; 871.69 MiB free; 8.78 GiB reserved in total by PyTorch)\n",
      "error 9260 CUDA out of memory. Tried to allocate 696.00 MiB (GPU 0; 10.76 GiB total capacity; 7.93 GiB already allocated; 459.69 MiB free; 9.19 GiB reserved in total by PyTorch)\n",
      "step 10000 loss 0.758818619489108\n",
      "error 10045 CUDA out of memory. Tried to allocate 1.66 GiB (GPU 0; 10.76 GiB total capacity; 8.17 GiB already allocated; 1.33 GiB free; 8.30 GiB reserved in total by PyTorch)\n",
      "error 10070 CUDA out of memory. Tried to allocate 1.04 GiB (GPU 0; 10.76 GiB total capacity; 7.35 GiB already allocated; 989.69 MiB free; 8.67 GiB reserved in total by PyTorch)\n",
      "error 10078 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 6.64 GiB already allocated; 627.69 MiB free; 9.02 GiB reserved in total by PyTorch)\n",
      "step 11000 loss 0.7544529560139434\n",
      "error 11962 CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 10.76 GiB total capacity; 8.00 GiB already allocated; 837.69 MiB free; 8.82 GiB reserved in total by PyTorch)\n",
      "step 12000 loss 0.7593470094823236\n",
      "error 12104 CUDA out of memory. Tried to allocate 1.21 GiB (GPU 0; 10.76 GiB total capacity; 9.00 GiB already allocated; 133.69 MiB free; 9.50 GiB reserved in total by PyTorch)\n",
      "error 12899 CUDA out of memory. Tried to allocate 1.08 GiB (GPU 0; 10.76 GiB total capacity; 8.64 GiB already allocated; 717.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
      "step 13000 loss 0.7606775300183983\n",
      "error 13146 CUDA out of memory. Tried to allocate 718.00 MiB (GPU 0; 10.76 GiB total capacity; 7.49 GiB already allocated; 715.69 MiB free; 8.94 GiB reserved in total by PyTorch)\n",
      "error 13704 CUDA out of memory. Tried to allocate 886.00 MiB (GPU 0; 10.76 GiB total capacity; 7.94 GiB already allocated; 759.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "step 14000 loss 0.7456426567145277\n",
      "error 14012 CUDA out of memory. Tried to allocate 1016.00 MiB (GPU 0; 10.76 GiB total capacity; 8.36 GiB already allocated; 599.69 MiB free; 9.05 GiB reserved in total by PyTorch)\n",
      "error 14092 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.79 GiB already allocated; 845.69 MiB free; 8.81 GiB reserved in total by PyTorch)\n",
      "error 14244 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 8.08 GiB already allocated; 827.69 MiB free; 8.83 GiB reserved in total by PyTorch)\n",
      "error 14571 CUDA out of memory. Tried to allocate 680.00 MiB (GPU 0; 10.76 GiB total capacity; 7.75 GiB already allocated; 625.69 MiB free; 9.02 GiB reserved in total by PyTorch)\n",
      "error 14972 CUDA out of memory. Tried to allocate 734.00 MiB (GPU 0; 10.76 GiB total capacity; 8.63 GiB already allocated; 203.69 MiB free; 9.44 GiB reserved in total by PyTorch)\n",
      "step 15000 loss 0.7426142490596195\n",
      "step 16000 loss 0.7377873893357552\n",
      "error 16287 CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 10.76 GiB total capacity; 7.86 GiB already allocated; 629.69 MiB free; 9.02 GiB reserved in total by PyTorch)\n",
      "error 16540 CUDA out of memory. Tried to allocate 832.00 MiB (GPU 0; 10.76 GiB total capacity; 7.76 GiB already allocated; 649.69 MiB free; 9.00 GiB reserved in total by PyTorch)\n",
      "step 17000 loss 0.7303464659507566\n",
      "error 17022 CUDA out of memory. Tried to allocate 1.13 GiB (GPU 0; 10.76 GiB total capacity; 8.60 GiB already allocated; 845.69 MiB free; 8.81 GiB reserved in total by PyTorch)\n",
      "error 17968 CUDA out of memory. Tried to allocate 756.00 MiB (GPU 0; 10.76 GiB total capacity; 7.67 GiB already allocated; 735.69 MiB free; 8.92 GiB reserved in total by PyTorch)\n",
      "step 18000 loss 0.7390649000975859\n",
      "error 18746 CUDA out of memory. Tried to allocate 1.13 GiB (GPU 0; 10.76 GiB total capacity; 8.34 GiB already allocated; 615.69 MiB free; 9.03 GiB reserved in total by PyTorch)\n",
      "error 18996 CUDA out of memory. Tried to allocate 1.17 GiB (GPU 0; 10.76 GiB total capacity; 7.90 GiB already allocated; 1.14 GiB free; 8.50 GiB reserved in total by PyTorch)\n",
      "step 19000 loss 0.7278704965323899\n",
      "error 19478 CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.76 GiB total capacity; 7.98 GiB already allocated; 931.69 MiB free; 8.72 GiB reserved in total by PyTorch)\n",
      "step 20000 loss 0.7146266344660498\n",
      "error 20314 CUDA out of memory. Tried to allocate 994.00 MiB (GPU 0; 10.76 GiB total capacity; 8.65 GiB already allocated; 675.69 MiB free; 8.97 GiB reserved in total by PyTorch)\n",
      "error 20379 CUDA out of memory. Tried to allocate 1.04 GiB (GPU 0; 10.76 GiB total capacity; 8.52 GiB already allocated; 969.69 MiB free; 8.69 GiB reserved in total by PyTorch)\n",
      "error 20696 CUDA out of memory. Tried to allocate 924.00 MiB (GPU 0; 10.76 GiB total capacity; 8.04 GiB already allocated; 925.69 MiB free; 8.73 GiB reserved in total by PyTorch)\n",
      "step 21000 loss 0.7269782561467648\n",
      "error 21432 CUDA out of memory. Tried to allocate 780.00 MiB (GPU 0; 10.76 GiB total capacity; 7.69 GiB already allocated; 737.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
      "error 21578 CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 10.76 GiB total capacity; 7.62 GiB already allocated; 649.69 MiB free; 9.00 GiB reserved in total by PyTorch)\n",
      "error 21582 CUDA out of memory. Tried to allocate 642.00 MiB (GPU 0; 10.76 GiB total capacity; 8.09 GiB already allocated; 563.69 MiB free; 9.08 GiB reserved in total by PyTorch)\n",
      "step 22000 loss 0.7082797583399971\n",
      "step 23000 loss 0.699612423973891\n",
      "error 23224 CUDA out of memory. Tried to allocate 734.00 MiB (GPU 0; 10.76 GiB total capacity; 8.36 GiB already allocated; 221.69 MiB free; 9.42 GiB reserved in total by PyTorch)\n",
      "error 23799 CUDA out of memory. Tried to allocate 1008.00 MiB (GPU 0; 10.76 GiB total capacity; 8.99 GiB already allocated; 103.69 MiB free; 9.53 GiB reserved in total by PyTorch)\n",
      "error 23974 CUDA out of memory. Tried to allocate 1.18 GiB (GPU 0; 10.76 GiB total capacity; 9.26 GiB already allocated; 255.69 MiB free; 9.38 GiB reserved in total by PyTorch)\n",
      "step 24000 loss 0.7151653931138241\n",
      "error 24218 CUDA out of memory. Tried to allocate 574.00 MiB (GPU 0; 10.76 GiB total capacity; 7.54 GiB already allocated; 429.69 MiB free; 9.21 GiB reserved in total by PyTorch)\n",
      "step 25000 loss 0.7163585537103836\n",
      "error 25315 CUDA out of memory. Tried to allocate 1.13 GiB (GPU 0; 10.76 GiB total capacity; 8.94 GiB already allocated; 405.69 MiB free; 9.24 GiB reserved in total by PyTorch)\n",
      "step 26000 loss 0.7175765298199578\n",
      "error 26387 CUDA out of memory. Tried to allocate 764.00 MiB (GPU 0; 10.76 GiB total capacity; 7.56 GiB already allocated; 683.69 MiB free; 8.97 GiB reserved in total by PyTorch)\n",
      "step 27000 loss 0.7084266342165536\n",
      "step 28000 loss 0.7094808707569696\n",
      "error 28108 CUDA out of memory. Tried to allocate 802.00 MiB (GPU 0; 10.76 GiB total capacity; 7.90 GiB already allocated; 697.69 MiB free; 8.95 GiB reserved in total by PyTorch)\n",
      "error 28123 CUDA out of memory. Tried to allocate 1.04 GiB (GPU 0; 10.76 GiB total capacity; 9.05 GiB already allocated; 159.69 MiB free; 9.48 GiB reserved in total by PyTorch)\n",
      "error 28582 CUDA out of memory. Tried to allocate 910.00 MiB (GPU 0; 10.76 GiB total capacity; 8.41 GiB already allocated; 251.69 MiB free; 9.39 GiB reserved in total by PyTorch)\n",
      "step 29000 loss 0.6940117910420943\n",
      "error 29670 CUDA out of memory. Tried to allocate 932.00 MiB (GPU 0; 10.76 GiB total capacity; 8.67 GiB already allocated; 697.69 MiB free; 8.95 GiB reserved in total by PyTorch)\n",
      "step 30000 loss 0.6998926631202852\n",
      "error 30334 CUDA out of memory. Tried to allocate 658.00 MiB (GPU 0; 10.76 GiB total capacity; 7.95 GiB already allocated; 293.69 MiB free; 9.35 GiB reserved in total by PyTorch)\n",
      "error 30411 CUDA out of memory. Tried to allocate 924.00 MiB (GPU 0; 10.76 GiB total capacity; 8.49 GiB already allocated; 863.69 MiB free; 8.79 GiB reserved in total by PyTorch)\n",
      "error 30910 CUDA out of memory. Tried to allocate 1.05 GiB (GPU 0; 10.76 GiB total capacity; 8.43 GiB already allocated; 893.69 MiB free; 8.76 GiB reserved in total by PyTorch)\n",
      "step 31000 loss 0.6896793897201174\n",
      "step 32000 loss 0.6893255105074728\n",
      "error 32442 CUDA out of memory. Tried to allocate 664.00 MiB (GPU 0; 10.76 GiB total capacity; 8.26 GiB already allocated; 87.69 MiB free; 9.55 GiB reserved in total by PyTorch)\n",
      "step 33000 loss 0.6937477080519528\n",
      "error 33707 CUDA out of memory. Tried to allocate 786.00 MiB (GPU 0; 10.76 GiB total capacity; 8.16 GiB already allocated; 757.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "error 33898 CUDA out of memory. Tried to allocate 802.00 MiB (GPU 0; 10.76 GiB total capacity; 7.82 GiB already allocated; 587.69 MiB free; 9.06 GiB reserved in total by PyTorch)\n",
      "step 34000 loss 0.688192859412077\n",
      "error 34370 CUDA out of memory. Tried to allocate 1.13 GiB (GPU 0; 10.76 GiB total capacity; 8.50 GiB already allocated; 981.69 MiB free; 8.68 GiB reserved in total by PyTorch)\n",
      "step 35000 loss 0.681735683638736\n",
      "error 35974 CUDA out of memory. Tried to allocate 870.00 MiB (GPU 0; 10.76 GiB total capacity; 8.08 GiB already allocated; 691.69 MiB free; 8.96 GiB reserved in total by PyTorch)\n",
      "step 36000 loss 0.684535192685544\n",
      "error 36379 CUDA out of memory. Tried to allocate 1.06 GiB (GPU 0; 10.76 GiB total capacity; 8.75 GiB already allocated; 391.69 MiB free; 9.25 GiB reserved in total by PyTorch)\n",
      "error 36392 CUDA out of memory. Tried to allocate 1000.00 MiB (GPU 0; 10.76 GiB total capacity; 8.29 GiB already allocated; 373.69 MiB free; 9.27 GiB reserved in total by PyTorch)\n",
      "error 36618 CUDA out of memory. Tried to allocate 886.00 MiB (GPU 0; 10.76 GiB total capacity; 8.19 GiB already allocated; 545.69 MiB free; 9.10 GiB reserved in total by PyTorch)\n",
      "step 37000 loss 0.6776466618145326\n",
      "error 37023 CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 10.76 GiB total capacity; 8.44 GiB already allocated; 127.69 MiB free; 9.51 GiB reserved in total by PyTorch)\n",
      "step 38000 loss 0.6799925004107512\n",
      "error 38770 CUDA out of memory. Tried to allocate 870.00 MiB (GPU 0; 10.76 GiB total capacity; 8.40 GiB already allocated; 293.69 MiB free; 9.35 GiB reserved in total by PyTorch)\n",
      "error 38848 CUDA out of memory. Tried to allocate 794.00 MiB (GPU 0; 10.76 GiB total capacity; 8.08 GiB already allocated; 721.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
      "step 39000 loss 0.6758897786114302\n",
      "error 39022 CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 10.76 GiB total capacity; 7.84 GiB already allocated; 717.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
      "error 39036 CUDA out of memory. Tried to allocate 1000.00 MiB (GPU 0; 10.76 GiB total capacity; 8.80 GiB already allocated; 111.69 MiB free; 9.53 GiB reserved in total by PyTorch)\n",
      "error 39052 CUDA out of memory. Tried to allocate 1.80 GiB (GPU 0; 10.76 GiB total capacity; 8.47 GiB already allocated; 333.69 MiB free; 9.31 GiB reserved in total by PyTorch)\n",
      "error 39086 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 6.68 GiB already allocated; 483.69 MiB free; 9.16 GiB reserved in total by PyTorch)\n",
      "error 39340 CUDA out of memory. Tried to allocate 1.01 GiB (GPU 0; 10.76 GiB total capacity; 8.05 GiB already allocated; 629.69 MiB free; 9.02 GiB reserved in total by PyTorch)\n",
      "error 39739 CUDA out of memory. Tried to allocate 902.00 MiB (GPU 0; 10.76 GiB total capacity; 8.64 GiB already allocated; 797.69 MiB free; 8.86 GiB reserved in total by PyTorch)\n",
      "step 40000 loss 0.6665187633299232\n",
      "error 40979 CUDA out of memory. Tried to allocate 1.10 GiB (GPU 0; 10.76 GiB total capacity; 8.43 GiB already allocated; 795.69 MiB free; 8.86 GiB reserved in total by PyTorch)\n",
      "step 41000 loss 0.6742194335248556\n",
      "error 41074 CUDA out of memory. Tried to allocate 832.00 MiB (GPU 0; 10.76 GiB total capacity; 7.97 GiB already allocated; 739.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
      "error 41565 CUDA out of memory. Tried to allocate 1.68 GiB (GPU 0; 10.76 GiB total capacity; 7.72 GiB already allocated; 1.40 GiB free; 8.23 GiB reserved in total by PyTorch)\n",
      "error 41575 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 6.83 GiB already allocated; 459.69 MiB free; 9.19 GiB reserved in total by PyTorch)\n",
      "step 42000 loss 0.6607054026097097\n",
      "error 42286 CUDA out of memory. Tried to allocate 932.00 MiB (GPU 0; 10.76 GiB total capacity; 7.99 GiB already allocated; 761.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "error 42686 CUDA out of memory. Tried to allocate 1016.00 MiB (GPU 0; 10.76 GiB total capacity; 8.69 GiB already allocated; 43.69 MiB free; 9.59 GiB reserved in total by PyTorch)\n",
      "error 42748 CUDA out of memory. Tried to allocate 1016.00 MiB (GPU 0; 10.76 GiB total capacity; 8.77 GiB already allocated; 715.69 MiB free; 8.94 GiB reserved in total by PyTorch)\n",
      "step 43000 loss 0.657895739687116\n",
      "error 43284 CUDA out of memory. Tried to allocate 1.11 GiB (GPU 0; 10.76 GiB total capacity; 8.47 GiB already allocated; 295.69 MiB free; 9.35 GiB reserved in total by PyTorch)\n",
      "error 43824 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 8.12 GiB already allocated; 649.69 MiB free; 9.00 GiB reserved in total by PyTorch)\n",
      "step 44000 loss 0.6579087492722052\n",
      "error 44399 CUDA out of memory. Tried to allocate 1.83 GiB (GPU 0; 10.76 GiB total capacity; 8.57 GiB already allocated; 995.69 MiB free; 8.66 GiB reserved in total by PyTorch)\n",
      "error 44556 CUDA out of memory. Tried to allocate 886.00 MiB (GPU 0; 10.76 GiB total capacity; 8.20 GiB already allocated; 717.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
      "error 44692 CUDA out of memory. Tried to allocate 870.00 MiB (GPU 0; 10.76 GiB total capacity; 8.10 GiB already allocated; 697.69 MiB free; 8.95 GiB reserved in total by PyTorch)\n",
      "error 44724 CUDA out of memory. Tried to allocate 870.00 MiB (GPU 0; 10.76 GiB total capacity; 8.00 GiB already allocated; 813.69 MiB free; 8.84 GiB reserved in total by PyTorch)\n",
      "step 45000 loss 0.6598257269547265\n",
      "error 45894 CUDA out of memory. Tried to allocate 1.24 GiB (GPU 0; 10.76 GiB total capacity; 9.15 GiB already allocated; 199.69 MiB free; 9.44 GiB reserved in total by PyTorch)\n",
      "error 45999 CUDA out of memory. Tried to allocate 1.22 GiB (GPU 0; 10.76 GiB total capacity; 8.18 GiB already allocated; 553.69 MiB free; 9.09 GiB reserved in total by PyTorch)\n",
      "step 46000 loss 0.653405723583226\n",
      "step 47000 loss 0.6513710985902629\n",
      "error 47284 CUDA out of memory. Tried to allocate 482.00 MiB (GPU 0; 10.76 GiB total capacity; 9.30 GiB already allocated; 227.69 MiB free; 9.41 GiB reserved in total by PyTorch)\n",
      "step 48000 loss 0.6570487918433546\n",
      "error 48304 CUDA out of memory. Tried to allocate 1.30 GiB (GPU 0; 10.76 GiB total capacity; 7.75 GiB already allocated; 953.69 MiB free; 8.70 GiB reserved in total by PyTorch)\n",
      "error 48875 CUDA out of memory. Tried to allocate 940.00 MiB (GPU 0; 10.76 GiB total capacity; 8.51 GiB already allocated; 53.69 MiB free; 9.58 GiB reserved in total by PyTorch)\n",
      "step 49000 loss 0.6447985561147015\n",
      "error 49010 CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.76 GiB total capacity; 8.36 GiB already allocated; 355.69 MiB free; 9.29 GiB reserved in total by PyTorch)\n",
      "error 49419 CUDA out of memory. Tried to allocate 986.00 MiB (GPU 0; 10.76 GiB total capacity; 8.59 GiB already allocated; 715.69 MiB free; 8.94 GiB reserved in total by PyTorch)\n",
      "error 49570 CUDA out of memory. Tried to allocate 1.34 GiB (GPU 0; 10.76 GiB total capacity; 8.13 GiB already allocated; 929.69 MiB free; 8.73 GiB reserved in total by PyTorch)\n",
      "error 49827 CUDA out of memory. Tried to allocate 932.00 MiB (GPU 0; 10.76 GiB total capacity; 8.32 GiB already allocated; 755.69 MiB free; 8.90 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "tq = trange(0, 50_000)\n",
    "cleanup()\n",
    "\n",
    "for i in tq:\n",
    "    x, y = get_batch(batch_size)\n",
    "    xpad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in x])\n",
    "    ypad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in y])\n",
    "    \n",
    "    try:\n",
    "        labels = torch.tensor(ypad['input_ids'], device=model.device)\n",
    "        labels[labels==tokenizer.pad_token_id] = -100\n",
    "        loss = model(\n",
    "            input_ids=torch.tensor(xpad['input_ids'], device=model.device),\n",
    "            attention_mask=torch.tensor(xpad['attention_mask'], device=model.device),\n",
    "            labels=labels,\n",
    "        ).loss\n",
    "        loss.backward()\n",
    "        if i % gradient_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "    except RuntimeError as e:\n",
    "        print('error', i, e)\n",
    "        loss = None\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        cleanup()\n",
    "        continue\n",
    "\n",
    "    w = 1 / max(1, min(i, window))\n",
    "    ewm_loss = ewm_loss * (1-w) + loss.item() * w\n",
    "    tq.set_description(f'{ewm_loss:3.4f}')\n",
    "\n",
    "    if i > 0 and i % report_steps == 0:\n",
    "        print('step', i, 'loss', ewm_loss, )\n",
    "        if i > 0:\n",
    "            model.save_pretrained(save_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "    if i % cleanup_steps == 0:\n",
    "        cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to play with my dog, too.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('paraphrase: I like to play with my nice dog.', model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я люблю играть со своей милой собакой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: I like to play with my nice dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я ненавижу играть со своей блядской собакой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate playing with my fucking dog. Oh, God.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: Ненавижу играть со своей долбаной собакой.', model, tokenizer, src_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't like to play with my dog.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('paraphrase: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='en_XX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't like to play with my dog.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('detox: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='en_XX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я ненавижу играть со своей собакой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('detox: Ненавижу играть со своей долбаной собакой.', model, tokenizer, src_lang='ru_RU', tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9383d58977a4cb483e791e6c7a88562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs = [paraphrase('detox: ' + text, model, tokenizer, src_lang='ru_RU', tgt_lang='ru_RU') for text in tqdm(test_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/multitask-1M-translate-yandex/'  # partial training results; need to train further.\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'results_ru.txt', 'w') as f:\n",
    "    for line in test_outputs:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full results (1m steps)  + 50K with Russian translated data\n",
    "```\n",
    "python evaluate_ru.py \\\n",
    "    --result_filename scores \\\n",
    "    --input_dir results/multitask-1M-translate-yandex \\\n",
    "    --output_dir results\n",
    "    \n",
    "Style accuracy:       0.6192111968994141\n",
    "Meaning preservation: 0.8775237798690796\n",
    "Joint fluency:        -0.09076358377933502\n",
    "Joint score:          -0.04992109537124634\n",
    "Scores after calibration:\n",
    "Style accuracy:       0.6572900414466858\n",
    "Meaning preservation: 0.8162857890129089\n",
    "Joint fluency:        0.8956218957901001\n",
    "Joint score:          0.46870917081832886\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3k",
   "language": "python",
   "name": "p3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
