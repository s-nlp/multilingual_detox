{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Russian SOTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/skoltech-nlp/russe_detox_2022/tree/main/baselines/t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/russian_data/test.tsv', sep='\\t')\n",
    "test_inputs = test_data[\"toxic_comment\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = 'sberbank-ai/ruT5-base'\n",
    "model_name = 'SkolkovoInstitute/ruT5-base-detox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25f8e0fd3684bc79366e07c25ca4809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd142eed6aa46e0adc43140fc2c0a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(text, model, n=None, max_length='auto', temperature=1.0, beams=3):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(model.device)\n",
    "    if max_length == 'auto':\n",
    "        max_length = int(inputs.shape[1] * 1.2) + 10\n",
    "    result = model.generate(\n",
    "        inputs, \n",
    "        num_return_sequences=n or 1, \n",
    "        do_sample=False, \n",
    "        temperature=temperature, \n",
    "        repetition_penalty=3.0, \n",
    "        max_length=max_length,\n",
    "        bad_words_ids=[[1]],  # unk\n",
    "        num_beams=beams,\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Дмитрий вы с ума сошли, уже все выложено']\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase(['Дмитрий вы ебанулись, уже все выложено'], model, beams=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6db98dbb3b04717808c3cca8b26e93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "para_results = []\n",
    "batch_size = 8\n",
    "\n",
    "for i in tqdm(range(0, len(test_inputs), batch_size)):\n",
    "    batch = [sentence for sentence in test_inputs[i:i + batch_size]]\n",
    "    para_results.extend(paraphrase(batch, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['укропидорг лавринов! общайся лучше с ией - так хоть на человека похож!',\n",
       " 'терпеть не могу самок, которые за мужской хер готовы родину продать',\n",
       " 'фсё для таких пидарасоф как ты',\n",
       " 'а работать не хочешь да? хочешь за свои фотки деньги получать? хуй на нос тебе',\n",
       " 'пидор !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! сука ,,продал 15 !!!!!!!!!! союзных республик ?????????? куда смотрело тогдашнее нквд ?????']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['лавринов! общайся лучше с ией - так хоть на человека похож!',\n",
       " 'терпеть не могу самок, которые за мужской бред готовы родину продать',\n",
       " 'Всё для таких как ты',\n",
       " 'а работать не хочешь да? хочешь за свои фотки деньги получать?',\n",
       " 'Продал 15 союзных республик. Почему НКВД его не наказало?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../results/ruT5-base-detox/'\n",
    "os.makedirs(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(p + 'results_ru.txt', 'w') as f:\n",
    "    for text in para_results:\n",
    "        f.write(text.replace('\\n', ' ') + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /home/dale/projects/multilingual_detox\n",
    "python evaluate_ru.py \\\n",
    "    --result_filename scores \\\n",
    "    --input_dir results/ruT5-base-detox \\\n",
    "    --output_dir results\n",
    "    \n",
    "Style accuracy:       0.7726958990097046\n",
    "Meaning preservation: 0.8499152064323425\n",
    "Joint fluency:        -0.1741630882024765\n",
    "Joint score:          -0.1071704551577568\n",
    "Scores after calibration:\n",
    "Style accuracy:       0.7954263091087341\n",
    "Meaning preservation: 0.7758175134658813\n",
    "Joint fluency:        0.7997124791145325\n",
    "Joint score:          0.5067287087440491\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English SOTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'SkolkovoInstitute/bart-base-detox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(text, model, n=None, max_length='auto', temperature=1.0, beams=3):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(model.device)\n",
    "    if max_length == 'auto':\n",
    "        max_length = int(inputs.shape[1] * 1.2) + 10\n",
    "    result = model.generate(\n",
    "        inputs, \n",
    "        num_return_sequences=n or 1, \n",
    "        do_sample=False, \n",
    "        temperature=temperature, \n",
    "        repetition_penalty=3.0, \n",
    "        max_length=max_length,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]],  # unk\n",
    "        num_beams=beams,\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't care!\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(\"I don't give a fuck!\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/english_data/test_toxic_parallel.txt', 'r') as f:\n",
    "    en_test_inputs = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. or the loud ass one - thousand ton beast roaring towards you howling its horn .',\n",
       " 'mandated  and \" right fucking now \" would be good .',\n",
       " '* neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .',\n",
       " '* well shit , cunt shot himself .',\n",
       " \"&gt i wouldn 't care how ignorant you are in you weren 't pretending to know shit .\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_test_inputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c27f64075641cba26a41e43344e03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "para_results_en = []\n",
    "batch_size = 8\n",
    "\n",
    "for i in tqdm(range(0, len(en_test_inputs), batch_size)):\n",
    "    batch = [sentence for sentence in en_test_inputs[i:i + batch_size]]\n",
    "    para_results_en.extend(paraphrase(batch, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. or the loud one - thousand ton beast roaring towards you howling its horn.',\n",
       " 'mandated  and \" right now \" would be good.',\n",
       " '* neither * of my coworkers cared when it came time to ditch mitch. ugh.',\n",
       " 'He shot himself.',\n",
       " \"I wouldn't care how ignorant you are in you weren't pretending to know.\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_results_en[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../results/bart-base-detox/'\n",
    "os.makedirs(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(p + 'results_en.txt', 'w') as f:\n",
    "    for text in para_results_en:\n",
    "        f.write(text.replace('\\n', ' ') + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /home/dale/projects/paradetox2/evaluation_detox\n",
    "python metric.py --inputs /home/dale/projects/multilingual_detox/data/english_data/test_toxic_parallel.txt \\\n",
    "    --preds /home/dale/projects/multilingual_detox/results/bart-base-detox/results_en.txt \\\n",
    "    --cola_classifier_path /home/dale/models/cola_classifier_fairseq \\\n",
    "    --wieting_model_path /home/dale/models/wieting_similarity/sim.pt \\\n",
    "    --wieting_tokenizer_path /home/dale/models/wieting_similarity/sim.sp.30k.model \\\n",
    "    --batch_size 32\n",
    "cat results.md\n",
    "```\n",
    "| Model | ACC | EMB_SIM | SIM | CharPPL | TokenPPL | FL | GM | J | BLEU |\n",
    "| ----- | --- | ------- | --- | ------- | -------- | -- | -- | - | ---- |\n",
    "results_en.txt|0.6766|0.7574|0.7180|6.5078|86.1255|0.8942|0.0000|0.4116|0.4885|\n",
    "results_en.txt|0.6662|0.7985|0.7741|6.1724|89.3471|0.9553|0.0000|0.4674|0.5799|\n",
    "results_en.txt|0.9016|0.8934|0.8592|6.2307|127.6455|0.8599|11.9160|0.6555|0.7101|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3k",
   "language": "python",
   "name": "p3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
