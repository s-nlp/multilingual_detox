{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same as `multitask-pretraining`, but initialized from a raw BART for fair evaluation in both transfer directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Paraphrasing: https://huggingface.co/datasets/GEM/opusparcus\n",
    "* Translation: https://huggingface.co/datasets/open_subtitles + news_commentary? + tatoeba?\n",
    "* Detox: ordinary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opusparcus (/home/dale/.cache/huggingface/datasets/GEM___opusparcus/en.80/1.0.0/79d36ae4eced4f3c2c5a2ab9f94a584de7adca957186408d33798d0d87b018f2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4e68456b584a858c31603583ca74a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 982\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1015\n",
       "    })\n",
       "    test.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1445\n",
       "    })\n",
       "    validation.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1455\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 5200000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opus_para_en = load_dataset(\"GEM/opusparcus\", \"en.80\")\n",
    "opus_para_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opusparcus (/home/dale/.cache/huggingface/datasets/GEM___opusparcus/ru.80/1.0.0/79d36ae4eced4f3c2c5a2ab9f94a584de7adca957186408d33798d0d87b018f2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbfe80d20db42fcbb37b60ffcb1c673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1068\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1020\n",
       "    })\n",
       "    test.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1855\n",
       "    })\n",
       "    validation.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1854\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 2300000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opus_para_ru = load_dataset(\"GEM/opusparcus\", \"ru.80\")\n",
    "opus_para_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lang': 'ru',\n",
       " 'input': 'На-ка.',\n",
       " 'target': 'Получи.',\n",
       " 'annot_score': 0.0,\n",
       " 'gem_id': 'gem-opusparcus-train-79939030',\n",
       " 'references': ['Получи.']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(opus_para_ru['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-ru-lang1=en,lang2=ru\n",
      "Reusing dataset open_subtitles (/home/dale/.cache/huggingface/datasets/open_subtitles/en-ru-lang1=en,lang2=ru/0.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4fc1603caa4b2e916ab1f3ae502561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'meta', 'translation'],\n",
       "        num_rows: 25910105\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opensub = load_dataset(\"open_subtitles\", lang1=\"en\", lang2='ru')\n",
    "opensub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2752315',\n",
       " 'meta': {'year': 1993,\n",
       "  'imdbId': 701066,\n",
       "  'subtitleId': {'en': 5591101, 'ru': 7006084},\n",
       "  'sentenceIds': {'en': [118], 'ru': [105]}},\n",
       " 'translation': {'en': 'Observe.', 'ru': 'Велосипедный замок.'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(opensub['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-ru-lang1=en,lang2=ru\n",
      "Reusing dataset news_commentary (/home/dale/.cache/huggingface/datasets/news_commentary/en-ru-lang1=en,lang2=ru/0.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d7b72f623f43a59cc52b3755d12ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 190104\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_commentary = load_dataset(\"news_commentary\", lang1=\"en\", lang2='ru')\n",
    "news_commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '87720',\n",
       " 'translation': {'en': 'How a concept is framed, the context and associations with which it is presented, affect human judgments enormously.',\n",
       "  'ru': 'Обрамление, контекст и ассоциации, с которыми связано то или иное понятие, очень сильно влияют на суждения людей.'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(news_commentary['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-ru-lang1=en,lang2=ru\n",
      "Reusing dataset tatoeba (/home/dale/.cache/huggingface/datasets/tatoeba/en-ru-lang1=en,lang2=ru/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68adaf86459f47728ec6fbd67dd61dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 523656\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tatoeba = load_dataset(\"tatoeba\", lang1=\"en\", lang2='ru')\n",
    "tatoeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '391340',\n",
       " 'translation': {'en': \"I still don't understand what you're talking about.\",\n",
       "  'ru': 'Я так и не понял, о чём ты.'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(tatoeba['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['paraphrase: Влезай.'], ['Входи.'], 'ru_RU', 'ru_RU')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_paraphrase_task(batch_size=1):\n",
    "    task = 'paraphrase: '\n",
    "    if random.random() < 0.5:\n",
    "        src = opus_para_en\n",
    "        src_id, tgt_id = 'en_XX', 'en_XX'\n",
    "    else:\n",
    "        src = opus_para_ru\n",
    "        src_id, tgt_id = 'ru_RU', 'ru_RU'\n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(batch_size):\n",
    "        item = random.choice(src['train'])\n",
    "        xx, yy = item['input'], item['target']\n",
    "        if random.random() < 0.5:\n",
    "            xx, yy = yy, xx\n",
    "        x.append(task + xx)\n",
    "        y.append(yy)\n",
    "    return x, y, src_id, tgt_id\n",
    "        \n",
    "get_paraphrase_task(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['translate: I think Tom really likes Mary.',\n",
       "  'translate: Tom caught three fish yesterday.'],\n",
       " ['Я думаю, Мэри действительно нравится Тому.', 'Вчера Том поймал три рыбы.'],\n",
       " 'en_XX',\n",
       " 'ru_RU')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_translate_task(batch_size=1):\n",
    "    task = 'translate: '\n",
    "    src = random.choice([tatoeba, opensub, news_commentary])\n",
    "    src_id, tgt_id = 'ru_RU', 'en_XX'\n",
    "    if random.random() < 0.5:\n",
    "        src_id, tgt_id = tgt_id, src_id\n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(batch_size):\n",
    "        item = random.choice(src['train'])\n",
    "        x.append(task + item['translation'][src_id[:2]])\n",
    "        y.append(item['translation'][tgt_id[:2]])\n",
    "    return x, y, src_id, tgt_id\n",
    "\n",
    "get_translate_task(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(\n",
    "    text, model, tokenizer, \n",
    "    n=None, \n",
    "    max_length=\"auto\", \n",
    "    beams=5,\n",
    "    src_lang='en_XX',\n",
    "    tgt_lang='en_XX',\n",
    "    **kwargs\n",
    "):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    tokenizer.tgt_lang = tgt_lang\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    tokenizer.tgt_lang\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    if max_length == \"auto\":\n",
    "        max_length = inputs.shape[1] + 10\n",
    "\n",
    "    result = model.generate(\n",
    "        inputs,\n",
    "        num_return_sequences=n or 1,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        repetition_penalty=10.0,\n",
    "        max_length=max_length,\n",
    "        min_length=int(0.5 * max_length),\n",
    "        num_beams=beams,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'facebook/mbart-large-50'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size=4):\n",
    "    x = []\n",
    "    y = []\n",
    "    bs = batch_size // 2\n",
    "    for generator in [get_paraphrase_task, get_translate_task]:\n",
    "        for _ in range(bs):\n",
    "            xx, yy, src_id, tgt_id = generator(1)\n",
    "            tokenizer.src_lang, tokenizer.tgt_lang = src_id, tgt_id\n",
    "            x.append(tokenizer(xx))\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                y.append(tokenizer(yy))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'input_ids': [[250021, 121, 179665, 184, 12, 1089, 695, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 121, 179665, 184, 12, 536, 49, 3920, 29732, 103, 162991, 32, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 121, 179665, 184, 12, 16434, 15303, 49, 77, 197709, 4544, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 3900, 19309, 12, 7218, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 3900, 19309, 12, 345, 6953, 70, 21, 3055, 6626, 52895, 47, 6, 178451, 44, 15663, 10821, 130815, 7, 58, 98, 101334, 7, 1379, 2962, 7, 171827, 3098, 14202, 25, 144189, 2481, 50509, 214, 17265, 21, 24658, 214, 4, 4127, 23131, 7154, 4, 3900, 16082, 27771, 4, 23991, 80244, 4, 136, 111789, 165439, 102880, 46518, 5844, 47, 25313, 75678, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 3900, 19309, 12, 3499, 130525, 736, 2192, 52646, 77343, 164571, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}],\n",
       " [{'input_ids': [[250021, 1089, 15158, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 589, 92274, 414, 9, 23257, 129, 9, 84283, 67627, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 66385, 77, 106169, 197, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 201, 12678, 2]], 'attention_mask': [[1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 19832, 40924, 1316, 60303, 173061, 20803, 122576, 135, 66072, 1086, 110293, 227088, 25455, 52, 336, 50121, 7246, 6467, 103484, 1281, 63, 969, 800, 57980, 145458, 718, 44664, 2151, 109530, 1258, 91931, 183, 8635, 26464, 50203, 4, 224980, 35736, 4, 6, 139868, 4544, 4, 200130, 103366, 35, 150079, 95321, 1794, 5902, 57615, 4, 15319, 11312, 17330, 74881, 115987, 105, 132621, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 19507, 26, 7, 17686, 592, 24714, 344, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batch(6)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru_RU paraphrase: Ого.</s>\n",
      "ru_RU Ооо.</s>\n",
      "ru_RU paraphrase: А в этом дереве золото?</s>\n",
      "ru_RU Скажи что-нибудь по-английски.</s>\n",
      "ru_RU paraphrase: Ты здесь в небезопасности.</s>\n",
      "ru_RU Здесь не безопасно.</s>\n",
      "ru_RU translate: 49</s>\n",
      "en_XX 49%</s>\n",
      "en_XX translate: Using the latter two types to impose \"odious debts\" on Africans undermines western lenders' credibility concerning money laundering, good governance, transparency, fiscal discipline, and macroeconomic policies conducive to economic growth.</s>\n",
      "ru_RU Использование последних двух типов с целью обложить африканцев “ненавистными долгами” подрывает доверие к западным кредиторам относительно отмывания денег, хорошего управления, прозрачности, финансового порядка и макроэкономической политики, способствующих экономическому росту.</s>\n",
      "ru_RU translate: Выигрышный бросок Путина</s>\n",
      "en_XX Putin’s Winning Streak</s>\n"
     ]
    }
   ],
   "source": [
    "for xx, yy in zip(x, y):\n",
    "    print(tokenizer.decode(xx['input_ids'][0]))\n",
    "    print(tokenizer.decode(yy['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[250021, 121, 179665, 184, 12, 1089, 695, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [250021, 121, 179665, 184, 12, 536, 49, 3920, 29732, 103, 162991, 32, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [250021, 121, 179665, 184, 12, 16434, 15303, 49, 77, 197709, 4544, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [250021, 3900, 19309, 12, 7218, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [250004, 3900, 19309, 12, 345, 6953, 70, 21, 3055, 6626, 52895, 47, 6, 178451, 44, 15663, 10821, 130815, 7, 58, 98, 101334, 7, 1379, 2962, 7, 171827, 3098, 14202, 25, 144189, 2481, 50509, 214, 17265, 21, 24658, 214, 4, 4127, 23131, 7154, 4, 3900, 16082, 27771, 4, 23991, 80244, 4, 136, 111789, 165439, 102880, 46518, 5844, 47, 25313, 75678, 5, 2], [250021, 3900, 19309, 12, 3499, 130525, 736, 2192, 52646, 77343, 164571, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad([{k: v[0] for k, v in item.items()} for item in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor\n",
    "optimizer = Adafactor(model.parameters(), scale_parameter=False, relative_step=False, lr=1e-5, clip_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "num_warmup_steps = 1000\n",
    "\n",
    "def lr_lambda(current_step: int):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1.0, num_warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_steps = 4\n",
    "batch_size = 6\n",
    "window = 1000\n",
    "report_steps = 1000\n",
    "cleanup_steps = 100\n",
    "save_path = '/home/dale/models/detox-parallel/bart-multitask-pretrain-clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewm_loss = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ~6 iterations per second, 1M iteration takes 1000000/6/60/60=45 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e17201b475471ca011ba7590c1bedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000 loss 7.685543497323983\n",
      "step 2000 loss 6.70197135323358\n",
      "error 2682 CUDA out of memory. Tried to allocate 1.05 GiB (GPU 0; 10.76 GiB total capacity; 8.48 GiB already allocated; 983.69 MiB free; 8.67 GiB reserved in total by PyTorch)\n",
      "step 3000 loss 4.517895718230879\n",
      "step 4000 loss 3.0584388129091327\n",
      "step 5000 loss 2.369700513029202\n",
      "step 6000 loss 2.0188946715580154\n",
      "error 6212 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.30 GiB already allocated; 759.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "error 6243 CUDA out of memory. Tried to allocate 1.05 GiB (GPU 0; 10.76 GiB total capacity; 8.48 GiB already allocated; 555.69 MiB free; 9.09 GiB reserved in total by PyTorch)\n",
      "error 6971 CUDA out of memory. Tried to allocate 1002.00 MiB (GPU 0; 10.76 GiB total capacity; 8.22 GiB already allocated; 715.69 MiB free; 8.94 GiB reserved in total by PyTorch)\n",
      "step 7000 loss 1.8909473272931656\n",
      "error 7248 CUDA out of memory. Tried to allocate 870.00 MiB (GPU 0; 10.76 GiB total capacity; 8.37 GiB already allocated; 661.69 MiB free; 8.99 GiB reserved in total by PyTorch)\n",
      "step 8000 loss 1.9512600308374977\n",
      "error 8770 CUDA out of memory. Tried to allocate 946.00 MiB (GPU 0; 10.76 GiB total capacity; 8.18 GiB already allocated; 737.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
      "step 9000 loss 1.9232175123579365\n",
      "error 9428 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.54 GiB already allocated; 649.69 MiB free; 9.00 GiB reserved in total by PyTorch)\n",
      "step 10000 loss 1.881121753461566\n",
      "step 11000 loss 1.8071275012541763\n",
      "step 12000 loss 1.7868287378318066\n",
      "step 13000 loss 1.7739876047274645\n",
      "step 14000 loss 1.747984744671148\n",
      "step 15000 loss 1.7448928110075885\n",
      "step 16000 loss 1.7468867350748702\n",
      "error 16643 CUDA out of memory. Tried to allocate 1.01 GiB (GPU 0; 10.76 GiB total capacity; 8.38 GiB already allocated; 855.69 MiB free; 8.80 GiB reserved in total by PyTorch)\n",
      "step 17000 loss 1.749822178475067\n",
      "step 18000 loss 1.7468828634688442\n",
      "step 19000 loss 1.71736302943344\n",
      "error 19476 CUDA out of memory. Tried to allocate 756.00 MiB (GPU 0; 10.76 GiB total capacity; 7.98 GiB already allocated; 755.69 MiB free; 8.90 GiB reserved in total by PyTorch)\n",
      "step 20000 loss 1.701320042088571\n",
      "error 20076 CUDA out of memory. Tried to allocate 808.00 MiB (GPU 0; 10.76 GiB total capacity; 7.68 GiB already allocated; 619.69 MiB free; 9.03 GiB reserved in total by PyTorch)\n",
      "step 21000 loss 1.6836583315785498\n",
      "step 22000 loss 1.6740937539305951\n",
      "error 22475 CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 10.76 GiB total capacity; 7.62 GiB already allocated; 735.69 MiB free; 8.92 GiB reserved in total by PyTorch)\n",
      "step 23000 loss 1.6780517957338486\n",
      "error 23706 CUDA out of memory. Tried to allocate 940.00 MiB (GPU 0; 10.76 GiB total capacity; 7.92 GiB already allocated; 823.69 MiB free; 8.83 GiB reserved in total by PyTorch)\n",
      "step 24000 loss 1.661821777645683\n",
      "error 24474 CUDA out of memory. Tried to allocate 1.35 GiB (GPU 0; 10.76 GiB total capacity; 9.01 GiB already allocated; 15.69 MiB free; 9.62 GiB reserved in total by PyTorch)\n",
      "step 25000 loss 1.6590025896409075\n",
      "error 25776 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.64 GiB already allocated; 685.69 MiB free; 8.96 GiB reserved in total by PyTorch)\n",
      "step 26000 loss 1.6367005427853603\n",
      "step 27000 loss 1.622386962153115\n",
      "error 27259 CUDA out of memory. Tried to allocate 1.10 GiB (GPU 0; 10.76 GiB total capacity; 9.25 GiB already allocated; 229.69 MiB free; 9.41 GiB reserved in total by PyTorch)\n",
      "step 28000 loss 1.6137599675260323\n",
      "step 29000 loss 1.6031451601466995\n",
      "error 29742 CUDA out of memory. Tried to allocate 876.00 MiB (GPU 0; 10.76 GiB total capacity; 7.91 GiB already allocated; 803.69 MiB free; 8.85 GiB reserved in total by PyTorch)\n",
      "step 30000 loss 1.5937872043462666\n",
      "step 31000 loss 1.5910410596303182\n",
      "step 32000 loss 1.5790295184458791\n",
      "error 32531 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.27 GiB already allocated; 861.69 MiB free; 8.79 GiB reserved in total by PyTorch)\n",
      "error 32680 CUDA out of memory. Tried to allocate 790.00 MiB (GPU 0; 10.76 GiB total capacity; 8.01 GiB already allocated; 635.69 MiB free; 9.01 GiB reserved in total by PyTorch)\n",
      "step 33000 loss 1.575214990783198\n",
      "step 34000 loss 1.5553547863006512\n",
      "step 35000 loss 1.5562960358481657\n",
      "error 35659 CUDA out of memory. Tried to allocate 1.23 GiB (GPU 0; 10.76 GiB total capacity; 9.05 GiB already allocated; 407.69 MiB free; 9.24 GiB reserved in total by PyTorch)\n",
      "step 36000 loss 1.5590585020396137\n",
      "error 36908 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 8.28 GiB already allocated; 781.69 MiB free; 8.87 GiB reserved in total by PyTorch)\n",
      "step 37000 loss 1.5533477836380176\n",
      "error 37168 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 8.66 GiB already allocated; 65.69 MiB free; 9.57 GiB reserved in total by PyTorch)\n",
      "step 38000 loss 1.5562195916100041\n",
      "step 39000 loss 1.5430985538777595\n",
      "step 40000 loss 1.5336838978698102\n",
      "step 41000 loss 1.5249024890297507\n",
      "error 41207 CUDA out of memory. Tried to allocate 980.00 MiB (GPU 0; 10.76 GiB total capacity; 8.05 GiB already allocated; 773.69 MiB free; 8.88 GiB reserved in total by PyTorch)\n",
      "error 41771 CUDA out of memory. Tried to allocate 1.08 GiB (GPU 0; 10.76 GiB total capacity; 8.26 GiB already allocated; 1.03 GiB free; 8.60 GiB reserved in total by PyTorch)\n",
      "step 42000 loss 1.525440197274741\n",
      "step 43000 loss 1.523957028739022\n",
      "step 44000 loss 1.518951717188152\n",
      "error 44182 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.62 GiB already allocated; 633.69 MiB free; 9.02 GiB reserved in total by PyTorch)\n",
      "error 44696 CUDA out of memory. Tried to allocate 1.06 GiB (GPU 0; 10.76 GiB total capacity; 8.34 GiB already allocated; 875.69 MiB free; 8.78 GiB reserved in total by PyTorch)\n",
      "step 45000 loss 1.5034336524354117\n",
      "step 46000 loss 1.5127112411984063\n",
      "error 46796 CUDA out of memory. Tried to allocate 974.00 MiB (GPU 0; 10.76 GiB total capacity; 8.29 GiB already allocated; 805.69 MiB free; 8.85 GiB reserved in total by PyTorch)\n",
      "step 47000 loss 1.5143984193871796\n",
      "error 47924 CUDA out of memory. Tried to allocate 934.00 MiB (GPU 0; 10.76 GiB total capacity; 8.18 GiB already allocated; 743.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
      "step 48000 loss 1.511765981315629\n",
      "step 49000 loss 1.505421817635039\n",
      "step 50000 loss 1.5018793081574204\n",
      "step 51000 loss 1.4912787716417428\n",
      "error 51751 CUDA out of memory. Tried to allocate 974.00 MiB (GPU 0; 10.76 GiB total capacity; 8.02 GiB already allocated; 667.69 MiB free; 8.98 GiB reserved in total by PyTorch)\n",
      "step 52000 loss 1.490857680446083\n",
      "error 52151 CUDA out of memory. Tried to allocate 866.00 MiB (GPU 0; 10.76 GiB total capacity; 7.70 GiB already allocated; 767.69 MiB free; 8.88 GiB reserved in total by PyTorch)\n",
      "step 53000 loss 1.5092221523241975\n",
      "step 54000 loss 1.4924341926665288\n",
      "error 54242 CUDA out of memory. Tried to allocate 624.00 MiB (GPU 0; 10.76 GiB total capacity; 7.53 GiB already allocated; 465.69 MiB free; 9.18 GiB reserved in total by PyTorch)\n",
      "step 55000 loss 1.4754890033500894\n",
      "error 55776 CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 10.76 GiB total capacity; 7.93 GiB already allocated; 807.69 MiB free; 8.85 GiB reserved in total by PyTorch)\n",
      "step 56000 loss 1.4862058826672826\n",
      "error 56467 CUDA out of memory. Tried to allocate 974.00 MiB (GPU 0; 10.76 GiB total capacity; 8.01 GiB already allocated; 773.69 MiB free; 8.88 GiB reserved in total by PyTorch)\n",
      "error 56939 CUDA out of memory. Tried to allocate 1.03 GiB (GPU 0; 10.76 GiB total capacity; 8.35 GiB already allocated; 579.69 MiB free; 9.07 GiB reserved in total by PyTorch)\n",
      "step 57000 loss 1.4604952055000053\n",
      "step 58000 loss 1.4485737641533287\n",
      "step 59000 loss 1.451489341156636\n",
      "error 59662 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.58 GiB already allocated; 557.69 MiB free; 9.09 GiB reserved in total by PyTorch)\n",
      "step 60000 loss 1.460430446376063\n",
      "step 61000 loss 1.4644613228632044\n",
      "step 62000 loss 1.4608996634498117\n",
      "error 62844 CUDA out of memory. Tried to allocate 1.01 GiB (GPU 0; 10.76 GiB total capacity; 8.23 GiB already allocated; 861.69 MiB free; 8.79 GiB reserved in total by PyTorch)\n",
      "error 62878 CUDA out of memory. Tried to allocate 802.00 MiB (GPU 0; 10.76 GiB total capacity; 7.65 GiB already allocated; 613.69 MiB free; 9.04 GiB reserved in total by PyTorch)\n",
      "error 62975 CUDA out of memory. Tried to allocate 808.00 MiB (GPU 0; 10.76 GiB total capacity; 7.77 GiB already allocated; 727.69 MiB free; 8.92 GiB reserved in total by PyTorch)\n",
      "step 63000 loss 1.4625323597963378\n",
      "error 63676 CUDA out of memory. Tried to allocate 338.00 MiB (GPU 0; 10.76 GiB total capacity; 9.26 GiB already allocated; 267.69 MiB free; 9.37 GiB reserved in total by PyTorch)\n",
      "step 64000 loss 1.4506625578611676\n",
      "error 64500 CUDA out of memory. Tried to allocate 1.05 GiB (GPU 0; 10.76 GiB total capacity; 8.23 GiB already allocated; 669.69 MiB free; 8.98 GiB reserved in total by PyTorch)\n",
      "error 64678 CUDA out of memory. Tried to allocate 786.00 MiB (GPU 0; 10.76 GiB total capacity; 7.88 GiB already allocated; 725.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
      "step 65000 loss 1.460038404926861\n",
      "error 65218 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.39 GiB already allocated; 575.69 MiB free; 9.07 GiB reserved in total by PyTorch)\n",
      "error 65976 CUDA out of memory. Tried to allocate 1.08 GiB (GPU 0; 10.76 GiB total capacity; 8.37 GiB already allocated; 433.69 MiB free; 9.21 GiB reserved in total by PyTorch)\n",
      "step 66000 loss 1.4497334932439832\n",
      "step 67000 loss 1.448026714796901\n",
      "step 68000 loss 1.4394961688849013\n",
      "step 69000 loss 1.4386122232831187\n",
      "error 69056 CUDA out of memory. Tried to allocate 980.00 MiB (GPU 0; 10.76 GiB total capacity; 7.97 GiB already allocated; 647.69 MiB free; 9.00 GiB reserved in total by PyTorch)\n",
      "error 69178 CUDA out of memory. Tried to allocate 786.00 MiB (GPU 0; 10.76 GiB total capacity; 7.97 GiB already allocated; 761.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "step 70000 loss 1.4430715215218695\n",
      "error 70198 CUDA out of memory. Tried to allocate 1014.00 MiB (GPU 0; 10.76 GiB total capacity; 8.06 GiB already allocated; 885.69 MiB free; 8.77 GiB reserved in total by PyTorch)\n",
      "step 71000 loss 1.452261923563077\n",
      "step 72000 loss 1.4426369915214596\n",
      "step 73000 loss 1.4409931348112763\n",
      "step 74000 loss 1.4192696189270302\n",
      "error 74638 CUDA out of memory. Tried to allocate 906.00 MiB (GPU 0; 10.76 GiB total capacity; 7.75 GiB already allocated; 881.69 MiB free; 8.77 GiB reserved in total by PyTorch)\n",
      "step 75000 loss 1.40755528332965\n",
      "error 75032 CUDA out of memory. Tried to allocate 974.00 MiB (GPU 0; 10.76 GiB total capacity; 8.02 GiB already allocated; 805.69 MiB free; 8.85 GiB reserved in total by PyTorch)\n",
      "step 76000 loss 1.422036600638176\n",
      "step 77000 loss 1.4047147481202744\n",
      "step 78000 loss 1.419058893714476\n",
      "error 78168 CUDA out of memory. Tried to allocate 940.00 MiB (GPU 0; 10.76 GiB total capacity; 8.00 GiB already allocated; 825.69 MiB free; 8.83 GiB reserved in total by PyTorch)\n",
      "step 79000 loss 1.4271555495224713\n",
      "step 80000 loss 1.416891846454907\n",
      "step 81000 loss 1.4089182196400036\n",
      "step 82000 loss 1.3993580376519947\n",
      "step 83000 loss 1.4129659455729116\n",
      "step 84000 loss 1.3999934596260049\n",
      "step 85000 loss 1.3917589692907901\n",
      "step 86000 loss 1.3994255838846108\n",
      "error 86432 CUDA out of memory. Tried to allocate 894.00 MiB (GPU 0; 10.76 GiB total capacity; 7.74 GiB already allocated; 757.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "step 87000 loss 1.3948722036795764\n",
      "step 88000 loss 1.393383800065634\n",
      "step 89000 loss 1.3915535076286525\n",
      "step 90000 loss 1.39922423064372\n",
      "error 90897 CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 10.76 GiB total capacity; 8.15 GiB already allocated; 1021.69 MiB free; 8.64 GiB reserved in total by PyTorch)\n",
      "step 91000 loss 1.406029000743445\n",
      "step 92000 loss 1.4198878596902187\n",
      "error 92254 CUDA out of memory. Tried to allocate 1.34 GiB (GPU 0; 10.76 GiB total capacity; 7.79 GiB already allocated; 961.69 MiB free; 8.70 GiB reserved in total by PyTorch)\n",
      "step 93000 loss 1.412081575709657\n",
      "error 93368 CUDA out of memory. Tried to allocate 866.00 MiB (GPU 0; 10.76 GiB total capacity; 7.76 GiB already allocated; 845.69 MiB free; 8.81 GiB reserved in total by PyTorch)\n",
      "error 93832 CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 10.76 GiB total capacity; 7.78 GiB already allocated; 847.69 MiB free; 8.81 GiB reserved in total by PyTorch)\n",
      "step 94000 loss 1.3928001509916623\n",
      "error 94235 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.85 GiB already allocated; 193.69 MiB free; 9.45 GiB reserved in total by PyTorch)\n",
      "step 95000 loss 1.396877911764375\n",
      "step 96000 loss 1.3864390530064008\n",
      "error 96734 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 6.66 GiB already allocated; 971.69 MiB free; 8.69 GiB reserved in total by PyTorch)\n",
      "step 97000 loss 1.3895178842770421\n",
      "step 98000 loss 1.3911613028604404\n",
      "error 98679 CUDA out of memory. Tried to allocate 968.00 MiB (GPU 0; 10.76 GiB total capacity; 8.10 GiB already allocated; 739.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
      "step 99000 loss 1.3921740509343392\n",
      "error 99370 CUDA out of memory. Tried to allocate 986.00 MiB (GPU 0; 10.76 GiB total capacity; 8.36 GiB already allocated; 759.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "error 99575 CUDA out of memory. Tried to allocate 1.06 GiB (GPU 0; 10.76 GiB total capacity; 8.20 GiB already allocated; 475.69 MiB free; 9.17 GiB reserved in total by PyTorch)\n",
      "step 100000 loss 1.3873786034095132\n",
      "error 100464 CUDA out of memory. Tried to allocate 906.00 MiB (GPU 0; 10.76 GiB total capacity; 7.78 GiB already allocated; 799.69 MiB free; 8.85 GiB reserved in total by PyTorch)\n",
      "step 101000 loss 1.3727599721840793\n",
      "step 102000 loss 1.3785231163371936\n",
      "error 102116 CUDA out of memory. Tried to allocate 1.85 GiB (GPU 0; 10.76 GiB total capacity; 8.65 GiB already allocated; 623.69 MiB free; 9.03 GiB reserved in total by PyTorch)\n",
      "error 102204 CUDA out of memory. Tried to allocate 980.00 MiB (GPU 0; 10.76 GiB total capacity; 8.05 GiB already allocated; 795.69 MiB free; 8.86 GiB reserved in total by PyTorch)\n",
      "step 103000 loss 1.3716397523275163\n",
      "error 103152 CUDA out of memory. Tried to allocate 1.05 GiB (GPU 0; 10.76 GiB total capacity; 8.70 GiB already allocated; 449.69 MiB free; 9.20 GiB reserved in total by PyTorch)\n",
      "error 103546 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 8.02 GiB already allocated; 455.69 MiB free; 9.19 GiB reserved in total by PyTorch)\n",
      "step 104000 loss 1.3699797154694082\n",
      "step 105000 loss 1.3731694633356808\n",
      "step 106000 loss 1.3698143842624106\n",
      "step 107000 loss 1.3753898625231895\n",
      "step 108000 loss 1.3690263518546106\n",
      "step 109000 loss 1.363408730105658\n",
      "step 110000 loss 1.3678221174950385\n",
      "step 111000 loss 1.3609435636707699\n",
      "step 112000 loss 1.3717629011438957\n",
      "error 112942 CUDA out of memory. Tried to allocate 1.01 GiB (GPU 0; 10.76 GiB total capacity; 8.18 GiB already allocated; 727.69 MiB free; 8.92 GiB reserved in total by PyTorch)\n",
      "step 113000 loss 1.3645471219364644\n",
      "error 113815 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.34 GiB already allocated; 961.69 MiB free; 8.70 GiB reserved in total by PyTorch)\n",
      "step 114000 loss 1.3687081523530322\n",
      "error 114375 CUDA out of memory. Tried to allocate 796.00 MiB (GPU 0; 10.76 GiB total capacity; 7.54 GiB already allocated; 755.69 MiB free; 8.90 GiB reserved in total by PyTorch)\n",
      "error 114830 CUDA out of memory. Tried to allocate 780.00 MiB (GPU 0; 10.76 GiB total capacity; 7.63 GiB already allocated; 781.69 MiB free; 8.87 GiB reserved in total by PyTorch)\n",
      "step 115000 loss 1.3699991910978433\n",
      "step 116000 loss 1.3576943367703644\n",
      "step 117000 loss 1.3548798833626094\n",
      "error 117042 CUDA out of memory. Tried to allocate 1.03 GiB (GPU 0; 10.76 GiB total capacity; 8.35 GiB already allocated; 563.69 MiB free; 9.08 GiB reserved in total by PyTorch)\n",
      "step 118000 loss 1.3582057239848049\n",
      "step 119000 loss 1.3562632284332257\n",
      "step 120000 loss 1.3618899545720924\n",
      "step 121000 loss 1.3599588529784439\n",
      "error 121256 CUDA out of memory. Tried to allocate 1.24 GiB (GPU 0; 10.76 GiB total capacity; 7.92 GiB already allocated; 1.14 GiB free; 8.49 GiB reserved in total by PyTorch)\n",
      "error 121283 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.69 GiB already allocated; 935.69 MiB free; 8.72 GiB reserved in total by PyTorch)\n",
      "step 122000 loss 1.363331178129246\n",
      "error 122027 CUDA out of memory. Tried to allocate 1.68 GiB (GPU 0; 10.76 GiB total capacity; 8.30 GiB already allocated; 515.69 MiB free; 9.13 GiB reserved in total by PyTorch)\n",
      "error 122030 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 6.63 GiB already allocated; 639.69 MiB free; 9.01 GiB reserved in total by PyTorch)\n",
      "error 122380 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 8.17 GiB already allocated; 791.69 MiB free; 8.86 GiB reserved in total by PyTorch)\n",
      "step 123000 loss 1.3454697506294746\n",
      "step 124000 loss 1.3499564841934961\n",
      "step 125000 loss 1.3631498651371206\n",
      "error 125319 CUDA out of memory. Tried to allocate 1.11 GiB (GPU 0; 10.76 GiB total capacity; 8.50 GiB already allocated; 321.69 MiB free; 9.32 GiB reserved in total by PyTorch)\n",
      "step 126000 loss 1.3443661467326422\n",
      "step 127000 loss 1.352356152658898\n",
      "step 128000 loss 1.3587738317144602\n",
      "step 129000 loss 1.3304261710801788\n",
      "step 130000 loss 1.3346724784019148\n",
      "step 131000 loss 1.3394039118571806\n",
      "step 132000 loss 1.336822094337028\n",
      "step 133000 loss 1.3389031298911112\n",
      "step 134000 loss 1.3356821968066495\n",
      "step 135000 loss 1.3346046462620023\n",
      "error 135028 CUDA out of memory. Tried to allocate 670.00 MiB (GPU 0; 10.76 GiB total capacity; 9.02 GiB already allocated; 213.69 MiB free; 9.43 GiB reserved in total by PyTorch)\n",
      "error 135744 CUDA out of memory. Tried to allocate 1.01 GiB (GPU 0; 10.76 GiB total capacity; 7.94 GiB already allocated; 949.69 MiB free; 8.71 GiB reserved in total by PyTorch)\n",
      "step 136000 loss 1.3330715625696665\n",
      "error 136954 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.38 GiB already allocated; 817.69 MiB free; 8.84 GiB reserved in total by PyTorch)\n",
      "step 137000 loss 1.3399581943150007\n",
      "step 138000 loss 1.332270415880236\n",
      "error 138026 CUDA out of memory. Tried to allocate 906.00 MiB (GPU 0; 10.76 GiB total capacity; 8.24 GiB already allocated; 807.69 MiB free; 8.85 GiB reserved in total by PyTorch)\n",
      "step 139000 loss 1.3252052582555134\n",
      "error 139828 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.48 GiB already allocated; 933.69 MiB free; 8.72 GiB reserved in total by PyTorch)\n",
      "step 140000 loss 1.331851804957245\n",
      "step 141000 loss 1.342607625076955\n",
      "error 141612 CUDA out of memory. Tried to allocate 1.45 GiB (GPU 0; 10.76 GiB total capacity; 7.71 GiB already allocated; 1.15 GiB free; 8.48 GiB reserved in total by PyTorch)\n",
      "step 142000 loss 1.342715829175554\n",
      "step 143000 loss 1.3370200486971537\n",
      "error 143136 CUDA out of memory. Tried to allocate 1.03 GiB (GPU 0; 10.76 GiB total capacity; 8.18 GiB already allocated; 499.69 MiB free; 9.15 GiB reserved in total by PyTorch)\n",
      "step 144000 loss 1.325990632462179\n",
      "error 144116 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 8.27 GiB already allocated; 793.69 MiB free; 8.86 GiB reserved in total by PyTorch)\n",
      "step 145000 loss 1.3270154895226152\n",
      "step 146000 loss 1.3380369942007004\n",
      "error 146588 CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 10.76 GiB total capacity; 8.63 GiB already allocated; 325.69 MiB free; 9.32 GiB reserved in total by PyTorch)\n",
      "step 147000 loss 1.341093996172653\n",
      "step 148000 loss 1.3354121142412207\n",
      "step 149000 loss 1.3461268855175397\n",
      "step 150000 loss 1.3392868915012204\n",
      "step 151000 loss 1.3367320368138238\n",
      "step 152000 loss 1.32869363371164\n",
      "step 153000 loss 1.3169915066604867\n",
      "step 154000 loss 1.3124631103985465\n",
      "step 155000 loss 1.3294105613330123\n",
      "error 155676 CUDA out of memory. Tried to allocate 854.00 MiB (GPU 0; 10.76 GiB total capacity; 7.78 GiB already allocated; 637.69 MiB free; 9.01 GiB reserved in total by PyTorch)\n",
      "step 156000 loss 1.330162118479778\n",
      "step 157000 loss 1.327622778353876\n",
      "step 158000 loss 1.3251246059803272\n",
      "step 159000 loss 1.3223980904988601\n",
      "error 159084 CUDA out of memory. Tried to allocate 1.10 GiB (GPU 0; 10.76 GiB total capacity; 8.36 GiB already allocated; 331.69 MiB free; 9.31 GiB reserved in total by PyTorch)\n",
      "error 159948 CUDA out of memory. Tried to allocate 900.00 MiB (GPU 0; 10.76 GiB total capacity; 7.93 GiB already allocated; 763.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "step 160000 loss 1.3224868864597685\n",
      "step 161000 loss 1.3169144486448017\n",
      "step 162000 loss 1.3114346010437563\n",
      "step 163000 loss 1.3092822786614116\n",
      "error 163051 CUDA out of memory. Tried to allocate 1.03 GiB (GPU 0; 10.76 GiB total capacity; 8.10 GiB already allocated; 577.69 MiB free; 9.07 GiB reserved in total by PyTorch)\n",
      "step 164000 loss 1.3101414143763657\n",
      "error 164826 CUDA out of memory. Tried to allocate 974.00 MiB (GPU 0; 10.76 GiB total capacity; 7.97 GiB already allocated; 811.69 MiB free; 8.84 GiB reserved in total by PyTorch)\n",
      "error 164880 CUDA out of memory. Tried to allocate 1.07 GiB (GPU 0; 10.76 GiB total capacity; 8.45 GiB already allocated; 665.69 MiB free; 8.98 GiB reserved in total by PyTorch)\n",
      "step 165000 loss 1.307362995554762\n",
      "error 165042 CUDA out of memory. Tried to allocate 1.26 GiB (GPU 0; 10.76 GiB total capacity; 7.87 GiB already allocated; 791.69 MiB free; 8.86 GiB reserved in total by PyTorch)\n",
      "error 165250 CUDA out of memory. Tried to allocate 1.01 GiB (GPU 0; 10.76 GiB total capacity; 8.43 GiB already allocated; 603.69 MiB free; 9.04 GiB reserved in total by PyTorch)\n",
      "error 165490 CUDA out of memory. Tried to allocate 956.00 MiB (GPU 0; 10.76 GiB total capacity; 8.32 GiB already allocated; 709.69 MiB free; 8.94 GiB reserved in total by PyTorch)\n",
      "step 166000 loss 1.3090554182532297\n",
      "error 166220 CUDA out of memory. Tried to allocate 786.00 MiB (GPU 0; 10.76 GiB total capacity; 7.87 GiB already allocated; 757.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "step 167000 loss 1.3073391749802719\n",
      "step 168000 loss 1.3062859345657658\n",
      "error 168016 CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 10.76 GiB total capacity; 7.97 GiB already allocated; 649.69 MiB free; 9.00 GiB reserved in total by PyTorch)\n",
      "error 168984 CUDA out of memory. Tried to allocate 1.02 GiB (GPU 0; 10.76 GiB total capacity; 8.29 GiB already allocated; 529.69 MiB free; 9.12 GiB reserved in total by PyTorch)\n",
      "step 169000 loss 1.3090545854960944\n",
      "step 170000 loss 1.2984603822289822\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "tq = trange(0 , 1_000_000)\n",
    "cleanup()\n",
    "\n",
    "for i in tq:\n",
    "    x, y = get_batch(batch_size)\n",
    "    xpad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in x])\n",
    "    ypad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in y])\n",
    "    \n",
    "    try:\n",
    "        labels = torch.tensor(ypad['input_ids'], device=model.device)\n",
    "        labels[labels==tokenizer.pad_token_id] = -100\n",
    "        loss = model(\n",
    "            input_ids=torch.tensor(xpad['input_ids'], device=model.device),\n",
    "            attention_mask=torch.tensor(xpad['attention_mask'], device=model.device),\n",
    "            labels=labels,\n",
    "        ).loss\n",
    "        loss.backward()\n",
    "        if i % gradient_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "    except RuntimeError as e:\n",
    "        print('error', i, e)\n",
    "        loss = None\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        cleanup()\n",
    "        continue\n",
    "\n",
    "    w = 1 / max(1, min(i, window))\n",
    "    ewm_loss = ewm_loss * (1-w) + loss.item() * w\n",
    "    tq.set_description(f'{ewm_loss:3.4f}')\n",
    "\n",
    "    if i > 0 and i % report_steps == 0:\n",
    "        print('step', i, 'loss', ewm_loss, )\n",
    "        if i > 0:\n",
    "            model.save_pretrained(save_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "    if i % cleanup_steps == 0:\n",
    "        cleanup()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49999 0.7369652006657895\n"
     ]
    }
   ],
   "source": [
    "print(i, ewm_loss)\n",
    "# step 141000 loss 0.9336458999525007\n",
    "# 598943 0.7913156676625156\n",
    "# 999999 0.7528545096685079"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang = 'ru_RU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang = 'en_XX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('привет')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids('en_XX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paraphrase('paraphrase: I like to play with my nice dog.', model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paraphrase('translate: I like to play with my nice dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paraphrase('translate: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paraphrase('translate: Ненавижу играть со своей долбаной собакой.', model, tokenizer, src_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paraphrase('paraphrase: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='en_XX'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer from English to Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en2ru = pd.read_csv('detox_en2ru_yandex.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en2ru_filtered = detox_en2ru[\n",
    "    (detox_en2ru.edit_distance_ru >= detox_en2ru.edit_distance_en.quantile(0.01)) \n",
    "    & (detox_en2ru.edit_distance_ru <= detox_en2ru.edit_distance_en.quantile(0.99)) \n",
    "    & (detox_en2ru.edit_sim_ru >= detox_en2ru.edit_sim_en.quantile(0.01)) \n",
    "    & (detox_en2ru.edit_sim_ru <= detox_en2ru.edit_sim_en.quantile(0.99))\n",
    "]\n",
    "\n",
    "print(detox_en2ru.shape)\n",
    "print(detox_en2ru_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en2ru_filtered.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(detox_en2ru_filtered, random_state=1, test_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detox_task_en(batch_size=1):\n",
    "    task = 'detox: '\n",
    "    sample = train.sample(batch_size)\n",
    "    src_id, tgt_id = 'en_XX', 'en_XX'\n",
    "    x = []\n",
    "    y = []\n",
    "    for row in sample.itertuples():\n",
    "        x.append(task + row.toxic_comment)\n",
    "        y.append(row.neutral_comment)\n",
    "    return x, y, src_id, tgt_id\n",
    "\n",
    "get_detox_task_en(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detox_task_ru(batch_size=1):\n",
    "    task = 'detox: '\n",
    "    sample = train.sample(batch_size)\n",
    "    src_id, tgt_id = 'ru_RU', 'ru_RU'\n",
    "    x = []\n",
    "    y = []\n",
    "    for row in sample.itertuples():\n",
    "        x.append(task + row.toxic_ru)\n",
    "        y.append(row.neutral_ru)\n",
    "    return x, y, src_id, tgt_id\n",
    "\n",
    "get_detox_task_ru(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size=4):\n",
    "    x = []\n",
    "    y = []\n",
    "    bs = batch_size // 4\n",
    "    for generator in [get_detox_task_en, get_detox_task_ru, get_paraphrase_task, get_translate_task]:\n",
    "        for _ in range(bs):\n",
    "            xx, yy, src_id, tgt_id = generator(1)\n",
    "            tokenizer.src_lang, tokenizer.tgt_lang = src_id, tgt_id\n",
    "            x.append(tokenizer(xx))\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                y.append(tokenizer(yy))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/dale/models/detox-parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "save_path = '/home/dale/models/detox-parallel/bart-multitask-v4-ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "tq = trange(0, 50_000)\n",
    "cleanup()\n",
    "\n",
    "for i in tq:\n",
    "    x, y = get_batch(batch_size)\n",
    "    xpad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in x])\n",
    "    ypad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in y])\n",
    "    \n",
    "    try:\n",
    "        labels = torch.tensor(ypad['input_ids'], device=model.device)\n",
    "        labels[labels==tokenizer.pad_token_id] = -100\n",
    "        loss = model(\n",
    "            input_ids=torch.tensor(xpad['input_ids'], device=model.device),\n",
    "            attention_mask=torch.tensor(xpad['attention_mask'], device=model.device),\n",
    "            labels=labels,\n",
    "        ).loss\n",
    "        loss.backward()\n",
    "        if i % gradient_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "    except RuntimeError as e:\n",
    "        print('error', i, e)\n",
    "        loss = None\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        cleanup()\n",
    "        continue\n",
    "\n",
    "    w = 1 / max(1, min(i, window))\n",
    "    ewm_loss = ewm_loss * (1-w) + loss.item() * w\n",
    "    tq.set_description(f'{ewm_loss:3.4f}')\n",
    "\n",
    "    if i > 0 and i % report_steps == 0:\n",
    "        print('step', i, 'loss', ewm_loss, )\n",
    "        if i > 0:\n",
    "            model.save_pretrained(save_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "    if i % cleanup_steps == 0:\n",
    "        cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paraphrase('paraphrase: I like to play with my nice dog.', model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paraphrase('translate: I like to play with my nice dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paraphrase('translate: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paraphrase('translate: Ненавижу играть со своей долбаной собакой.', model, tokenizer, src_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paraphrase('paraphrase: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='en_XX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paraphrase('detox: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='en_XX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paraphrase('detox: Ненавижу играть со своей долбаной собакой.', model, tokenizer, src_lang='ru_RU', tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/russian_data/test.tsv', sep='\\t')\n",
    "test_inputs = test_data[\"toxic_comment\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = [paraphrase('detox: ' + text, model, tokenizer, src_lang='ru_RU', tgt_lang='ru_RU') for text in tqdm(test_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/multitask-1M-translate-yandex-v4/' \n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'results_ru.txt', 'w') as f:\n",
    "    for line in test_outputs:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full results (1m steps)  + 50K with Russian translated data\n",
    "```\n",
    "cd /home/dale/projects/multilingual_detox\n",
    "python evaluate_ru.py \\\n",
    "    --result_filename scores \\\n",
    "    --input_dir results/multitask-1M-translate-yandex-v4 \\\n",
    "    --output_dir results\n",
    "\n",
    "Style accuracy:       0.6400219798088074\n",
    "Meaning preservation: 0.8625849485397339\n",
    "Joint fluency:        -0.09933383762836456\n",
    "Joint score:          -0.052584096789360046\n",
    "Scores after calibration:\n",
    "Style accuracy:       0.6760197877883911\n",
    "Meaning preservation: 0.7941577434539795\n",
    "Joint fluency:        0.8857660889625549\n",
    "Joint score:          0.47143980860710144\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/english_data/test_toxic_parallel.txt', 'r') as f:\n",
    "    test_inputs_en = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/english_data/test_toxic_parallel.txt', 'r') as f:\n",
    "    test_inputs_en = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1a9dbc99c54af892d0b968ba338af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs_en = [paraphrase('detox: ' + text, model, tokenizer, src_lang='en_XX', tgt_lang='en_XX') for text in tqdm(test_inputs_en)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'results_en.txt', 'w') as f:\n",
    "    for line in test_outputs_en:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /home/dale/projects/paradetox2/evaluation_detox\n",
    "python metric.py --inputs /home/dale/projects/multilingual_detox/data/english_data/test_toxic_parallel.txt \\\n",
    "    --preds /home/dale/projects/multilingual_detox/results/multitask-1M-translate-yandex-v4/results_en.txt \\\n",
    "    --cola_classifier_path /home/dale/models/cola_classifier_fairseq \\\n",
    "    --wieting_model_path /home/dale/models/wieting_similarity/sim.pt \\\n",
    "    --wieting_tokenizer_path /home/dale/models/wieting_similarity/sim.sp.30k.model \\\n",
    "    --batch_size 32\n",
    "cat results.md\n",
    "```\n",
    "\n",
    "| Model | ACC | EMB_SIM | SIM | CharPPL | TokenPPL | FL | GM | J | BLEU |\n",
    "| ----- | --- | ------- | --- | ------- | -------- | -- | -- | - | ---- |\n",
    "results_en.txt|0.8852|0.9024|0.8583|6.5192|145.7229|0.8689|11.4473|0.6523|0.7337|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer from Russian to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/home/dale/models/detox-parallel/bart-multitask-pretrain-clean'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_ru2en = pd.read_csv('detox_ru2en_yandex.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7058, 16)\n",
      "(6739, 16)\n"
     ]
    }
   ],
   "source": [
    "detox_ru2en_filtered = detox_ru2en[\n",
    "    ((detox_ru2en.edit_distance_en >= detox_ru2en.edit_distance_ru.quantile(0.01)) \n",
    "    & (detox_ru2en.edit_distance_en <= detox_ru2en.edit_distance_ru.quantile(0.99)) \n",
    "    & (detox_ru2en.edit_sim_en >= detox_ru2en.edit_sim_ru.quantile(0.01)) \n",
    "    & (detox_ru2en.edit_sim_en <= detox_ru2en.edit_sim_ru.quantile(0.99))\n",
    "    )\n",
    "]\n",
    "\n",
    "print(detox_ru2en.shape)\n",
    "print(detox_ru2en_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6239, 16)\n",
      "(500, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(detox_ru2en_filtered, random_state=1, test_size=500)\n",
    "print(train.shape)\n",
    "print(val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>neutral_comment</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>toxic</th>\n",
       "      <th>confidence_toxic</th>\n",
       "      <th>is_match</th>\n",
       "      <th>confidence_is_match</th>\n",
       "      <th>split</th>\n",
       "      <th>toxic_en</th>\n",
       "      <th>neutral_en</th>\n",
       "      <th>edit_distance_ru</th>\n",
       "      <th>edit_distance_en</th>\n",
       "      <th>edit_sim_ru</th>\n",
       "      <th>edit_sim_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3830</th>\n",
       "      <td>6075</td>\n",
       "      <td>убить иее мала пусть бог иее рассудит</td>\n",
       "      <td>Пусть бог её рассудит</td>\n",
       "      <td>0.999279</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>train</td>\n",
       "      <td>kill iee small let god judge iee</td>\n",
       "      <td>May God judge her</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4426</th>\n",
       "      <td>12772</td>\n",
       "      <td>в поезд не сядет тот, кто помнит время ссср то...</td>\n",
       "      <td>В поезд не сядет тот, кто помнит время при гор...</td>\n",
       "      <td>0.999282</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9620</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9882</td>\n",
       "      <td>train</td>\n",
       "      <td>the one who remembers the time of the USSR onl...</td>\n",
       "      <td>No one who remembers the time under Gorbachev ...</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.619048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5663</th>\n",
       "      <td>10432</td>\n",
       "      <td>не смотрите, меня хватило на 30 минут этой мут...</td>\n",
       "      <td>Не смотрите, у меня хватило терпения на 30 мин...</td>\n",
       "      <td>0.991059</td>\n",
       "      <td>ru</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9689</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>dev</td>\n",
       "      <td>don't look, I had enough for 30 minutes of thi...</td>\n",
       "      <td>Don't look, I had enough patience for 30 minut...</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>0.452055</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        idx                                      toxic_comment  \\\n",
       "3830   6075              убить иее мала пусть бог иее рассудит   \n",
       "4426  12772  в поезд не сядет тот, кто помнит время ссср то...   \n",
       "5663  10432  не смотрите, меня хватило на 30 минут этой мут...   \n",
       "\n",
       "                                        neutral_comment  toxicity_score  \\\n",
       "3830                              Пусть бог её рассудит        0.999279   \n",
       "4426  В поезд не сядет тот, кто помнит время при гор...        0.999282   \n",
       "5663  Не смотрите, у меня хватило терпения на 30 мин...        0.991059   \n",
       "\n",
       "     dataset  toxic  confidence_toxic  is_match  confidence_is_match  split  \\\n",
       "3830      ru  False            0.9999      True               0.9839  train   \n",
       "4426      ru  False            0.9620      True               0.9882  train   \n",
       "5663      ru  False            0.9689      True               0.9915    dev   \n",
       "\n",
       "                                               toxic_en  \\\n",
       "3830                   kill iee small let god judge iee   \n",
       "4426  the one who remembers the time of the USSR onl...   \n",
       "5663  don't look, I had enough for 30 minutes of thi...   \n",
       "\n",
       "                                             neutral_en  edit_distance_ru  \\\n",
       "3830                                  May God judge her                18   \n",
       "4426  No one who remembers the time under Gorbachev ...                32   \n",
       "5663  Don't look, I had enough patience for 30 minut...                40   \n",
       "\n",
       "      edit_distance_en  edit_sim_ru  edit_sim_en  \n",
       "3830                20     0.513514     0.375000  \n",
       "4426                40     0.614458     0.619048  \n",
       "5663                24     0.452055     0.666667  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['detox: God, what kind of fuckers are there in the world. ..',\n",
       "  'detox: to be honest, the shit is rare'],\n",
       " ['My God, who is not in this world', \"To be honest, it's rare nonsense\"],\n",
       " 'en_XX',\n",
       " 'en_XX')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_detox_task_en(batch_size=1):\n",
    "    task = 'detox: '\n",
    "    sample = train.sample(batch_size)\n",
    "    src_id, tgt_id = 'en_XX', 'en_XX'\n",
    "    x = []\n",
    "    y = []\n",
    "    for row in sample.itertuples():\n",
    "        x.append(task + row.toxic_en)\n",
    "        y.append(row.neutral_en)\n",
    "    return x, y, src_id, tgt_id\n",
    "\n",
    "get_detox_task_en(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['detox: никому не нужны чужие выблядки.',\n",
       "  'detox: суки опять какую то хуйню спустили в реку уебки'],\n",
       " ['Никому не нужны чужие дети', 'Опять что-то в реку спустили'],\n",
       " 'ru_RU',\n",
       " 'ru_RU')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_detox_task_ru(batch_size=1):\n",
    "    task = 'detox: '\n",
    "    sample = train.sample(batch_size)\n",
    "    src_id, tgt_id = 'ru_RU', 'ru_RU'\n",
    "    x = []\n",
    "    y = []\n",
    "    for row in sample.itertuples():\n",
    "        x.append(task + row.toxic_comment)\n",
    "        y.append(row.neutral_comment)\n",
    "    return x, y, src_id, tgt_id\n",
    "\n",
    "get_detox_task_ru(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size=4):\n",
    "    x = []\n",
    "    y = []\n",
    "    bs = batch_size // 4\n",
    "    for generator in [get_detox_task_en, get_detox_task_ru, get_paraphrase_task, get_translate_task]:\n",
    "        for _ in range(bs):\n",
    "            xx, yy, src_id, tgt_id = generator(1)\n",
    "            tokenizer.src_lang, tokenizer.tgt_lang = src_id, tgt_id\n",
    "            x.append(tokenizer(xx))\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                y.append(tokenizer(yy))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_steps = 4\n",
    "window = 1000\n",
    "report_steps = 1000\n",
    "cleanup_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dale/models/detox-parallel/bart-multitask-v4-en'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "save_path = '/home/dale/models/detox-parallel/bart-multitask-v4-en'\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor\n",
    "optimizer = Adafactor(model.parameters(), scale_parameter=False, relative_step=False, lr=1e-5, clip_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewm_loss = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa40e34428a546e1aaa9d49b5253d8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 72 CUDA out of memory. Tried to allocate 990.00 MiB (GPU 0; 10.76 GiB total capacity; 8.05 GiB already allocated; 739.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
      "step 1000 loss 1.2333002176196652\n",
      "step 2000 loss 1.172667905884665\n",
      "step 3000 loss 1.1308728053043728\n",
      "step 4000 loss 1.0866697279344393\n",
      "step 5000 loss 1.0693783238909067\n",
      "step 6000 loss 1.0342307965602748\n",
      "step 7000 loss 1.0176406868389598\n",
      "step 8000 loss 1.00346553988644\n",
      "step 9000 loss 0.9658832415699443\n",
      "step 10000 loss 0.9669977334382361\n",
      "step 11000 loss 0.9555475747068011\n",
      "step 12000 loss 0.9293789452994402\n",
      "step 13000 loss 0.9130843327986824\n",
      "step 14000 loss 0.8970300269012222\n",
      "step 15000 loss 0.8924606968906944\n",
      "step 16000 loss 0.8660455307541913\n",
      "step 17000 loss 0.8734970424154661\n",
      "step 18000 loss 0.8545141288301037\n",
      "step 19000 loss 0.8468137140176221\n",
      "step 20000 loss 0.8327704592709183\n",
      "step 21000 loss 0.8169362085610442\n",
      "step 22000 loss 0.8032452978234469\n",
      "step 23000 loss 0.7969312192213301\n",
      "step 24000 loss 0.7958858616933577\n",
      "step 25000 loss 0.7772726818867776\n",
      "step 26000 loss 0.7684846786265179\n",
      "step 27000 loss 0.7588623079144335\n",
      "step 28000 loss 0.7405269905399514\n",
      "step 29000 loss 0.7325993780169252\n",
      "step 30000 loss 0.7253928889708445\n",
      "step 31000 loss 0.7164386858338531\n",
      "error 31872 CUDA out of memory. Tried to allocate 840.00 MiB (GPU 0; 10.76 GiB total capacity; 8.84 GiB already allocated; 49.69 MiB free; 9.59 GiB reserved in total by PyTorch)\n",
      "step 32000 loss 0.7181800801251085\n",
      "error 32754 CUDA out of memory. Tried to allocate 662.00 MiB (GPU 0; 10.76 GiB total capacity; 8.42 GiB already allocated; 229.69 MiB free; 9.41 GiB reserved in total by PyTorch)\n",
      "step 33000 loss 0.7071413058014595\n",
      "step 34000 loss 0.7038053330874191\n",
      "step 35000 loss 0.685612823567147\n",
      "step 36000 loss 0.682500082991742\n",
      "step 37000 loss 0.6861666156229685\n",
      "step 38000 loss 0.6773540363147127\n",
      "step 39000 loss 0.665850235755407\n",
      "step 40000 loss 0.6566127439174155\n",
      "step 41000 loss 0.670812675932586\n",
      "step 42000 loss 0.663120443310744\n",
      "step 43000 loss 0.6557544976693371\n",
      "step 44000 loss 0.6451127748673838\n",
      "step 45000 loss 0.6431719076613552\n",
      "step 46000 loss 0.6350072618706817\n",
      "step 47000 loss 0.6354631718668585\n",
      "step 48000 loss 0.6446072685378472\n",
      "step 49000 loss 0.6333267919700374\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "tq = trange(0, 50_000)\n",
    "cleanup()\n",
    "\n",
    "for i in tq:\n",
    "    x, y = get_batch(batch_size)\n",
    "    xpad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in x])\n",
    "    ypad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in y])\n",
    "    \n",
    "    try:\n",
    "        labels = torch.tensor(ypad['input_ids'], device=model.device)\n",
    "        labels[labels==tokenizer.pad_token_id] = -100\n",
    "        loss = model(\n",
    "            input_ids=torch.tensor(xpad['input_ids'], device=model.device),\n",
    "            attention_mask=torch.tensor(xpad['attention_mask'], device=model.device),\n",
    "            labels=labels,\n",
    "        ).loss\n",
    "        loss.backward()\n",
    "        if i % gradient_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            #scheduler.step()\n",
    "    except RuntimeError as e:\n",
    "        print('error', i, e)\n",
    "        loss = None\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        cleanup()\n",
    "        continue\n",
    "\n",
    "    w = 1 / max(1, min(i, window))\n",
    "    ewm_loss = ewm_loss * (1-w) + loss.item() * w\n",
    "    tq.set_description(f'{ewm_loss:3.4f}')\n",
    "\n",
    "    if i > 0 and i % report_steps == 0:\n",
    "        print('step', i, 'loss', ewm_loss, )\n",
    "        if i > 0:\n",
    "            model.save_pretrained(save_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "    if i % cleanup_steps == 0:\n",
    "        cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to play with my dog, you know?\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('paraphrase: I like to play with my nice dog.', model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мне нравится играть со своей милой собакой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: I like to play with my nice dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ненавижу играть со своей собакой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate to play with my dog, you bastard.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: Ненавижу играть со своей долбаной собакой.', model, tokenizer, src_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't want to play with my dog.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('paraphrase: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='en_XX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't like to play with my dog\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('detox: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='en_XX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я ненавижу играть со своей собакой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('detox: Ненавижу играть со своей долбаной собакой.', model, tokenizer, src_lang='ru_RU', tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/russian_data/test.tsv', sep='\\t')\n",
    "test_inputs = test_data[\"toxic_comment\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882685339e464aaf8811122e6cf939ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs = [paraphrase('detox: ' + text, model, tokenizer, src_lang='ru_RU', tgt_lang='ru_RU') for text in tqdm(test_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/multitask-1M-translate-yandex-v4-ru2en/' \n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'results_ru.txt', 'w') as f:\n",
    "    for line in test_outputs:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full results (1m steps)  + 50K with Russian translated data\n",
    "```\n",
    "cd /home/dale/projects/multilingual_detox\n",
    "python evaluate_ru.py \\\n",
    "    --result_filename scores \\\n",
    "    --input_dir results/multitask-1M-translate-yandex-v4-ru2en \\\n",
    "    --output_dir results\n",
    "\n",
    "Style accuracy:       0.7263924479484558\n",
    "Meaning preservation: 0.8321331739425659\n",
    "Joint fluency:        -0.1356080323457718\n",
    "Joint score:          -0.07764336466789246\n",
    "Scores after calibration:\n",
    "Style accuracy:       0.7537532448768616\n",
    "Meaning preservation: 0.7489004135131836\n",
    "Joint fluency:        0.8440507650375366\n",
    "Joint score:          0.4865816831588745\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/english_data/test_toxic_parallel.txt', 'r') as f:\n",
    "    test_inputs_en = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7973365ff9e4ba1b95e3e29cc215181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs_en = [paraphrase('detox: ' + text, model, tokenizer, src_lang='en_XX', tgt_lang='en_XX') for text in tqdm(test_inputs_en)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'results_en.txt', 'w') as f:\n",
    "    for line in test_outputs_en:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /home/dale/projects/paradetox2/evaluation_detox\n",
    "python metric.py --inputs /home/dale/projects/multilingual_detox/data/english_data/test_toxic_parallel.txt \\\n",
    "    --preds /home/dale/projects/multilingual_detox/results/multitask-1M-translate-yandex-v4-ru2en/results_en.txt \\\n",
    "    --cola_classifier_path /home/dale/models/cola_classifier_fairseq \\\n",
    "    --wieting_model_path /home/dale/models/wieting_similarity/sim.pt \\\n",
    "    --wieting_tokenizer_path /home/dale/models/wieting_similarity/sim.sp.30k.model \\\n",
    "    --batch_size 32\n",
    "cat results.md\n",
    "```\n",
    "\n",
    "| Model | ACC | EMB_SIM | SIM | CharPPL | TokenPPL | FL | GM | J | BLEU |\n",
    "| ----- | --- | ------- | --- | ------- | -------- | -- | -- | - | ---- |\n",
    "results_en.txt|0.7273|0.8489|0.8193|6.0657|104.0004|0.8838|0.0000|0.5032|0.6794|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3k",
   "language": "python",
   "name": "p3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
