{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem statement, the encoder does not know in what language it is going to decode, so it has to produce language-invariant embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import random\n",
    "\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect the training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Paraphrasing: https://huggingface.co/datasets/GEM/opusparcus\n",
    "* Translation: https://huggingface.co/datasets/open_subtitles + news_commentary? + tatoeba?\n",
    "* Detox: ordinary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opusparcus (/home/dale/.cache/huggingface/datasets/GEM___opusparcus/en.80/1.0.0/79d36ae4eced4f3c2c5a2ab9f94a584de7adca957186408d33798d0d87b018f2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a12dc44f6e84aef872491c004651cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 982\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1015\n",
       "    })\n",
       "    test.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1445\n",
       "    })\n",
       "    validation.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1455\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 5200000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opus_para_en = load_dataset(\"GEM/opusparcus\", \"en.80\")\n",
    "opus_para_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opusparcus (/home/dale/.cache/huggingface/datasets/GEM___opusparcus/ru.80/1.0.0/79d36ae4eced4f3c2c5a2ab9f94a584de7adca957186408d33798d0d87b018f2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4034d958e9bb4220a1050a3f40fbd7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1068\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1020\n",
       "    })\n",
       "    test.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1855\n",
       "    })\n",
       "    validation.full: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 1854\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['lang', 'input', 'target', 'annot_score', 'gem_id', 'references'],\n",
       "        num_rows: 2300000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opus_para_ru = load_dataset(\"GEM/opusparcus\", \"ru.80\")\n",
    "opus_para_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lang': 'ru',\n",
       " 'input': 'Я думаю, что могу.',\n",
       " 'target': 'Я думаю, что делаю.',\n",
       " 'annot_score': 0.0,\n",
       " 'gem_id': 'gem-opusparcus-train-79213069',\n",
       " 'references': ['Я думаю, что делаю.']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(opus_para_ru['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-ru-lang1=en,lang2=ru\n",
      "Reusing dataset open_subtitles (/home/dale/.cache/huggingface/datasets/open_subtitles/en-ru-lang1=en,lang2=ru/0.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0633716ebce44876be4d97e24492f49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'meta', 'translation'],\n",
       "        num_rows: 25910105\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opensub = load_dataset(\"open_subtitles\", lang1=\"en\", lang2='ru')\n",
    "opensub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '11986737',\n",
       " 'meta': {'year': 2011,\n",
       "  'imdbId': 1640807,\n",
       "  'subtitleId': {'en': 4414177, 'ru': 6337203},\n",
       "  'sentenceIds': {'en': [961], 'ru': [907]}},\n",
       " 'translation': {'en': 'Package?', 'ru': 'Посылка?'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(opensub['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-ru-lang1=en,lang2=ru\n",
      "Reusing dataset news_commentary (/home/dale/.cache/huggingface/datasets/news_commentary/en-ru-lang1=en,lang2=ru/0.0.0/cfab724ce975dc2da51cdae45302389860badc88b74db8570d561ced6004f8b4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cd05d3c6154c2da0c14ab7d5ee4a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 190104\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_commentary = load_dataset(\"news_commentary\", lang1=\"en\", lang2='ru')\n",
    "news_commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '105617',\n",
       " 'translation': {'en': 'Peace-related activities – particularly reintegration and other reconciliation programs – need to be given priority in budget allocations.',\n",
       "  'ru': 'Деятельность, направленная на установление мира, – в особенности возвращение людей в общество и т.п. – должна пользоваться приоритетом при распределении бюджета.'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(news_commentary['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-ru-lang1=en,lang2=ru\n",
      "Reusing dataset tatoeba (/home/dale/.cache/huggingface/datasets/tatoeba/en-ru-lang1=en,lang2=ru/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2c0e7735a348e185e4f522866d1f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 523656\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tatoeba = load_dataset(\"tatoeba\", lang1=\"en\", lang2='ru')\n",
    "tatoeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '40914',\n",
       " 'translation': {'en': \"I'd like to dance with you.\",\n",
       "  'ru': 'Я хотел бы потанцевать с тобой.'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(tatoeba['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Argh, me back.'], [\"I'm back.\"], 'en_XX', 'en_XX')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_paraphrase_task(batch_size=1):\n",
    "    task = '' #'paraphrase: '\n",
    "    if random.random() < 0.5:\n",
    "        src = opus_para_en\n",
    "        src_id, tgt_id = 'en_XX', 'en_XX'\n",
    "    else:\n",
    "        src = opus_para_ru\n",
    "        src_id, tgt_id = 'ru_RU', 'ru_RU'\n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(batch_size):\n",
    "        item = random.choice(src['train'])\n",
    "        xx, yy = item['input'], item['target']\n",
    "        if random.random() < 0.5:\n",
    "            xx, yy = yy, xx\n",
    "        x.append(task + xx)\n",
    "        y.append(yy)\n",
    "    return x, y, src_id, tgt_id\n",
    "        \n",
    "get_paraphrase_task(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"You don't get to go to hotels with boys that I don't know, with boys I do know, with any kind of boy!\",\n",
       "  'I want some fucking Colace.'],\n",
       " ['15 лет! Ты не должна ходить по отелям с парнями, которых я не знаю. И даже с парнями, которых я знаю!',\n",
       "  'Мне нужен Колас.'],\n",
       " 'en_XX',\n",
       " 'ru_RU')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_translate_task(batch_size=1):\n",
    "    task = '' # 'translate: '\n",
    "    src = random.choice([tatoeba, opensub, news_commentary])\n",
    "    src_id, tgt_id = 'ru_RU', 'en_XX'\n",
    "    if random.random() < 0.5:\n",
    "        src_id, tgt_id = tgt_id, src_id\n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(batch_size):\n",
    "        item = random.choice(src['train'])\n",
    "        x.append(task + item['translation'][src_id[:2]])\n",
    "        y.append(item['translation'][tgt_id[:2]])\n",
    "    return x, y, src_id, tgt_id\n",
    "\n",
    "get_translate_task(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'facebook/mbart-large-50'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size=4):\n",
    "    x = []\n",
    "    y = []\n",
    "    bs = batch_size // 2\n",
    "    for generator in [get_paraphrase_task, get_translate_task]:\n",
    "        for _ in range(bs):\n",
    "            xx, yy, src_id, tgt_id = generator(1)\n",
    "            tokenizer.src_lang, tokenizer.tgt_lang = src_id, tgt_id\n",
    "            x.append(tokenizer(xx))\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                y.append(tokenizer(yy))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'input_ids': [[250004, 32774, 6528, 4, 642, 1221, 73203, 25842, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 1401, 2258, 20740, 297, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 33473, 4, 4789, 77, 131666, 414, 89476, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 1509, 19458, 7193, 811, 4, 12386, 96357, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 1851, 690, 4, 414, 23494, 1725, 33868, 49, 37325, 2325, 115458, 6, 22782, 49, 1392, 8165, 165769, 24192, 106, 80389, 1745, 8118, 38850, 49, 38592, 46, 5641, 424, 242910, 244, 114, 827, 9561, 46, 12578, 57750, 10757, 87879, 227, 36553, 4544, 5, 447, 34496, 99582, 4, 414, 2176, 3216, 1086, 7736, 88426, 213585, 19596, 129, 210507, 216019, 312, 4, 68929, 31825, 36553, 4544, 718, 42066, 8165, 4, 20946, 145089, 13299, 54064, 1219, 49, 2660, 4, 3077, 114628, 90870, 312, 518, 228317, 36553, 4544, 129, 40206, 2151, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 113522, 1584, 38800, 82661, 6467, 218, 3281, 55848, 6439, 1281, 35, 226027, 89, 80532, 38850, 30296, 255, 6, 48531, 1082, 32602, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}],\n",
       " [{'input_ids': [[250004, 581, 118285, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 9925, 25, 7, 10, 5915, 111, 132442, 1257, 3688, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250021, 16434, 1355, 77, 131666, 4, 7193, 1097, 4387, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 87, 3714, 2750, 398, 621, 4, 1657, 13665, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 7494, 181175, 69503, 1302, 641, 4239, 164508, 460, 1682, 4078, 471, 38629, 5, 2588, 471, 123021, 659, 146236, 10559, 8663, 9917, 125, 13284, 29273, 44588, 13073, 968, 146190, 175584, 23508, 11692, 4239, 460, 1682, 5850, 1392, 421, 29405, 471, 96671, 6370, 13465, 2220, 91610, 659, 20, 10619, 32534, 15728, 421, 646, 967, 20, 21277, 81495, 27104, 8906, 9917, 125, 4239, 134040, 9179, 1682, 10215, 10131, 646, 2093, 42066, 7231, 223801, 4, 191575, 123021, 659, 629, 126036, 4050, 287, 50660, 287, 1812, 9974, 84506, 1293, 4, 122488, 114396, 17290, 123021, 659, 166078, 95760, 629, 4592, 2138, 1187, 287, 1812, 4592, 31859, 10664, 629, 58498, 175867, 3813, 125, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input_ids': [[250004, 83664, 933, 621, 70, 7311, 61527, 72004, 4, 136, 1836, 3542, 70, 144732, 43904, 24189, 70, 1492, 927, 142642, 5, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batch(6)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_XX Great idea, we will eat together.</s>\n",
      "en_XX The breakfast.</s>\n",
      "en_XX We carpooled.</s>\n",
      "en_XX That's a lot of driving up here.</s>\n",
      "ru_RU Нет, ты не знаешь что произошло.</s>\n",
      "ru_RU Ты же не знаешь, кто это был.</s>\n",
      "ru_RU Я знаю кто вы, Коннор.</s>\n",
      "en_XX I know who you are, Connor.</s>\n",
      "ru_RU Но то, что должно их действительно встревожить ‐ в 2011 году приблизительно 14,5 % всего населения в мире – один из каждых семи человек – жил ниже этой черты бедности. Учитывая, что мы уже обязались достичь цели по ликвидации чрезвычайной, хронической бедности к 2030 году, наше первое решение состояло в том, чтобы считать критерий для измерения бедности постоянным.</s>\n",
      "en_XX कुछ आलोचकों का यह तर्क है कि 2005 की $1.25 की गरीबी रेखा बहुत कम थी। लेकिन उन्हें जिस बात पर चिंता करनी चाहिए वह यह है कि वर्ष 2011 में दुनिया की लगभग 14.5% आबादी - हर सात लोगों में से एक - इसके नीचे रह रही थी। यह देखते हुए कि हम पहले से ही 2030 तक चरम, चिर गरीबी को समाप्त करने के लक्ष्य के लिए प्रतिबद्ध हैं, हमारा पहला निर्णय गरीबी स्थिरांक को मापने के लिए मापदंड को बनाए रखना था।</s>\n",
      "ru_RU Копты являются коренными египтянами и составляли большинство населения страны до десятого века.</s>\n",
      "en_XX Copts are the original Egyptians, and they were the majority population until the tenth century.</s>\n"
     ]
    }
   ],
   "source": [
    "for xx, yy in zip(x, y):\n",
    "    print(tokenizer.decode(xx['input_ids'][0]))\n",
    "    print(tokenizer.decode(yy['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[250004, 32774, 6528, 4, 642, 1221, 73203, 25842, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [250004, 1401, 2258, 20740, 297, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [250021, 33473, 4, 4789, 77, 131666, 414, 89476, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [250021, 1509, 19458, 7193, 811, 4, 12386, 96357, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [250021, 1851, 690, 4, 414, 23494, 1725, 33868, 49, 37325, 2325, 115458, 6, 22782, 49, 1392, 8165, 165769, 24192, 106, 80389, 1745, 8118, 38850, 49, 38592, 46, 5641, 424, 242910, 244, 114, 827, 9561, 46, 12578, 57750, 10757, 87879, 227, 36553, 4544, 5, 447, 34496, 99582, 4, 414, 2176, 3216, 1086, 7736, 88426, 213585, 19596, 129, 210507, 216019, 312, 4, 68929, 31825, 36553, 4544, 718, 42066, 8165, 4, 20946, 145089, 13299, 54064, 1219, 49, 2660, 4, 3077, 114628, 90870, 312, 518, 228317, 36553, 4544, 129, 40206, 2151, 5, 2], [250021, 113522, 1584, 38800, 82661, 6467, 218, 3281, 55848, 6439, 1281, 35, 226027, 89, 80532, 38850, 30296, 255, 6, 48531, 1082, 32602, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad([{k: v[0] for k, v in item.items()} for item in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor\n",
    "optimizer = Adafactor(model.parameters(), scale_parameter=False, relative_step=False, lr=1e-5, clip_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "num_warmup_steps = 1000\n",
    "\n",
    "def lr_lambda(current_step: int):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1.0, num_warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_steps = 4\n",
    "batch_size = 6\n",
    "window = 1000\n",
    "report_steps = 1000\n",
    "cleanup_steps = 100\n",
    "save_path = '/home/dale/models/detox-parallel/bart-multitask-pretrain-invariant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewm_loss = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ~6 iterations per second, 1M iteration takes 1000000/6/60/60=45 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195f3c8a2d0d48d89d2d1c2fb7fe92ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000 loss 7.8495250988006555\n",
      "error 1151 CUDA out of memory. Tried to allocate 1002.00 MiB (GPU 0; 10.76 GiB total capacity; 8.36 GiB already allocated; 725.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
      "step 2000 loss 6.971980854110847\n",
      "step 3000 loss 4.503658014299016\n",
      "step 4000 loss 3.0397753827503657\n",
      "step 5000 loss 2.328842712437607\n",
      "error 5990 CUDA out of memory. Tried to allocate 854.00 MiB (GPU 0; 10.76 GiB total capacity; 8.12 GiB already allocated; 787.69 MiB free; 8.87 GiB reserved in total by PyTorch)\n",
      "step 6000 loss 2.0246113876562606\n",
      "step 7000 loss 1.8595813628118134\n",
      "error 7090 CUDA out of memory. Tried to allocate 1.02 GiB (GPU 0; 10.76 GiB total capacity; 8.28 GiB already allocated; 525.69 MiB free; 9.12 GiB reserved in total by PyTorch)\n",
      "error 7115 CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 10.76 GiB total capacity; 9.40 GiB already allocated; 151.69 MiB free; 9.49 GiB reserved in total by PyTorch)\n",
      "error 7338 CUDA out of memory. Tried to allocate 912.00 MiB (GPU 0; 10.76 GiB total capacity; 7.82 GiB already allocated; 797.69 MiB free; 8.86 GiB reserved in total by PyTorch)\n",
      "error 7926 CUDA out of memory. Tried to allocate 912.00 MiB (GPU 0; 10.76 GiB total capacity; 8.09 GiB already allocated; 799.69 MiB free; 8.85 GiB reserved in total by PyTorch)\n",
      "step 8000 loss 1.7656280765961248\n",
      "error 8075 CUDA out of memory. Tried to allocate 1.09 GiB (GPU 0; 10.76 GiB total capacity; 8.46 GiB already allocated; 587.69 MiB free; 9.06 GiB reserved in total by PyTorch)\n",
      "step 9000 loss 1.7232480028436197\n",
      "step 10000 loss 1.7234741253498884\n",
      "error 10670 CUDA out of memory. Tried to allocate 1.68 GiB (GPU 0; 10.76 GiB total capacity; 8.24 GiB already allocated; 1.14 GiB free; 8.49 GiB reserved in total by PyTorch)\n",
      "step 11000 loss 1.6784266652217328\n",
      "step 12000 loss 1.6633502409092902\n",
      "step 13000 loss 1.6524237591194655\n",
      "error 13827 CUDA out of memory. Tried to allocate 876.00 MiB (GPU 0; 10.76 GiB total capacity; 8.06 GiB already allocated; 813.69 MiB free; 8.84 GiB reserved in total by PyTorch)\n",
      "step 14000 loss 1.6367858148524403\n",
      "error 14126 CUDA out of memory. Tried to allocate 1.55 GiB (GPU 0; 10.76 GiB total capacity; 7.95 GiB already allocated; 1.11 GiB free; 8.53 GiB reserved in total by PyTorch)\n",
      "error 14188 CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 10.76 GiB total capacity; 7.60 GiB already allocated; 169.69 MiB free; 9.47 GiB reserved in total by PyTorch)\n",
      "step 15000 loss 1.621678100603032\n",
      "step 16000 loss 1.6167106889640455\n",
      "step 17000 loss 1.610022509102348\n",
      "step 18000 loss 1.5817169323459432\n",
      "step 19000 loss 1.5722098078269628\n",
      "step 20000 loss 1.5679702238868234\n",
      "error 20750 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 6.73 GiB already allocated; 921.69 MiB free; 8.73 GiB reserved in total by PyTorch)\n",
      "step 21000 loss 1.5610694801590381\n",
      "step 22000 loss 1.561103833062269\n",
      "step 23000 loss 1.5497197078101173\n",
      "error 23499 CUDA out of memory. Tried to allocate 1.07 GiB (GPU 0; 10.76 GiB total capacity; 8.42 GiB already allocated; 563.69 MiB free; 9.08 GiB reserved in total by PyTorch)\n",
      "step 24000 loss 1.5374166345673048\n",
      "error 24079 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.83 GiB already allocated; 385.69 MiB free; 9.26 GiB reserved in total by PyTorch)\n",
      "step 25000 loss 1.5412099212469161\n",
      "step 26000 loss 1.5387520727699775\n",
      "step 27000 loss 1.5450489197441235\n",
      "step 28000 loss 1.5465633031182555\n",
      "error 28402 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.96 GiB already allocated; 739.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
      "step 29000 loss 1.548109695684887\n",
      "step 30000 loss 1.5392959037461993\n",
      "step 31000 loss 1.5214723890115276\n",
      "step 32000 loss 1.5197352038997345\n",
      "error 32651 CUDA out of memory. Tried to allocate 952.00 MiB (GPU 0; 10.76 GiB total capacity; 7.85 GiB already allocated; 831.69 MiB free; 8.82 GiB reserved in total by PyTorch)\n",
      "step 33000 loss 1.5191886800053331\n",
      "error 33936 CUDA out of memory. Tried to allocate 900.00 MiB (GPU 0; 10.76 GiB total capacity; 7.93 GiB already allocated; 839.69 MiB free; 8.81 GiB reserved in total by PyTorch)\n",
      "step 34000 loss 1.5071750278883136\n",
      "step 35000 loss 1.4951174692222116\n",
      "step 36000 loss 1.5110572869894487\n",
      "step 37000 loss 1.5035447392609362\n",
      "step 38000 loss 1.4937061133745018\n",
      "error 38324 CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 10.76 GiB total capacity; 7.70 GiB already allocated; 861.69 MiB free; 8.79 GiB reserved in total by PyTorch)\n",
      "step 39000 loss 1.5010554298354957\n",
      "step 40000 loss 1.486516372689772\n",
      "step 41000 loss 1.4813608206837234\n",
      "step 42000 loss 1.4922767825661034\n",
      "step 43000 loss 1.4789339267590702\n",
      "error 43139 CUDA out of memory. Tried to allocate 1.23 GiB (GPU 0; 10.76 GiB total capacity; 7.80 GiB already allocated; 863.69 MiB free; 8.79 GiB reserved in total by PyTorch)\n",
      "step 44000 loss 1.4735573651956568\n",
      "step 45000 loss 1.4621720642907539\n",
      "error 45376 CUDA out of memory. Tried to allocate 1020.00 MiB (GPU 0; 10.76 GiB total capacity; 8.14 GiB already allocated; 639.69 MiB free; 9.01 GiB reserved in total by PyTorch)\n",
      "step 46000 loss 1.4624380894905862\n",
      "error 46596 CUDA out of memory. Tried to allocate 1.05 GiB (GPU 0; 10.76 GiB total capacity; 8.68 GiB already allocated; 483.69 MiB free; 9.16 GiB reserved in total by PyTorch)\n",
      "step 47000 loss 1.4647909544826903\n",
      "step 48000 loss 1.4605912022122536\n",
      "step 49000 loss 1.4638088939337457\n",
      "error 49089 CUDA out of memory. Tried to allocate 1.26 GiB (GPU 0; 10.76 GiB total capacity; 7.60 GiB already allocated; 1.13 GiB free; 8.50 GiB reserved in total by PyTorch)\n",
      "step 50000 loss 1.4601541562835065\n",
      "step 51000 loss 1.4568530646179267\n",
      "step 52000 loss 1.4455715473337545\n",
      "error 52204 CUDA out of memory. Tried to allocate 808.00 MiB (GPU 0; 10.76 GiB total capacity; 7.39 GiB already allocated; 763.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "step 53000 loss 1.4385002881553164\n",
      "error 53650 CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 10.76 GiB total capacity; 8.31 GiB already allocated; 589.69 MiB free; 9.06 GiB reserved in total by PyTorch)\n",
      "step 54000 loss 1.4275163370868755\n",
      "error 54400 CUDA out of memory. Tried to allocate 1.35 GiB (GPU 0; 10.76 GiB total capacity; 8.82 GiB already allocated; 467.69 MiB free; 9.18 GiB reserved in total by PyTorch)\n",
      "step 55000 loss 1.4280296323270911\n",
      "error 55164 CUDA out of memory. Tried to allocate 716.00 MiB (GPU 0; 10.76 GiB total capacity; 8.21 GiB already allocated; 251.69 MiB free; 9.39 GiB reserved in total by PyTorch)\n",
      "step 56000 loss 1.415750276519965\n",
      "error 56218 CUDA out of memory. Tried to allocate 1.22 GiB (GPU 0; 10.76 GiB total capacity; 8.82 GiB already allocated; 3.69 MiB free; 9.63 GiB reserved in total by PyTorch)\n",
      "step 57000 loss 1.4304115232860126\n",
      "step 58000 loss 1.4350156803490897\n",
      "error 58438 CUDA out of memory. Tried to allocate 916.00 MiB (GPU 0; 10.76 GiB total capacity; 7.74 GiB already allocated; 803.69 MiB free; 8.85 GiB reserved in total by PyTorch)\n",
      "step 59000 loss 1.4369355844267218\n",
      "step 60000 loss 1.4283767094088695\n",
      "error 60054 CUDA out of memory. Tried to allocate 922.00 MiB (GPU 0; 10.76 GiB total capacity; 8.00 GiB already allocated; 721.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
      "step 61000 loss 1.429359077257854\n",
      "step 62000 loss 1.4302132939200987\n",
      "step 63000 loss 1.4428398229490416\n",
      "error 63918 CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 10.76 GiB total capacity; 8.59 GiB already allocated; 285.69 MiB free; 9.36 GiB reserved in total by PyTorch)\n",
      "step 64000 loss 1.4318759719514011\n",
      "error 64328 CUDA out of memory. Tried to allocate 1.11 GiB (GPU 0; 10.76 GiB total capacity; 8.42 GiB already allocated; 275.69 MiB free; 9.37 GiB reserved in total by PyTorch)\n",
      "step 65000 loss 1.4255519478190595\n",
      "step 66000 loss 1.4234853517876795\n",
      "error 66314 CUDA out of memory. Tried to allocate 1.34 GiB (GPU 0; 10.76 GiB total capacity; 7.77 GiB already allocated; 945.69 MiB free; 8.71 GiB reserved in total by PyTorch)\n",
      "step 67000 loss 1.413439031855459\n",
      "step 68000 loss 1.4148290818568556\n",
      "error 68903 CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 10.76 GiB total capacity; 7.89 GiB already allocated; 759.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "step 69000 loss 1.405174908258505\n",
      "step 70000 loss 1.4159819581703303\n",
      "error 70270 CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 10.76 GiB total capacity; 8.72 GiB already allocated; 557.69 MiB free; 9.09 GiB reserved in total by PyTorch)\n",
      "step 71000 loss 1.4142312050551875\n",
      "step 72000 loss 1.4098882850239693\n",
      "step 73000 loss 1.4093389173919988\n",
      "step 74000 loss 1.3966641162948248\n",
      "error 74559 CUDA out of memory. Tried to allocate 1.26 GiB (GPU 0; 10.76 GiB total capacity; 7.84 GiB already allocated; 1.09 GiB free; 8.54 GiB reserved in total by PyTorch)\n",
      "step 75000 loss 1.4124522088922455\n",
      "error 75267 CUDA out of memory. Tried to allocate 2.40 GiB (GPU 0; 10.76 GiB total capacity; 7.39 GiB already allocated; 2.17 GiB free; 7.47 GiB reserved in total by PyTorch)\n",
      "step 76000 loss 1.4068154301855185\n",
      "step 77000 loss 1.409533058917267\n",
      "step 78000 loss 1.4075952773595297\n",
      "error 78058 CUDA out of memory. Tried to allocate 1.15 GiB (GPU 0; 10.76 GiB total capacity; 8.07 GiB already allocated; 731.69 MiB free; 8.92 GiB reserved in total by PyTorch)\n",
      "step 79000 loss 1.400638904607081\n",
      "step 80000 loss 1.4066685893286581\n",
      "error 80173 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 8.45 GiB already allocated; 633.69 MiB free; 9.02 GiB reserved in total by PyTorch)\n",
      "step 81000 loss 1.4057836900763392\n",
      "error 81624 CUDA out of memory. Tried to allocate 1002.00 MiB (GPU 0; 10.76 GiB total capacity; 8.36 GiB already allocated; 719.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
      "step 82000 loss 1.385551296633067\n",
      "error 82182 CUDA out of memory. Tried to allocate 780.00 MiB (GPU 0; 10.76 GiB total capacity; 7.43 GiB already allocated; 395.69 MiB free; 9.25 GiB reserved in total by PyTorch)\n",
      "step 83000 loss 1.3909922026470212\n",
      "step 84000 loss 1.4009384998257617\n",
      "error 84126 CUDA out of memory. Tried to allocate 1.68 GiB (GPU 0; 10.76 GiB total capacity; 8.26 GiB already allocated; 1.05 GiB free; 8.58 GiB reserved in total by PyTorch)\n",
      "step 85000 loss 1.4118246969220813\n",
      "step 86000 loss 1.4109142263599457\n",
      "step 87000 loss 1.402157676788603\n",
      "error 87381 CUDA out of memory. Tried to allocate 448.00 MiB (GPU 0; 10.76 GiB total capacity; 8.63 GiB already allocated; 399.69 MiB free; 9.24 GiB reserved in total by PyTorch)\n",
      "step 88000 loss 1.40006970371994\n",
      "step 89000 loss 1.390521758636719\n",
      "error 89447 CUDA out of memory. Tried to allocate 894.00 MiB (GPU 0; 10.76 GiB total capacity; 7.91 GiB already allocated; 895.69 MiB free; 8.76 GiB reserved in total by PyTorch)\n",
      "step 90000 loss 1.3839341735154245\n",
      "step 91000 loss 1.3880099119437543\n",
      "error 91883 CUDA out of memory. Tried to allocate 980.00 MiB (GPU 0; 10.76 GiB total capacity; 8.04 GiB already allocated; 753.69 MiB free; 8.90 GiB reserved in total by PyTorch)\n",
      "step 92000 loss 1.3886318891892488\n",
      "step 93000 loss 1.390096048908005\n",
      "error 93368 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.83 GiB already allocated; 799.69 MiB free; 8.85 GiB reserved in total by PyTorch)\n",
      "step 94000 loss 1.3868484540830661\n",
      "error 94140 CUDA out of memory. Tried to allocate 3.47 GiB (GPU 0; 10.76 GiB total capacity; 8.44 GiB already allocated; 1.05 GiB free; 8.58 GiB reserved in total by PyTorch)\n",
      "step 95000 loss 1.3847126415979027\n",
      "error 95712 CUDA out of memory. Tried to allocate 1.14 GiB (GPU 0; 10.76 GiB total capacity; 9.29 GiB already allocated; 93.69 MiB free; 9.54 GiB reserved in total by PyTorch)\n",
      "step 96000 loss 1.376475900571145\n",
      "error 96919 CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 10.76 GiB total capacity; 7.81 GiB already allocated; 1.24 GiB free; 8.39 GiB reserved in total by PyTorch)\n",
      "step 97000 loss 1.3697450944932394\n",
      "step 98000 loss 1.3691839251840876\n",
      "step 99000 loss 1.3860356547859314\n",
      "step 100000 loss 1.3807216223047314\n",
      "error 100407 CUDA out of memory. Tried to allocate 830.00 MiB (GPU 0; 10.76 GiB total capacity; 8.21 GiB already allocated; 313.69 MiB free; 9.33 GiB reserved in total by PyTorch)\n",
      "step 101000 loss 1.3963250578672481\n",
      "error 101122 CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 10.76 GiB total capacity; 9.47 GiB already allocated; 25.69 MiB free; 9.61 GiB reserved in total by PyTorch)\n",
      "error 101248 CUDA out of memory. Tried to allocate 1008.00 MiB (GPU 0; 10.76 GiB total capacity; 8.19 GiB already allocated; 697.69 MiB free; 8.95 GiB reserved in total by PyTorch)\n",
      "step 102000 loss 1.381718494378525\n",
      "error 102179 CUDA out of memory. Tried to allocate 790.00 MiB (GPU 0; 10.76 GiB total capacity; 7.98 GiB already allocated; 767.69 MiB free; 8.88 GiB reserved in total by PyTorch)\n",
      "error 102662 CUDA out of memory. Tried to allocate 952.00 MiB (GPU 0; 10.76 GiB total capacity; 7.85 GiB already allocated; 847.69 MiB free; 8.81 GiB reserved in total by PyTorch)\n",
      "step 103000 loss 1.3867386296175699\n",
      "error 103432 CUDA out of memory. Tried to allocate 1.05 GiB (GPU 0; 10.76 GiB total capacity; 8.02 GiB already allocated; 615.69 MiB free; 9.03 GiB reserved in total by PyTorch)\n",
      "step 104000 loss 1.3747667828236734\n",
      "step 105000 loss 1.3757010894743196\n",
      "step 106000 loss 1.3627061974459742\n",
      "step 107000 loss 1.3761756791113087\n",
      "error 107415 CUDA out of memory. Tried to allocate 922.00 MiB (GPU 0; 10.76 GiB total capacity; 7.75 GiB already allocated; 857.69 MiB free; 8.80 GiB reserved in total by PyTorch)\n",
      "step 108000 loss 1.3648930810086939\n",
      "error 108346 CUDA out of memory. Tried to allocate 866.00 MiB (GPU 0; 10.76 GiB total capacity; 7.68 GiB already allocated; 843.69 MiB free; 8.81 GiB reserved in total by PyTorch)\n",
      "step 109000 loss 1.375110728236904\n",
      "error 109027 CUDA out of memory. Tried to allocate 1.11 GiB (GPU 0; 10.76 GiB total capacity; 8.41 GiB already allocated; 521.69 MiB free; 9.12 GiB reserved in total by PyTorch)\n",
      "step 110000 loss 1.375891402740335\n",
      "step 111000 loss 1.3806493404146039\n",
      "step 112000 loss 1.3648925215626757\n",
      "error 112882 CUDA out of memory. Tried to allocate 1.08 GiB (GPU 0; 10.76 GiB total capacity; 8.35 GiB already allocated; 1.04 GiB free; 8.60 GiB reserved in total by PyTorch)\n",
      "step 113000 loss 1.3668712401155252\n",
      "step 114000 loss 1.351714990858381\n",
      "step 115000 loss 1.3580255847234042\n",
      "step 116000 loss 1.3600642680345183\n",
      "step 117000 loss 1.3513963500297228\n",
      "step 118000 loss 1.3671170076103165\n",
      "error 118430 CUDA out of memory. Tried to allocate 1008.00 MiB (GPU 0; 10.76 GiB total capacity; 8.15 GiB already allocated; 715.69 MiB free; 8.94 GiB reserved in total by PyTorch)\n",
      "step 119000 loss 1.3609962514393041\n",
      "error 119075 CUDA out of memory. Tried to allocate 876.00 MiB (GPU 0; 10.76 GiB total capacity; 7.94 GiB already allocated; 839.69 MiB free; 8.81 GiB reserved in total by PyTorch)\n",
      "step 120000 loss 1.3540291630676364\n",
      "step 121000 loss 1.3638124138623196\n",
      "error 121274 CUDA out of memory. Tried to allocate 1.11 GiB (GPU 0; 10.76 GiB total capacity; 8.48 GiB already allocated; 849.69 MiB free; 8.80 GiB reserved in total by PyTorch)\n",
      "step 122000 loss 1.3662583619871882\n",
      "step 123000 loss 1.3456631593237443\n",
      "step 124000 loss 1.3446384864127559\n",
      "step 125000 loss 1.3523965853922022\n",
      "error 125568 CUDA out of memory. Tried to allocate 986.00 MiB (GPU 0; 10.76 GiB total capacity; 8.02 GiB already allocated; 795.69 MiB free; 8.86 GiB reserved in total by PyTorch)\n",
      "step 126000 loss 1.3513094280967\n",
      "step 127000 loss 1.3651944989943638\n",
      "error 127058 CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 10.76 GiB total capacity; 8.59 GiB already allocated; 629.69 MiB free; 9.02 GiB reserved in total by PyTorch)\n",
      "step 128000 loss 1.350642460967927\n",
      "step 129000 loss 1.339702469783254\n",
      "error 129066 CUDA out of memory. Tried to allocate 3.47 GiB (GPU 0; 10.76 GiB total capacity; 8.45 GiB already allocated; 575.69 MiB free; 9.07 GiB reserved in total by PyTorch)\n",
      "step 130000 loss 1.340435085508887\n",
      "step 131000 loss 1.351269869766291\n",
      "error 131548 CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 10.76 GiB total capacity; 7.91 GiB already allocated; 1.05 GiB free; 8.58 GiB reserved in total by PyTorch)\n",
      "step 132000 loss 1.344327490579696\n",
      "error 132342 CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 10.76 GiB total capacity; 8.31 GiB already allocated; 797.69 MiB free; 8.86 GiB reserved in total by PyTorch)\n",
      "step 133000 loss 1.3535759422240654\n",
      "step 134000 loss 1.358697595711307\n",
      "error 134799 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.33 GiB already allocated; 821.69 MiB free; 8.83 GiB reserved in total by PyTorch)\n",
      "step 135000 loss 1.3550793867665347\n",
      "step 136000 loss 1.3556051740578334\n",
      "error 136074 CUDA out of memory. Tried to allocate 1.85 GiB (GPU 0; 10.76 GiB total capacity; 8.64 GiB already allocated; 403.69 MiB free; 9.24 GiB reserved in total by PyTorch)\n",
      "error 136664 CUDA out of memory. Tried to allocate 1.35 GiB (GPU 0; 10.76 GiB total capacity; 8.90 GiB already allocated; 417.69 MiB free; 9.23 GiB reserved in total by PyTorch)\n",
      "step 137000 loss 1.3539432265586322\n",
      "error 137093 CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 10.76 GiB total capacity; 8.12 GiB already allocated; 933.69 MiB free; 8.72 GiB reserved in total by PyTorch)\n",
      "step 138000 loss 1.351645375457388\n",
      "error 138803 CUDA out of memory. Tried to allocate 992.00 MiB (GPU 0; 10.76 GiB total capacity; 8.73 GiB already allocated; 729.69 MiB free; 8.92 GiB reserved in total by PyTorch)\n",
      "step 139000 loss 1.3329083108055126\n",
      "error 139255 CUDA out of memory. Tried to allocate 876.00 MiB (GPU 0; 10.76 GiB total capacity; 7.91 GiB already allocated; 853.69 MiB free; 8.80 GiB reserved in total by PyTorch)\n",
      "step 140000 loss 1.328960430712879\n",
      "error 140486 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 6.78 GiB already allocated; 615.69 MiB free; 9.03 GiB reserved in total by PyTorch)\n",
      "step 141000 loss 1.3395015047754533\n",
      "step 142000 loss 1.3376708943471236\n",
      "step 143000 loss 1.3399167724838987\n",
      "error 143830 CUDA out of memory. Tried to allocate 956.00 MiB (GPU 0; 10.76 GiB total capacity; 7.84 GiB already allocated; 789.69 MiB free; 8.86 GiB reserved in total by PyTorch)\n",
      "step 144000 loss 1.3391197170874636\n",
      "error 144456 CUDA out of memory. Tried to allocate 1.24 GiB (GPU 0; 10.76 GiB total capacity; 7.72 GiB already allocated; 1.22 GiB free; 8.42 GiB reserved in total by PyTorch)\n",
      "step 145000 loss 1.3340307605203736\n",
      "step 146000 loss 1.3435670633954624\n",
      "error 146107 CUDA out of memory. Tried to allocate 980.00 MiB (GPU 0; 10.76 GiB total capacity; 8.27 GiB already allocated; 761.69 MiB free; 8.89 GiB reserved in total by PyTorch)\n",
      "error 146607 CUDA out of memory. Tried to allocate 946.00 MiB (GPU 0; 10.76 GiB total capacity; 7.90 GiB already allocated; 737.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
      "step 147000 loss 1.3415995513449999\n",
      "step 148000 loss 1.3384918063549696\n",
      "step 149000 loss 1.350963800868836\n",
      "step 150000 loss 1.3358103464593043\n",
      "error 150948 CUDA out of memory. Tried to allocate 786.00 MiB (GPU 0; 10.76 GiB total capacity; 7.64 GiB already allocated; 779.69 MiB free; 8.87 GiB reserved in total by PyTorch)\n",
      "step 151000 loss 1.334069401766121\n",
      "error 151810 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 6.54 GiB already allocated; 743.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
      "step 152000 loss 1.3442333004223217\n",
      "step 153000 loss 1.3272806209559203\n",
      "error 153828 CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 10.76 GiB total capacity; 7.82 GiB already allocated; 1.27 GiB free; 8.37 GiB reserved in total by PyTorch)\n",
      "step 154000 loss 1.3324173407753037\n",
      "error 154735 CUDA out of memory. Tried to allocate 1.08 GiB (GPU 0; 10.76 GiB total capacity; 8.92 GiB already allocated; 253.69 MiB free; 9.39 GiB reserved in total by PyTorch)\n",
      "step 155000 loss 1.3310733793326808\n",
      "step 156000 loss 1.3299070366686063\n",
      "step 157000 loss 1.3278618451018642\n",
      "error 157006 CUDA out of memory. Tried to allocate 900.00 MiB (GPU 0; 10.76 GiB total capacity; 7.92 GiB already allocated; 705.69 MiB free; 8.95 GiB reserved in total by PyTorch)\n",
      "step 158000 loss 1.3411352952373696\n",
      "error 158963 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 8.24 GiB already allocated; 735.69 MiB free; 8.92 GiB reserved in total by PyTorch)\n",
      "step 159000 loss 1.3254056563856131\n",
      "error 159942 CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 10.76 GiB total capacity; 7.72 GiB already allocated; 825.69 MiB free; 8.83 GiB reserved in total by PyTorch)\n",
      "step 160000 loss 1.3227620278713996\n",
      "step 161000 loss 1.3318189009700612\n",
      "step 162000 loss 1.34123620795669\n",
      "error 162576 CUDA out of memory. Tried to allocate 664.00 MiB (GPU 0; 10.76 GiB total capacity; 7.90 GiB already allocated; 583.69 MiB free; 9.06 GiB reserved in total by PyTorch)\n",
      "step 163000 loss 1.3384616679225427\n",
      "error 163160 CUDA out of memory. Tried to allocate 876.00 MiB (GPU 0; 10.76 GiB total capacity; 7.93 GiB already allocated; 819.69 MiB free; 8.83 GiB reserved in total by PyTorch)\n",
      "step 164000 loss 1.3167609319689726\n",
      "step 165000 loss 1.316144160657587\n",
      "error 165599 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 8.24 GiB already allocated; 745.69 MiB free; 8.91 GiB reserved in total by PyTorch)\n",
      "error 165854 CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 10.76 GiB total capacity; 9.40 GiB already allocated; 97.69 MiB free; 9.54 GiB reserved in total by PyTorch)\n",
      "step 166000 loss 1.319476455271136\n",
      "step 167000 loss 1.3287669541319378\n",
      "error 167744 CUDA out of memory. Tried to allocate 916.00 MiB (GPU 0; 10.76 GiB total capacity; 8.09 GiB already allocated; 815.69 MiB free; 8.84 GiB reserved in total by PyTorch)\n",
      "step 168000 loss 1.320705268067573\n",
      "step 169000 loss 1.3181178431390548\n",
      "step 170000 loss 1.3223915783077766\n",
      "error 170094 CUDA out of memory. Tried to allocate 992.00 MiB (GPU 0; 10.76 GiB total capacity; 8.74 GiB already allocated; 795.69 MiB free; 8.86 GiB reserved in total by PyTorch)\n",
      "step 171000 loss 1.3080522195015636\n",
      "step 172000 loss 1.3111654091495746\n",
      "error 172630 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.18 GiB already allocated; 949.69 MiB free; 8.71 GiB reserved in total by PyTorch)\n",
      "step 173000 loss 1.3377228920739324\n",
      "step 174000 loss 1.33661053732826\n",
      "error 174223 CUDA out of memory. Tried to allocate 1.02 GiB (GPU 0; 10.76 GiB total capacity; 8.28 GiB already allocated; 607.69 MiB free; 9.04 GiB reserved in total by PyTorch)\n",
      "error 174871 CUDA out of memory. Tried to allocate 866.00 MiB (GPU 0; 10.76 GiB total capacity; 7.59 GiB already allocated; 399.69 MiB free; 9.24 GiB reserved in total by PyTorch)\n",
      "step 175000 loss 1.3441622881373327\n",
      "error 175040 CUDA out of memory. Tried to allocate 1002.00 MiB (GPU 0; 10.76 GiB total capacity; 8.37 GiB already allocated; 731.69 MiB free; 8.92 GiB reserved in total by PyTorch)\n",
      "step 176000 loss 1.329836377286269\n",
      "step 177000 loss 1.3191058659188997\n",
      "step 178000 loss 1.3219125257458657\n",
      "error 178435 CUDA out of memory. Tried to allocate 1014.00 MiB (GPU 0; 10.76 GiB total capacity; 8.04 GiB already allocated; 681.69 MiB free; 8.97 GiB reserved in total by PyTorch)\n",
      "error 178816 CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 10.76 GiB total capacity; 8.61 GiB already allocated; 605.69 MiB free; 9.04 GiB reserved in total by PyTorch)\n",
      "step 179000 loss 1.3140343884660775\n",
      "step 180000 loss 1.3227725058951971\n",
      "step 181000 loss 1.3192630414527942\n",
      "step 182000 loss 1.3155189349693552\n",
      "step 183000 loss 1.314023183256946\n",
      "step 184000 loss 1.3261253488463896\n",
      "step 185000 loss 1.321916067652572\n",
      "error 185026 CUDA out of memory. Tried to allocate 916.00 MiB (GPU 0; 10.76 GiB total capacity; 8.15 GiB already allocated; 813.69 MiB free; 8.84 GiB reserved in total by PyTorch)\n",
      "step 186000 loss 1.3146131114423225\n",
      "error 186071 CUDA out of memory. Tried to allocate 1.05 GiB (GPU 0; 10.76 GiB total capacity; 8.61 GiB already allocated; 457.69 MiB free; 9.19 GiB reserved in total by PyTorch)\n",
      "step 187000 loss 1.3158746480019954\n",
      "error 187154 CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 10.76 GiB total capacity; 8.61 GiB already allocated; 917.69 MiB free; 8.74 GiB reserved in total by PyTorch)\n",
      "step 188000 loss 1.309435959482614\n",
      "error 188656 CUDA out of memory. Tried to allocate 1.21 GiB (GPU 0; 10.76 GiB total capacity; 8.43 GiB already allocated; 527.69 MiB free; 9.12 GiB reserved in total by PyTorch)\n",
      "step 189000 loss 1.3132142503440258\n",
      "step 190000 loss 1.3130569536308465\n",
      "step 191000 loss 1.3133588763676434\n",
      "step 192000 loss 1.3149334933265298\n",
      "step 193000 loss 1.3100141813526676\n",
      "error 193039 CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 10.76 GiB total capacity; 7.87 GiB already allocated; 789.69 MiB free; 8.86 GiB reserved in total by PyTorch)\n",
      "error 193190 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 8.09 GiB already allocated; 785.69 MiB free; 8.87 GiB reserved in total by PyTorch)\n",
      "error 193215 CUDA out of memory. Tried to allocate 1.06 GiB (GPU 0; 10.76 GiB total capacity; 8.25 GiB already allocated; 399.69 MiB free; 9.24 GiB reserved in total by PyTorch)\n",
      "error 193574 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 7.89 GiB already allocated; 107.69 MiB free; 9.53 GiB reserved in total by PyTorch)\n",
      "error 193943 CUDA out of memory. Tried to allocate 940.00 MiB (GPU 0; 10.76 GiB total capacity; 8.17 GiB already allocated; 749.69 MiB free; 8.90 GiB reserved in total by PyTorch)\n",
      "step 194000 loss 1.3317128545010182\n",
      "error 194254 CUDA out of memory. Tried to allocate 860.00 MiB (GPU 0; 10.76 GiB total capacity; 7.80 GiB already allocated; 773.69 MiB free; 8.88 GiB reserved in total by PyTorch)\n",
      "step 195000 loss 1.3224027818585606\n",
      "step 196000 loss 1.309476686901512\n",
      "error 196363 CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 10.76 GiB total capacity; 8.60 GiB already allocated; 265.69 MiB free; 9.38 GiB reserved in total by PyTorch)\n",
      "step 197000 loss 1.310167286517061\n",
      "error 197968 CUDA out of memory. Tried to allocate 934.00 MiB (GPU 0; 10.76 GiB total capacity; 8.17 GiB already allocated; 651.69 MiB free; 9.00 GiB reserved in total by PyTorch)\n",
      "step 198000 loss 1.3082282657392057\n",
      "step 199000 loss 1.3063352371723151\n",
      "step 200000 loss 1.3153543664657465\n",
      "error 200692 CUDA out of memory. Tried to allocate 854.00 MiB (GPU 0; 10.76 GiB total capacity; 8.12 GiB already allocated; 555.69 MiB free; 9.09 GiB reserved in total by PyTorch)\n",
      "step 201000 loss 1.2990609985683863\n",
      "error 201786 CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 10.76 GiB total capacity; 8.07 GiB already allocated; 701.69 MiB free; 8.95 GiB reserved in total by PyTorch)\n",
      "step 202000 loss 1.3213456139707835\n",
      "step 203000 loss 1.3117750872213911\n",
      "step 204000 loss 1.3061624213358713\n",
      "step 205000 loss 1.3067514004474508\n",
      "step 206000 loss 1.298021090947502\n",
      "step 207000 loss 1.3050726174984952\n",
      "step 208000 loss 1.2940952094890747\n",
      "error 208516 CUDA out of memory. Tried to allocate 1.21 GiB (GPU 0; 10.76 GiB total capacity; 8.16 GiB already allocated; 1.20 GiB free; 8.43 GiB reserved in total by PyTorch)\n",
      "step 209000 loss 1.3060495546278659\n",
      "step 210000 loss 1.294749523387651\n",
      "error 210094 CUDA out of memory. Tried to allocate 992.00 MiB (GPU 0; 10.76 GiB total capacity; 8.03 GiB already allocated; 751.69 MiB free; 8.90 GiB reserved in total by PyTorch)\n",
      "step 211000 loss 1.2909695474597749\n",
      "error 211800 CUDA out of memory. Tried to allocate 1.45 GiB (GPU 0; 10.76 GiB total capacity; 7.81 GiB already allocated; 1.12 GiB free; 8.51 GiB reserved in total by PyTorch)\n",
      "step 212000 loss 1.3036224128995029\n",
      "error 212846 CUDA out of memory. Tried to allocate 1.01 GiB (GPU 0; 10.76 GiB total capacity; 8.41 GiB already allocated; 581.69 MiB free; 9.07 GiB reserved in total by PyTorch)\n",
      "step 213000 loss 1.3004102189749753\n",
      "error 213195 CUDA out of memory. Tried to allocate 992.00 MiB (GPU 0; 10.76 GiB total capacity; 8.74 GiB already allocated; 233.69 MiB free; 9.41 GiB reserved in total by PyTorch)\n",
      "step 214000 loss 1.2888341406709947\n",
      "error 214316 CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 10.76 GiB total capacity; 7.87 GiB already allocated; 811.69 MiB free; 8.84 GiB reserved in total by PyTorch)\n",
      "step 215000 loss 1.2888423265367135\n",
      "step 216000 loss 1.293618114140688\n",
      "step 217000 loss 1.286390335067559\n",
      "step 218000 loss 1.305842937402965\n",
      "step 219000 loss 1.3031337922162076\n",
      "step 220000 loss 1.3037511910869009\n",
      "step 221000 loss 1.2849795153547088\n",
      "error 221124 CUDA out of memory. Tried to allocate 1.20 GiB (GPU 0; 10.76 GiB total capacity; 7.88 GiB already allocated; 811.69 MiB free; 8.84 GiB reserved in total by PyTorch)\n",
      "error 221167 CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 10.76 GiB total capacity; 7.68 GiB already allocated; 721.69 MiB free; 8.93 GiB reserved in total by PyTorch)\n",
      "step 222000 loss 1.3000277144312373\n",
      "step 223000 loss 1.2981221889389567\n",
      "error 223179 CUDA out of memory. Tried to allocate 1.03 GiB (GPU 0; 10.76 GiB total capacity; 8.17 GiB already allocated; 581.69 MiB free; 9.07 GiB reserved in total by PyTorch)\n",
      "step 224000 loss 1.292322057781449\n",
      "step 225000 loss 1.3024142567633405\n",
      "step 226000 loss 1.2926525958200987\n",
      "step 227000 loss 1.284270309031029\n",
      "step 228000 loss 1.2844790143156553\n",
      "step 229000 loss 1.2826334835374635\n",
      "step 230000 loss 1.2884224988645745\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "tq = trange(0 , 1_000_000)\n",
    "cleanup()\n",
    "\n",
    "for i in tq:\n",
    "    x, y = get_batch(batch_size)\n",
    "    xpad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in x])\n",
    "    ypad = tokenizer.pad([{k: v[0] for k, v in item.items()} for item in y])\n",
    "    \n",
    "    try:\n",
    "        labels = torch.tensor(ypad['input_ids'], device=model.device)\n",
    "        labels[labels==tokenizer.pad_token_id] = -100\n",
    "        loss = model(\n",
    "            input_ids=torch.tensor(xpad['input_ids'], device=model.device),\n",
    "            attention_mask=torch.tensor(xpad['attention_mask'], device=model.device),\n",
    "            labels=labels,\n",
    "        ).loss\n",
    "        loss.backward()\n",
    "        if i % gradient_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "    except RuntimeError as e:\n",
    "        print('error', i, e)\n",
    "        loss = None\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        cleanup()\n",
    "        continue\n",
    "\n",
    "    w = 1 / max(1, min(i, window))\n",
    "    ewm_loss = ewm_loss * (1-w) + loss.item() * w\n",
    "    tq.set_description(f'{ewm_loss:3.4f}')\n",
    "\n",
    "    if i > 0 and i % report_steps == 0:\n",
    "        print('step', i, 'loss', ewm_loss, )\n",
    "        if i > 0:\n",
    "            model.save_pretrained(save_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "    if i % cleanup_steps == 0:\n",
    "        cleanup()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999999 1.0905373548772155\n"
     ]
    }
   ],
   "source": [
    "print(i, ewm_loss)\n",
    "# step 141000 loss 0.9336458999525007\n",
    "# 598943 0.7913156676625156\n",
    "# 999999 0.7528545096685079"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "```\n",
    "step 1000 loss 7.8495250988006555\n",
    "step 2000 loss 6.971980854110847\n",
    "step 3000 loss 4.503658014299016\n",
    "step 4000 loss 3.0397753827503657\n",
    "step 5000 loss 2.328842712437607\n",
    "step 6000 loss 2.0246113876562606\n",
    "step 7000 loss 1.8595813628118134\n",
    "step 8000 loss 1.7656280765961248\n",
    "step 9000 loss 1.7232480028436197\n",
    "step 10000 loss 1.7234741253498884\n",
    "step 11000 loss 1.6784266652217328\n",
    "step 12000 loss 1.6633502409092902\n",
    "step 13000 loss 1.6524237591194655\n",
    "step 14000 loss 1.6367858148524403\n",
    "step 15000 loss 1.621678100603032\n",
    "step 16000 loss 1.6167106889640455\n",
    "step 17000 loss 1.610022509102348\n",
    "step 18000 loss 1.5817169323459432\n",
    "step 19000 loss 1.5722098078269628\n",
    "step 20000 loss 1.5679702238868234\n",
    "step 21000 loss 1.5610694801590381\n",
    "step 22000 loss 1.561103833062269\n",
    "step 23000 loss 1.5497197078101173\n",
    "step 24000 loss 1.5374166345673048\n",
    "step 25000 loss 1.5412099212469161\n",
    "step 26000 loss 1.5387520727699775\n",
    "step 27000 loss 1.5450489197441235\n",
    "step 28000 loss 1.5465633031182555\n",
    "step 29000 loss 1.548109695684887\n",
    "step 30000 loss 1.5392959037461993\n",
    "step 31000 loss 1.5214723890115276\n",
    "step 32000 loss 1.5197352038997345\n",
    "step 33000 loss 1.5191886800053331\n",
    "step 34000 loss 1.5071750278883136\n",
    "step 35000 loss 1.4951174692222116\n",
    "step 36000 loss 1.5110572869894487\n",
    "step 37000 loss 1.5035447392609362\n",
    "step 38000 loss 1.4937061133745018\n",
    "step 39000 loss 1.5010554298354957\n",
    "step 40000 loss 1.486516372689772\n",
    "step 41000 loss 1.4813608206837234\n",
    "step 42000 loss 1.4922767825661034\n",
    "step 43000 loss 1.4789339267590702\n",
    "step 44000 loss 1.4735573651956568\n",
    "step 45000 loss 1.4621720642907539\n",
    "step 46000 loss 1.4624380894905862\n",
    "step 47000 loss 1.4647909544826903\n",
    "step 48000 loss 1.4605912022122536\n",
    "step 49000 loss 1.4638088939337457\n",
    "step 50000 loss 1.4601541562835065\n",
    "step 51000 loss 1.4568530646179267\n",
    "step 52000 loss 1.4455715473337545\n",
    "step 53000 loss 1.4385002881553164\n",
    "step 54000 loss 1.4275163370868755\n",
    "step 55000 loss 1.4280296323270911\n",
    "step 56000 loss 1.415750276519965\n",
    "step 57000 loss 1.4304115232860126\n",
    "step 58000 loss 1.4350156803490897\n",
    "step 59000 loss 1.4369355844267218\n",
    "step 60000 loss 1.4283767094088695\n",
    "step 61000 loss 1.429359077257854\n",
    "step 62000 loss 1.4302132939200987\n",
    "step 63000 loss 1.4428398229490416\n",
    "step 64000 loss 1.4318759719514011\n",
    "step 65000 loss 1.4255519478190595\n",
    "step 66000 loss 1.4234853517876795\n",
    "step 67000 loss 1.413439031855459\n",
    "step 68000 loss 1.4148290818568556\n",
    "step 69000 loss 1.405174908258505\n",
    "step 70000 loss 1.4159819581703303\n",
    "step 71000 loss 1.4142312050551875\n",
    "step 72000 loss 1.4098882850239693\n",
    "step 73000 loss 1.4093389173919988\n",
    "step 74000 loss 1.3966641162948248\n",
    "step 75000 loss 1.4124522088922455\n",
    "step 76000 loss 1.4068154301855185\n",
    "step 77000 loss 1.409533058917267\n",
    "step 78000 loss 1.4075952773595297\n",
    "step 79000 loss 1.400638904607081\n",
    "step 80000 loss 1.4066685893286581\n",
    "step 81000 loss 1.4057836900763392\n",
    "step 82000 loss 1.385551296633067\n",
    "step 83000 loss 1.3909922026470212\n",
    "step 84000 loss 1.4009384998257617\n",
    "step 85000 loss 1.4118246969220813\n",
    "step 86000 loss 1.4109142263599457\n",
    "step 87000 loss 1.402157676788603\n",
    "step 88000 loss 1.40006970371994\n",
    "step 89000 loss 1.390521758636719\n",
    "step 90000 loss 1.3839341735154245\n",
    "step 91000 loss 1.3880099119437543\n",
    "step 92000 loss 1.3886318891892488\n",
    "step 93000 loss 1.390096048908005\n",
    "step 94000 loss 1.3868484540830661\n",
    "step 95000 loss 1.3847126415979027\n",
    "step 96000 loss 1.376475900571145\n",
    "step 97000 loss 1.3697450944932394\n",
    "step 98000 loss 1.3691839251840876\n",
    "step 99000 loss 1.3860356547859314\n",
    "step 100000 loss 1.3807216223047314\n",
    "step 101000 loss 1.3963250578672481\n",
    "step 102000 loss 1.381718494378525\n",
    "step 103000 loss 1.3867386296175699\n",
    "step 104000 loss 1.3747667828236734\n",
    "step 105000 loss 1.3757010894743196\n",
    "step 106000 loss 1.3627061974459742\n",
    "step 107000 loss 1.3761756791113087\n",
    "step 108000 loss 1.3648930810086939\n",
    "step 109000 loss 1.375110728236904\n",
    "step 110000 loss 1.375891402740335\n",
    "step 111000 loss 1.3806493404146039\n",
    "step 112000 loss 1.3648925215626757\n",
    "step 113000 loss 1.3668712401155252\n",
    "step 114000 loss 1.351714990858381\n",
    "step 115000 loss 1.3580255847234042\n",
    "step 116000 loss 1.3600642680345183\n",
    "step 117000 loss 1.3513963500297228\n",
    "step 118000 loss 1.3671170076103165\n",
    "step 119000 loss 1.3609962514393041\n",
    "step 120000 loss 1.3540291630676364\n",
    "step 121000 loss 1.3638124138623196\n",
    "step 122000 loss 1.3662583619871882\n",
    "step 123000 loss 1.3456631593237443\n",
    "step 124000 loss 1.3446384864127559\n",
    "step 125000 loss 1.3523965853922022\n",
    "step 126000 loss 1.3513094280967\n",
    "step 127000 loss 1.3651944989943638\n",
    "step 128000 loss 1.350642460967927\n",
    "step 129000 loss 1.339702469783254\n",
    "step 130000 loss 1.340435085508887\n",
    "step 131000 loss 1.351269869766291\n",
    "step 132000 loss 1.344327490579696\n",
    "step 133000 loss 1.3535759422240654\n",
    "step 134000 loss 1.358697595711307\n",
    "step 135000 loss 1.3550793867665347\n",
    "step 136000 loss 1.3556051740578334\n",
    "step 137000 loss 1.3539432265586322\n",
    "step 138000 loss 1.351645375457388\n",
    "step 139000 loss 1.3329083108055126\n",
    "step 140000 loss 1.328960430712879\n",
    "step 141000 loss 1.3395015047754533\n",
    "step 142000 loss 1.3376708943471236\n",
    "step 143000 loss 1.3399167724838987\n",
    "step 144000 loss 1.3391197170874636\n",
    "step 145000 loss 1.3340307605203736\n",
    "step 146000 loss 1.3435670633954624\n",
    "step 147000 loss 1.3415995513449999\n",
    "step 148000 loss 1.3384918063549696\n",
    "step 149000 loss 1.350963800868836\n",
    "step 150000 loss 1.3358103464593043\n",
    "step 151000 loss 1.334069401766121\n",
    "step 152000 loss 1.3442333004223217\n",
    "step 153000 loss 1.3272806209559203\n",
    "step 154000 loss 1.3324173407753037\n",
    "step 155000 loss 1.3310733793326808\n",
    "step 156000 loss 1.3299070366686063\n",
    "step 157000 loss 1.3278618451018642\n",
    "step 158000 loss 1.3411352952373696\n",
    "step 159000 loss 1.3254056563856131\n",
    "step 160000 loss 1.3227620278713996\n",
    "step 161000 loss 1.3318189009700612\n",
    "step 162000 loss 1.34123620795669\n",
    "step 163000 loss 1.3384616679225427\n",
    "step 164000 loss 1.3167609319689726\n",
    "step 165000 loss 1.316144160657587\n",
    "step 166000 loss 1.319476455271136\n",
    "step 167000 loss 1.3287669541319378\n",
    "step 168000 loss 1.320705268067573\n",
    "step 169000 loss 1.3181178431390548\n",
    "step 170000 loss 1.3223915783077766\n",
    "step 171000 loss 1.3080522195015636\n",
    "step 172000 loss 1.3111654091495746\n",
    "step 173000 loss 1.3377228920739324\n",
    "step 174000 loss 1.33661053732826\n",
    "step 175000 loss 1.3441622881373327\n",
    "step 176000 loss 1.329836377286269\n",
    "step 177000 loss 1.3191058659188997\n",
    "step 178000 loss 1.3219125257458657\n",
    "step 179000 loss 1.3140343884660775\n",
    "step 180000 loss 1.3227725058951971\n",
    "step 181000 loss 1.3192630414527942\n",
    "step 182000 loss 1.3155189349693552\n",
    "step 183000 loss 1.314023183256946\n",
    "step 184000 loss 1.3261253488463896\n",
    "step 185000 loss 1.321916067652572\n",
    "step 186000 loss 1.3146131114423225\n",
    "step 187000 loss 1.3158746480019954\n",
    "step 188000 loss 1.309435959482614\n",
    "step 189000 loss 1.3132142503440258\n",
    "step 190000 loss 1.3130569536308465\n",
    "step 191000 loss 1.3133588763676434\n",
    "step 192000 loss 1.3149334933265298\n",
    "step 193000 loss 1.3100141813526676\n",
    "step 194000 loss 1.3317128545010182\n",
    "step 195000 loss 1.3224027818585606\n",
    "step 196000 loss 1.309476686901512\n",
    "step 197000 loss 1.310167286517061\n",
    "step 198000 loss 1.3082282657392057\n",
    "step 199000 loss 1.3063352371723151\n",
    "step 200000 loss 1.3153543664657465\n",
    "step 201000 loss 1.2990609985683863\n",
    "step 202000 loss 1.3213456139707835\n",
    "step 203000 loss 1.3117750872213911\n",
    "step 204000 loss 1.3061624213358713\n",
    "step 205000 loss 1.3067514004474508\n",
    "step 206000 loss 1.298021090947502\n",
    "step 207000 loss 1.3050726174984952\n",
    "step 208000 loss 1.2940952094890747\n",
    "step 209000 loss 1.3060495546278659\n",
    "step 210000 loss 1.294749523387651\n",
    "step 211000 loss 1.2909695474597749\n",
    "step 212000 loss 1.3036224128995029\n",
    "step 213000 loss 1.3004102189749753\n",
    "step 214000 loss 1.2888341406709947\n",
    "step 215000 loss 1.2888423265367135\n",
    "step 216000 loss 1.293618114140688\n",
    "step 217000 loss 1.286390335067559\n",
    "step 218000 loss 1.305842937402965\n",
    "step 219000 loss 1.3031337922162076\n",
    "step 220000 loss 1.3037511910869009\n",
    "step 221000 loss 1.2849795153547088\n",
    "step 222000 loss 1.3000277144312373\n",
    "step 223000 loss 1.2981221889389567\n",
    "step 224000 loss 1.292322057781449\n",
    "step 225000 loss 1.3024142567633405\n",
    "step 226000 loss 1.2926525958200987\n",
    "step 227000 loss 1.284270309031029\n",
    "step 228000 loss 1.2844790143156553\n",
    "step 229000 loss 1.2826334835374635\n",
    "step 230000 loss 1.2884224988645745\n",
    "...\n",
    "999999 1.0905373548772155\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang = 'ru_RU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang = 'en_XX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [250004, 146038, 2], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('привет')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250004"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('en_XX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(\n",
    "    text, model, tokenizer, \n",
    "    n=None, \n",
    "    max_length=\"auto\", \n",
    "    beams=5,\n",
    "    src_lang='en_XX',\n",
    "    tgt_lang='en_XX',\n",
    "    **kwargs\n",
    "):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    tokenizer.tgt_lang = tgt_lang\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    tokenizer.tgt_lang\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    if max_length == \"auto\":\n",
    "        max_length = inputs.shape[1] + 10\n",
    "\n",
    "    result = model.generate(\n",
    "        inputs,\n",
    "        num_return_sequences=n or 1,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        repetition_penalty=10.0,\n",
    "        max_length=max_length,\n",
    "        min_length=int(0.5 * max_length),\n",
    "        num_beams=beams,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd like to play with my nice dog.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('paraphrase: I like to play with my nice dog.', model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я люблю играть со своей миленькой собакой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: I like to play with my nice dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я ненавижу играть со своей чёртовой собакой.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate playing with my motherfucking dog, man.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('translate: Ненавижу играть со своей долбаной собакой.', model, tokenizer, src_lang='ru_RU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't want to play with my dog.\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase('paraphrase: I hate to play with my fucking dog.', model, tokenizer, tgt_lang='en_XX'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a detoxifier adapter on top of the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "\n",
    "def paraphrase_adapt(\n",
    "    text, model, tokenizer, adapter,\n",
    "    n=None, \n",
    "    max_length=\"auto\", \n",
    "    beams=5,\n",
    "    src_lang='en',\n",
    "    tgt_lang='en',\n",
    "    **kwargs\n",
    "):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    tokenizer.tgt_lang = tgt_lang\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(model.device)\n",
    "\n",
    "    if max_length == \"auto\":\n",
    "        max_length = inputs.shape[1] + 10\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        encoded = model.model.encoder(inputs)\n",
    "        adapted = BaseModelOutput(last_hidden_state=adapter(encoded.last_hidden_state))\n",
    "\n",
    "    result = model.generate(\n",
    "        encoder_outputs=adapted,\n",
    "        num_return_sequences=n or 1,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        repetition_penalty=1.0,\n",
    "        max_length=max_length,\n",
    "        #min_length=int(0.5 * max_length),\n",
    "        num_beams=beams,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/home/dale/models/detox-parallel/bart-multitask-pretrain-invariant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/russian_data/test.tsv', sep='\\t')\n",
    "test_inputs_ru = test_data[\"toxic_comment\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/english_data/test_toxic_parallel.txt', 'r') as f:\n",
    "    test_inputs_en = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adapter(torch.nn.Module):\n",
    "    def __init__(self, model_dim=1024, ffn_dim=4096):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.ffn_dim = ffn_dim\n",
    "        # self.delta = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.delta = torch.nn.Sequential(\n",
    "            torch.nn.Linear(model_dim, ffn_dim, bias=True),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(ffn_dim, model_dim, bias=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.delta(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## en->ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "detox_en2ru = pd.read_csv('detox_en2ru_yandex.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17789, 15) (1977, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, dev = train_test_split(detox_en2ru, test_size=0.1, random_state=1)\n",
    "print(train.shape, dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adapter(\n",
       "  (delta): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter = Adapter()\n",
    "adapter.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adapter(\n",
       "  (delta): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "adapter.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(adapter.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb635f1c2b44eab87629e9d927d2310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6552413702011108\n",
      "1.0699604988098144\n",
      "0.9457136896252633\n",
      "0.9134821289777756\n",
      "0.9283235603570938\n",
      "0.9031834518909454\n",
      "0.881840660572052\n",
      "0.8903419297933578\n",
      "0.8470703360438346\n",
      "0.8436727476119995\n",
      "0.8622586065530777\n",
      "0.8464151296019554\n",
      "0.7979262927174569\n",
      "0.8425427156686783\n",
      "0.8484166666865349\n",
      "0.8185198873281478\n",
      "0.8075836738944053\n",
      "0.8029882007837296\n",
      "0.8022268304228782\n",
      "0.7951979848742485\n",
      "0.8026005041599273\n",
      "0.7974074131250382\n",
      "0.7784970554709435\n",
      "0.8007240790128708\n",
      "0.7517123523354531\n",
      "0.7912288221716881\n",
      "0.7903843292593956\n",
      "0.7914382806420326\n",
      "0.7596189120411873\n",
      "0.7278305983543396\n",
      "0.7563984030485154\n",
      "0.7332312130928039\n",
      "0.763190213739872\n",
      "0.7413530832529068\n",
      "0.7073853594064713\n",
      "0.7406839892268181\n",
      "0.7398436322808266\n",
      "0.7264939627051353\n",
      "0.7382057321071624\n",
      "0.7121048700809479\n",
      "0.7396052488684655\n",
      "0.7141054171323776\n",
      "0.7169818970561027\n",
      "0.6934740725159645\n",
      "0.7164686095714569\n",
      "0.6986848366260529\n",
      "0.701336784362793\n",
      "0.7051332432031632\n",
      "0.6789515882730484\n",
      "0.6860926705598831\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for i in trange(5_000):\n",
    "    text_batch = train.sample(16)\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    batch_in = tokenizer(text_batch.toxic_comment.tolist(), return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    batch_out = tokenizer(text_batch.neutral_comment.tolist(), return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    batch_out.input_ids[batch_out.input_ids==tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded = model.model.encoder(**batch_in)\n",
    "    transformed = adapter(encoded.last_hidden_state)\n",
    "\n",
    "    total_out = model(\n",
    "        encoder_outputs=[transformed], \n",
    "        attention_mask=batch_in.attention_mask,\n",
    "        decoder_attention_mask=batch_out.attention_mask,\n",
    "        labels=batch_out.input_ids,\n",
    "    )\n",
    "    total_out.loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(adapter.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    losses.append(total_out.loss.item())\n",
    "    if i % 100 == 0:\n",
    "        print(np.mean(losses[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "adapter.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awwww , thanks baba ! thank fuck it 's over '\n",
      "awwww , thanks baba ! thank it 's over '\n",
      "awwww, thanks baba! thank it's over\n"
     ]
    }
   ],
   "source": [
    "row = detox_en2ru.sample(1).iloc[0]\n",
    "text = row.toxic_comment\n",
    "print(text)\n",
    "print(row.neutral_comment)\n",
    "print(paraphrase_adapt(text, model, tokenizer, adapter, src_lang='en_XX', tgt_lang='en_XX', beams=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5014b0781514404f95cc7e00f547f9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs_ru = [paraphrase_adapt(text, model, tokenizer, adapter, src_lang='ru_RU', tgt_lang='ru_RU') for text in tqdm(test_inputs_ru)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366d8b303dbe4c4dbfb0b0459fbb0b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs_en = [paraphrase_adapt(text, model, tokenizer, adapter, src_lang='en_XX', tgt_lang='en_XX') for text in tqdm(test_inputs_en)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/enc_adapter_mbart_invariante_v1_en2ru/' \n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'results_ru.txt', 'w') as f:\n",
    "    for line in test_outputs_ru:\n",
    "        f.write(line + '\\n')\n",
    "with open(path + 'results_en.txt', 'w') as f:\n",
    "    for line in test_outputs_en:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(adapter.state_dict(), '/home/dale/models/detox-parallel/bart-multitask-pretrain-invariant/enc_adapter_mbart_invariante_v1_en2ru')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /home/dale/projects/multilingual_detox\n",
    "python evaluate_ru.py \\\n",
    "    --result_filename scores \\\n",
    "    --input_dir results/enc_adapter_mbart_invariante_v1_en2ru\\\n",
    "    --output_dir results\n",
    "\n",
    "Style accuracy:       0.6464164853096008\n",
    "Meaning preservation: 0.863937497138977\n",
    "Joint fluency:        -0.15687231719493866\n",
    "Joint score:          -0.08820652961730957\n",
    "Scores after calibration:\n",
    "Style accuracy:       0.681774914264679\n",
    "Meaning preservation: 0.7961680889129639\n",
    "Joint fluency:        0.8195968866348267\n",
    "Joint score:          0.4360015392303467\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /home/dale/projects/paradetox2/evaluation_detox\n",
    "python metric.py --inputs /home/dale/projects/multilingual_detox/data/english_data/test_toxic_parallel.txt \\\n",
    "    --preds /home/dale/projects/multilingual_detox/results/enc_adapter_mbart_invariante_v1_en2ru/results_en.txt \\\n",
    "    --cola_classifier_path /home/dale/models/cola_classifier_fairseq \\\n",
    "    --wieting_model_path /home/dale/models/wieting_similarity/sim.pt \\\n",
    "    --wieting_tokenizer_path /home/dale/models/wieting_similarity/sim.sp.30k.model \\\n",
    "    --batch_size 32\n",
    "cat results.md\n",
    "```\n",
    "\n",
    "| Model | ACC | EMB_SIM | SIM | CharPPL | TokenPPL | FL | GM | J | BLEU |\n",
    "| ----- | --- | ------- | --- | ------- | -------- | -- | -- | - | ---- |\n",
    "results_en.txt|0.8644|0.8945|0.8631|6.6233|222.5190|0.8495|10.6144|0.6213|0.7151|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ru -> en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    5058\n",
       "dev      1000\n",
       "test     1000\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_ru2en = pd.read_csv('detox_ru2en_yandex.tsv', sep='\\t')\n",
    "detox_ru2en.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ru, dev_ru = detox_ru2en[detox_ru2en.split=='train'], detox_ru2en[detox_ru2en.split=='dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = Adapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adapter(\n",
       "  (delta): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter.cuda()\n",
    "model.train()\n",
    "adapter.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(adapter.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250004, 250021)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('en_XX'), tokenizer.convert_tokens_to_ids('ru_RU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_in, batch_out, encoded, transformed, total_out = None, None, None, None, None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b22e9a1b3b849188eabb64a00c89f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.068622589111328\n",
      "1.3378848874568938\n",
      "1.2991722577810287\n",
      "1.2289144092798232\n",
      "1.2466943675279618\n",
      "1.1212247076630593\n",
      "1.1497117337584495\n",
      "1.067726196050644\n",
      "1.169718002974987\n",
      "1.086239886879921\n",
      "1.1233404409885406\n",
      "1.071696414500475\n",
      "1.025642173886299\n",
      "1.0709281569719316\n",
      "1.0384460037946701\n",
      "1.002386423945427\n",
      "0.9811897620558738\n",
      "0.9852833244204521\n",
      "0.9410369583964348\n",
      "0.9337509196996688\n",
      "0.9226903989911079\n",
      "0.9151546737551689\n",
      "0.8957649406790733\n",
      "0.9339609095454215\n",
      "0.8964924520254135\n",
      "0.8888657385110855\n",
      "0.8600492045283318\n",
      "0.8435651037096977\n",
      "0.8476184010505676\n",
      "0.8520551937818527\n",
      "0.8466779348254204\n",
      "0.8357182237505912\n",
      "0.8519056951999664\n",
      "0.8565433916449546\n",
      "0.8138781163096428\n",
      "0.8407841888070107\n",
      "0.8198639583587647\n",
      "0.8384282004833221\n",
      "0.8114404201507568\n",
      "0.7829724657535553\n",
      "0.8283802568912506\n",
      "0.7674878004193306\n",
      "0.7641333591938019\n",
      "0.7590398514270782\n",
      "0.7427831056714058\n",
      "0.7247042085230351\n",
      "0.7783362740278243\n",
      "0.7672648370265961\n",
      "0.7224531561136246\n",
      "0.71433149933815\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for i in trange(5_000):\n",
    "    text_batch = train_ru.sample(8)  # 16 seems too much for CUDA\n",
    "    tokenizer.src_lang = \"ru_RU\"\n",
    "    batch_in = tokenizer(text_batch.toxic_comment.tolist(), return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    batch_out = tokenizer(text_batch.neutral_comment.tolist(), return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    batch_out.input_ids[batch_out.input_ids==tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded = model.model.encoder(**batch_in)\n",
    "    transformed = adapter(encoded.last_hidden_state)\n",
    "\n",
    "    total_out = model(\n",
    "        encoder_outputs=[transformed], \n",
    "        attention_mask=batch_in.attention_mask,\n",
    "        decoder_attention_mask=batch_out.attention_mask,\n",
    "        labels=batch_out.input_ids,\n",
    "    )\n",
    "    total_out.loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(adapter.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    losses.append(total_out.loss.item())\n",
    "    if i % 100 == 0:\n",
    "        print(np.mean(losses[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "adapter.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ну что поверили этим педираторам всё это фуфло как и они сами\n",
      "Ну что поверили им? Все это обманчива как и они сами\n",
      "Ну что поверили этим людям, всё это обман, как и они сами\n"
     ]
    }
   ],
   "source": [
    "row = detox_ru2en.sample(1).iloc[0]\n",
    "text = row.toxic_comment\n",
    "print(text)\n",
    "print(row.neutral_comment)\n",
    "print(paraphrase_adapt(text, model, tokenizer, adapter, src_lang='ru_RU', tgt_lang='ru_RU', beams=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7acaaa1d4e48d59cbeaf0a99ffc1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs_ru = [paraphrase_adapt(text, model, tokenizer, adapter, src_lang='ru_RU', tgt_lang='ru_RU') for text in tqdm(test_inputs_ru)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5de407a063e40e29f0cbac8a38363fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_outputs_en = [paraphrase_adapt(text, model, tokenizer, adapter, src_lang='en_XX', tgt_lang='en_XX') for text in tqdm(test_inputs_en)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/enc_adapter_mbart_invariante_v1_ru2en/' \n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'results_ru.txt', 'w') as f:\n",
    "    for line in test_outputs_ru:\n",
    "        f.write(line + '\\n')\n",
    "with open(path + 'results_en.txt', 'w') as f:\n",
    "    for line in test_outputs_en:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(adapter.state_dict(), '/home/dale/models/detox-parallel/bart-multitask-pretrain-invariant/enc_adapter_mbart_invariante_v1_ru2en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /home/dale/projects/multilingual_detox\n",
    "python evaluate_ru.py \\\n",
    "    --result_filename scores \\\n",
    "    --input_dir results/enc_adapter_mbart_invariante_v1_ru2en\\\n",
    "    --output_dir results\n",
    "\n",
    "Style accuracy:       0.6849309206008911\n",
    "Meaning preservation: 0.8704279661178589\n",
    "Joint fluency:        -0.1665312647819519\n",
    "Joint score:          -0.0964374989271164\n",
    "Scores after calibration:\n",
    "Style accuracy:       0.7164378762245178\n",
    "Meaning preservation: 0.8056419491767883\n",
    "Joint fluency:        0.8084890842437744\n",
    "Joint score:          0.4693962335586548\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /home/dale/projects/paradetox2/evaluation_detox\n",
    "python metric.py --inputs /home/dale/projects/multilingual_detox/data/english_data/test_toxic_parallel.txt \\\n",
    "    --preds /home/dale/projects/multilingual_detox/results/enc_adapter_mbart_invariante_v1_ru2en/results_en.txt \\\n",
    "    --cola_classifier_path /home/dale/models/cola_classifier_fairseq \\\n",
    "    --wieting_model_path /home/dale/models/wieting_similarity/sim.pt \\\n",
    "    --wieting_tokenizer_path /home/dale/models/wieting_similarity/sim.sp.30k.model \\\n",
    "    --batch_size 32\n",
    "cat results.md\n",
    "```\n",
    "\n",
    "| Model | ACC | EMB_SIM | SIM | CharPPL | TokenPPL | FL | GM | J | BLEU |\n",
    "| ----- | --- | ------- | --- | ------- | -------- | -- | -- | - | ---- |\n",
    "results_en.txt|0.7004|0.8163|0.8151|36.4955|202.3833|0.8614|0.0000|0.4526|0.6083|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3k",
   "language": "python",
   "name": "p3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
